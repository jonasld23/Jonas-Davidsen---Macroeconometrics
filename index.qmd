---
title: "The Effect of Money Supply Shocks on Asset Prices and Inflation in the US Economy: A Bayesian SVAR Approach"
author: 
  - name: "Jonas Loopers Davidsen"
format:
  html:
    toc: true
    toc-location: left
---

> This research project is part of the assessment for the subject Macroeconometrics (ECOM90003) at the University of Melbourne held by Dr. Tomasz WoÅºniak.

> **Keywords.** money supply shocks, QE, asset prices, inflation, bsvar, gibbs sampler, impulse responses, R


```{r downloading and transforming data}
#| echo: false
#| message: false
#| warning: false
rm(list=ls())
#load packages
library(fredr)
library(quantmod)
library(xts)
library(ggplot2)
library(gridExtra)
library(datetimeutils)
library(mvtnorm)
library(MASS)
library(tseries)
library(tidyverse)
library(parallel)
library(HDInterval)

#input FRED key
fredr_set_key("2ffcd7e6c4f6e03de63ae1a03e4c3e6e")

#Load all data from 1987-01-01 to 2022-12-31 and transform to log for M1, CPI, HP and IP
M1   <- as.data.frame(fredr(series_id = "M1SL", observation_start = as.Date("1987-01-01"), observation_end = as.Date("2022-12-31")))
M1   <- ts(log(M1[,3]), start=c(1987,1), frequency=12)

FF   <- as.data.frame(fredr(series_id = "FEDFUNDS", observation_start = as.Date("1987-01-01"), observation_end = as.Date("2022-12-31")))
FF   <- ts(FF[,3], start=c(1987,1), frequency=12)

CPI  <- as.data.frame(fredr(series_id = "USACPIALLMINMEI", observation_start = as.Date("1987-01-01"), observation_end = as.Date("2022-12-31")))
CPI  <- ts(log(CPI[,3]), start=c(1987,1), frequency=12)

HP   <- as.data.frame(fredr(series_id = "CSUSHPISA", observation_start = as.Date("1987-01-01"), observation_end = as.Date("2022-12-31")))
HP   <- ts(log(HP[,3]), start=c(1987,1), frequency=12)

IP   <- as.data.frame(fredr(series_id = "INDPRO", observation_start = as.Date("1987-01-01"), observation_end = as.Date("2022-12-31")))
IP   <- ts(log(IP[,3]), start=c(1987,1), frequency=12)

#SPX needs some additional modification
SPX  <- as.xts(getSymbols("^GSPC", src = "yahoo", auto.assign = FALSE, from = "1987-01-01", to = "2022-12-31"))

#Adjust SPX to use start of month data and find first date in each month
dates <- nth_day(index(SPX), period = "month", n = "first")

#Pull only values on those dates
SPX <- SPX[index(SPX) %in% dates]

#transform to log
SPX <- as.data.frame(SPX)
SPX <- ts(log(SPX[,6]), start=c(1987,1), frequency=12)

#Create y matrix
y <- cbind(CPI, IP, FF, M1, HP, SPX)

#compute regressors based on 12 lags 
p=12
Y       = y[(p+1):nrow(y),]
X       = matrix(1,nrow(Y),1)
for (i in 1:p){
  X     = cbind(X,y[((p+1):nrow(y))-i,])
}

#transpose X and Y. Is needed for the algorithm
Y       = t(Y)
X       = t(X)
```

## Introduction

Despite extensive Quantitative Easing (QE) programs following the financial crisis in 2008, inflation continued to remain well under the target level in many advanced economies. Rather, the increase in the money supply primarily seemed to inflate asset prices instead of the general price level and in other words struggled to stimulate aggregate demand. However, following the Covid-19 pandemic central banks quite drastically expanded their QE programs and thereby raised the money supply to unprecedented levels in response to the economic downturn. Among other factors such as supply chain issues, surging energy prices and massive fiscal stimulus, this has been one of the drivers behind inflation reaching double digits recently in many countries. This raises questions about the effectiveness of monetary policy in stimulating the economy and simultaneously controlling inflation. Another concern regarding QE mainly inflating asset prices, is that it can lead to financial instability in terms of increased risk of assets becoming overvalued and detached from the underlying fundamentals. This can lead to asset price bubbles and increase the amount of speculation among investors. It is therefore crucial for both policy makers and investors to understand the mechanisms through which a money supply shock affects different economic variables such as asset prices and inflation in light of economic and financial stability.

**The research objective and question**

The objective of this research project is to investigate the effects of a money supply shock on asset prices and inflation in the US economy. So the research question is how does money supply affect asset prices and inflation and what are the implications for monetary policy and financial stability?

**The structure of this research project**

This research project is divided into several subsections. Throughout the report I will present three different models, which I define as the basic model, extended model and the model with $t$-distributed errors. In the basic model the hyperparameters for the prior distribution are being set exogenously, whereas they are being estimated in the extended model. For the third model, the error term is modelled to follow a $t$-distribution. I will start out by presenting the data and its properties. Afterwards I present the estimation procedure and the corresponding R-code, which I test on a bivariate random walk to validate that the code is running correctly. Next, I use the actual data and present the empirical results and interpretation.

## Data

In this section I present the data I use for this research project by plotting the time series and analyze its properties.

### Choice of variables

Given my focus on the relationship between money supply, asset prices and inflation, a measure for those three variables are needed. As a measure for money supply the M1 aggregate is chosen as it serves as a good proxy for the availability of liquidity in the economy. As measures for asset prices, both stock prices and house prices are included. These two types of assets are big components of the total assets in the US economy and provide a way to investigate the transmission mechanism of money supply shocks to asset prices and the real economy. Further, the CPI is chosen as it is commonly used to construct the so-called headline inflation. Moreover, the effective fed funds rate and industrial production in the US is included. The effective fed funds rate is the rate at which banks lend and borrow funds from each other overnight and is obviously heavily influenced by the actual fed funds rate. Industrial production is a measure for monthly US real activity and is chosen since actual GDP data is only available quarterly. These two variables are important to include as they play a crucial role in the relationship between money supply, asset prices and inflation and therefore serve as control variables. Thus, I have the following six variables for the US economy in my SVAR model:

-   $M_t$: M1 aggregate from FRED Database

-   $SPX_t$: SP500 index from Yahoo Finance

-   $HP_t$: S&P/Case-Shiller U.S. National Home Price Index from FRED Database

-   $CPI_t$: Consumer Price Index: All Items for the US from FRED Database

-   $ff_t$: Effective Fed Funds Rate from FRED Database

-   $IP_t$: Industrial Production: Total Index from FRED Database

Data from FRED Database is downloaded using the **fredr** package, while data from Yahoo Finance is downloaded using the **quantmod** package. My sample period will be from M1 1987 - M12 2022 as data for $HP_t$ only goes back to this period. As I am including stock prices in my model I choose the frequency of the data to be monthly as stock prices are highly volatile and liquid. By choosing quarterly data for stock price one would lose a lot of nuances and informationHence, the choice of industrial production as a proxy for GDP.

### Transformation and properties of the data

Since the effective fed funds rate, $ff_t$, is in percentages it is not being transformed. However, for the rest of the variables the log-transformation is being applied and we therefore get the following:

-   $m_t=\log(M_t)$
-   $spx_t=\log(SPX_t)$
-   $hp_t=\log(HP_t)$
-   $cpi_t=\log(CPI_t)$
-   $ip_t=\log(IP_t)$

This results in the following plots for the variables:

```{r visualize data}
#| echo: false
#| message: false
#| warning: false
#| label: fig-line-plot
#| fig-cap: "Time Series Plots"

#create date and variable name vector
date <- time(y)
names <- c("cpi","ip","ff","m","hp","spx")

#plot all time series
par(mfrow=c(3,2), mar=c(2,2,2,2))
for (i in 1:6){
  plot(date, y = y[,i], type = "l", 
       main = paste(names[i]), ylab = "", xlab = "",
       col = "plum4", lwd = 2.5,
       ylim = c(min(y[,i]),max(y[,i])))
}
```

From a graphical inspection one can clearly see that $m_t$, $spx_t$, $hp_t$, $cpi_t$ and $ip_t$ are not stationary processes and might contain one or more unit roots. However, for $ff_t$ it is rather ambiguous whether the series is stationary or not. It is essential to know whether we are dealing with non-stationary processes or not when setting the prior distributions for the variables. By making use of the Augmented Dickey Fuller (ADF) test it can be tested formally whether the variables are unit root processes.

```{r ADF Test}
#| echo: false
#| message: false
#| warning: false
#| results: hide

#ADF test for both levels and first diff
max_lag = 12
adf_ <- list()
adf_diff <- list()
for (i in 1:6) {
  adf_result = adf.test(y[,i], k = max_lag)
  adf_[[i]] <- adf_result
  adf_diff_result = adf.test(diff(y[,i]), k = max_lag) 
  adf_diff[[i]] <- adf_diff_result
}

#set up tables
adf_table_levels <- data.frame(Test_Statistic = numeric(length(adf_)), 
                        p_value = numeric(length(adf_)), 
                        Lags_Used = numeric(length(adf_)))

adf_table_diff <- data.frame(Test_Statistic = numeric(length(adf_diff)), 
                        p_value = numeric(length(adf_diff)), 
                        Lags_Used = numeric(length(adf_diff)))

for (i in 1:length(adf_)) {
  adf_table_levels[i, "Test_Statistic"] = round(adf_[[i]]$statistic,3)
  adf_table_levels[i, "p_value"] = round(adf_[[i]]$p.value,3)
  adf_table_levels[i, "Lags_Used"] = round(adf_[[i]]$parameter,3)
}

for (i in 1:length(adf_diff)) {
  adf_table_diff[i, "Test_Statistic"] = round(adf_diff[[i]]$statistic,3)
  adf_table_diff[i, "p_value"] = round(adf_diff[[i]]$p.value,3)
  adf_table_diff[i, "Lags_Used"] = round(adf_diff[[i]]$parameter,3)
}
```


```{r}
#| echo: false
# Compute tables
rownames(adf_table_levels) <- c("cpi", "ip", "ff", "m","hp","spx")
colnames(adf_table_levels)<- c("Test statistic", "P-value", "Lags")
```

```{r}
#| echo: false
rownames(adf_table_diff) <- c("\u0394cpi", "\u0394ip", "\u0394ff", "\u0394m","\u0394hp","\u0394spx")
colnames(adf_table_diff)<- c("Test statistic", "P-value", "Lags")
```

I start out by testing for unit roots for the variables in levels as in @tbl-adf-level. By looking at the p-values it is clear that all variables are non-stationary as we cannot reject the null hypothesis of the variables being a I(1) process. However, the test statistic for $ff_t$ seems to be very sensitive to the choice of lags as we do reject the null hypothesis for other $p$. I perform the ADF test once more to check whether the variables in first difference as in @tbl-adf-firstdifference are stationary and to establish that we are dealing with I(1) processes.

```{r show adf levels table}
#| echo: false
#| label: tbl-adf-level 
#| tbl-cap: ADF test results - Levels 
knitr::kable(adf_table_levels, digits = 3)
```

```{r show adf diff table}
#| echo: false
#| label: tbl-adf-firstdifference  
#| tbl-cap: ADF test results - First Differences  
knitr::kable(adf_table_diff, digits = 3)
```

It is clear that the variables in first differences are stationary as the null hypothesis can be rejected clearly.

The existence of a unit root in the time series can also be seen by plotting the Autocorrelation Functions (ACF) and Partial Autocorrelation Functions (PACF):

```{r ACF}
#| echo: false
#| message: false
#| warning: false
#| label: fig-acf-plot
#| fig-cap: "ACF Plots"
par(mfrow=c(3,2), mar=c(2,2,2,2))
for (j in 1:length(colnames(y))){
  acf(y[,j],main="")
  title(main = paste(names[j]), line = 1)
}
```

```{r PACF}
#| echo: false
#| message: false
#| warning: false
#| label: fig-pacf-plot
#| fig-cap: "PACF Plots"
par(mfrow=c(3,2), mar=c(2,2,2,2))
for (j in 1:length(colnames(y))){
  pacf(y[,j],main="")
  title(main = paste(names[j]), line = 1)
}
```

It is apparent that there is a clear memory pattern in the time series.

## The Econometric Model
For investigating the effect of money supply on asset prices and inflation, a structural VAR model will be used in this research project. The structural VAR model with $p$ lags can written as

```{=tex}
\begin{align}
B_0y_t &= b_0 + B_1y_{t-1}+\dots+B_py_{t-p}+w_t
\end{align}
```
where $y_t=[cpi_t$ $ip_t$ $ff_t$ $m_t$ $hp_t$ $spx_t]'$ and contains the six variables presented above. The error term $w_t$ conditioned on the past is assumed to be $w_t|Y_{t-1}\sim\;iid(\textbf{0}_N,I_N)$, where $N=6$ in this case. The $B_0$ is the so-called structural matrix and contains all contemporaneous relationships between the variables, which I essentially am interested in. However, this matrix cannot just be estimated without imposing certain assumptions. Therefore, the first step is to premultiply $B_0^{-1}$ on both sides so that we obtain the reduced form of the SVAR model:

```{=tex}
\begin{align}
y_t &= \mu_0 + A_1y_{t-1}+\dots+A_py_{t-p}+u_t
\end{align}
```
Where $A_i=B_0^{-1}B_i$ and $u_t=B_0^{-1}w_t$. It is assumed that $u_t|Y_{t-1}\sim\;iid(\textbf{0}_N,\Sigma)$, which allows us to denote $\Sigma = B_0^{-1} (B_0^{-1})'$. In order to reconstruct $B_0^{-1}$ and thereby identify the SVAR model, restrictions on the matrix need to imposed. As $B_0^{-1}$ consists of $K(K+1)/2$ variables, at least $K(K-1)/2$ restrictions need to be imposed. This can be done in multiple ways. In this project I will impose zero exclusion restrictions on $B_0^{-1}$ by implying a recursive system between the variables, which has to be economically justified. I will elaborate more on this later in this research project. It is important to note that if I choose a recursive system the ordering of $y_t$ is crucial. The estimation output I will interpret to measure how money supply shocks affect asset prices and inflation is impulse response functions (IRFs), which measure the dynamic response of a variable to a given shock.

# Estimation Procedure

The estimation procedure in this paper is based on the Markov Chain Monte Carlo (MCMC) Gibbs sampler algorithm presented in @waggoner2003gibbs, since I will make use of exclusion restrictions to identify the SVAR model.

## Basic Model

First, I redefine the model presented in the previous section to the following:

```{=tex}
\begin{align}
B_0y_t &= b_0 + B_1y_{t-1} + \dots + B_py_{t-p} u_t\\
       &= B_+ x_t + u_t
\end{align}
```
Where $B_+=\big[b_0\;B_1\;\dots\;B_p\big]$ and $x_t=\big[1\;y_{t-1}'\;\dots\;y_{t-p}'\big]$. As $B_0$ is the structural matrix the exclusion restrictions will be set on its rows such that $B_0=\left[b_1V_1\;\dots\;b_NV_N\right]'$ holds, where $B_{0[n\cdot]}=b_n\;V_n$ and represents the $n$th row of $B_0$. The dimension of $b_n$ is $1\times r_n$ and is a vector of the unrestricted elements of the $n$th row of $B_0$. The matrix $V_n$ is of dimension $r_n\times N$ and consists only of ones and zeroes since it is the restriction matrix. Now the structural model can be written equation-by-equation in the following way:

```{=tex}
\begin{align}
b_nV_ny_t &= B_nx_t + u_{n.t}\\
u_{n.t}   &\sim \mathcal{N}(0,1)
\end{align}
```
Which subsequently can be rewritten in matrix form as:

```{=tex}
\begin{align}
b_nV_nY &= B_nX+ U_n\\
U_n   &\sim \mathcal{N}(0_T,I_T)
\end{align}
```
where $\underset{(N \times T)}{Y}=\begin{pmatrix} y_1, \dots , y_T \end{pmatrix}$, $\underset{(K \times T)}{X}=\begin{pmatrix} x_1, \dots , x_T \end{pmatrix}$, $\underset{(1 \times T)}{U_n}=\begin{pmatrix} u_{n.1}, \dots , u_{n.T} \end{pmatrix}$ and $\underset{(1 \times K)}{B_n}= B_{+[n.]}$.

For convenience the likelihood function of $B_0$ and $B_+$ given data can be written as a $\mathcal{NGN}$ distribution:

```{=tex}
\begin{align}
L(B_+,B_0 | Y, X) \propto |\det(B_0)|^T \exp \left\{-\frac{1}{2} \sum_{n=1}^N (b_nV_nY-B_nX)(b_nV_nY-B_nX)'  \right\}
\end{align}
```
Moving to the prior distribution, the $\mathcal{NGN}$ distribution is being used as a natural-conjugate prior. Therefore, I define $p(B_+,B_0)\sim \mathcal{NGN}(\underline{B}, \underline{\Omega}, \underline{S}, \underline{\nu})$, where the following holds:

```{=tex}
\begin{align}
p(B_+,B_0)&=\left(\prod_{n=1}^N p(B_n|b_n)\right)p(b_1,\dots,b_n)\\
p(B_n|b_n)&\sim \mathcal{N}_K (b_nV_n\underline{B},\underline{\Omega})\\
p(b_1,\dots,b_n) &\propto |\det (B_0)|^{\underline{\nu}-N} \exp \left\{-\frac{1}{2}\sum_{n=1}^Nb_nV_n\underline{S}^{-1}V_n'b_n'\right\}
\end{align}
```
Which results in the following kernel of the natural-conjugate prior distribution:

```{=tex}
\begin{align}
|\det(B_0)|^{\underline{\nu}-N} \exp \left\{-\frac{1}{2} \sum_{n=1}^N b_nV_n\underline{S}^{-1}V_n'B_n'\right\} \times \exp \left\{-\frac{1}{2} \sum_{n=1}^N (B_n-b_nV_n\underline{B})\underline{\Omega}^{-1}(B_n-b_nV_n\underline{B})'\right\}
\end{align}
```
For the prior parameters the Minnesota prior parameters are being exploited:

```{=tex}
\begin{align}
\underline{B} &= \left[0_{N\times 1}\;I_N\;0_{N\times(p-1)N}\right]\\
\underline{\Omega} &= \text{diag} \left(\left[\kappa_2\;\kappa_1(\textbf{p}^{-2}\otimes I_N')\right)\right]\\
\underline{S} &= \kappa_0I_N\\
\underline{\nu} &= N
\end{align}
```
This enables us to derive the posterior distribution:

```{=tex}
\begin{align}
p(B_+,B_0|Y,X) &\propto L(B_+,B_0|Y,X)p(B_+,B_0)\\
               &\propto |\det(B_0)|^T \exp \left\{-\frac{1}{2} \sum_{n=1}^N (b_nV_nY-B_nX)(b_nV_nY-B_nX)'  \right\}\\
               &\times |\det(B_0)|^{\underline{\nu}-N} \exp \left\{-\frac{1}{2} \sum_{n=1}^N b_nV_n\underline{S}^{-1}V_n'B_n'\right\} \\ &\times \exp \left\{-\frac{1}{2} \sum_{n=1}^N (B_n-b_nV_n\underline{B})\underline{\Omega}^{-1}(B_n-b_nV_n\underline{B})'\right\}
\end{align}
```
By performing appropriate operations this can be expressed more densely the following way:

```{=tex}
\begin{align}
p(B_+,B_0|Y,X) &\propto |\det(B_0)|^{T+\underline{\nu}-N} \exp \left\{-\frac{1}{2} \sum_{n=1}^N (B_n-b_nV_n\bar{B})\bar{\Omega}^{-1}(B_n-b_nV_n\bar{B})'+b_nV_n\bar{S}^{-1}V_n'b_n'\right\}
\end{align}
```
Leading to the following posterior parameters:

```{=tex}
\begin{align}
p(B_+,B_0|Y,X) &\sim \mathcal{NGN}(\bar{B},\bar{\Omega},\bar{S},\bar{\nu})\\
\bar{\Omega}&=\left[XX'+\underline{\Omega}^{-1}\right]^{-1}\\
\bar{B}&=\left[YX'+\underline{B}\underline{\Omega}^{-1}\right]\bar{\Omega}\\
\bar{S}&=\left[YY'+\underline{S}^{-1}+\underline{B}\underline{\Omega}^{-1}\underline{B}'-\bar{B}\bar{\Omega}^{-1}\bar{B}'\right]^{-1}\\
\bar{\nu}&= T+\underline{\nu}
\end{align}
```
## The Gibbs Sampler

Having derived the posterior parameters, the gibbs sampler can now be scrutinized. As already outlined, the sampler is based on the $\mathcal{NGN}$ distribution. Further, the algorithm is divided into two steps. First, $B_0$ is drawn $S1+S2$ times from \begin{gather*}
    p(b_n | Y, X, b_1, \dots, b_{n-1}, b_{n+1}, \dots, b_N) 
\end{gather*} From which we get the posterior samples $\{b_1^{(s)},\dots, b_N^{(s)}\}^{S}_{s=1}$. Next step is to normalize these samples, so we subsequently can sample $B_n$ directly for each draw of $b_n^{(s)}$ from $p(B_n|Y,X,b_n)$. Based on this, the posterior draws $\left\{B_+^{(s)},B_0^{(s)}\right\}_{s=1}^{S1+S2}$ can be returned.

The gibbs sampler for $b_n^{(s)} \sim p(b_n | Y, X, b_1^{(s)}, \dots, b_{n-1}^{(s)}, b_{n+1}^{(s-1)}, \dots, b_N^{(s-1)})$ is computed by following the algorithm proposed by @waggoner2003gibbs. To facilitate this, following is defined:

-   $U_n = \text{chol}\Big(\bar{\nu}\Big(V_n\bar{S}^{-1}V_n'\Big)^{-1}\Big)$, where $U_n$ is a $r_n \times r_n$ upper-triangular matrix.

-   $w = \left[B_{0[-n.]}^{(s)}\right]_\perp$, where $w$ is a $1 \times N$ matrix.

-   $w_1 = wV_n'U_n'\cdot \Big( wV_n'U_n'V_nU_nw'\Big)^{-\frac{1}{2}}$, where $w_1$ is a $1 \times r_n$ vector.

-   $W_n=\begin{pmatrix} w_1' & w_{1\perp}' \end{pmatrix}$, where $W_n$ is a matrix of dimensions $r_n \times r_n$.

The $1 \times r_n$ matrix $\alpha_n$ can now be constructed by drawing the first element of $\alpha_n$ by following this procedure:

-   Draw $u \sim N(0_{\nu+1},{\bar{\nu}^{-1}I_{\nu+1}})$

-   Set $\alpha_{n[\cdot 1]} = \begin{cases}\sqrt{u'u} \text{ with probability 0.5}\\-\sqrt{u'u} \text{ with probability 0.5}\end{cases}$

The remaining $r_n-1$ elements of $\alpha_n$ can be drawn from $N(0_{r_n-1},\bar{\nu}^{-1}I_{r_n-1})$, after which the draw of the full conditional distribution of $b_n$ can be computed by $b_n^{(s)}\alpha_nW_nU_n$.

As already mentioned, these samples need to be normalized in order to ensure that a unique maximum is being found. I will not go into details of this procedure here, but rather refer to @waggoner2003likelihood for a rigorous outline.

## R Code Snippets

This section provides the R code behind the estimation procedure. In order to facilitate this, the following R functions are being used by the courtesy of Tomasz WoÅºniak.

The following function computes an orthogonal complement matrix to the input x, which is used in the **rgn()** function presented below.

```{r}
orthogonal.complement.matrix.TW = function(x){
  # x is a mxn matrix and m>n
  # the function returns a mx(m-n) matrix, out, that is an orthogonal complement of x, i.e.:
  # t(x)%*%out = 0 and det(cbind(x,out))!=0
  if( dim(x)[1] == 1 & dim(x)[2] == 2){
    x = t(x)
  }
  # x <- ifelse(dim(x)[1] == 1 && dim(x)[2] == 2, t(x), x)
  N     = dim(x)
  tmp   = qr.Q(qr(x, tol = 1e-10),complete=TRUE)
  out   = as.matrix(tmp[,(N[2]+1):N[1]])
  return(out)
}
```

The **rgn()** function simulates draws for $b_n$ from a $\mathcal{NGN}$ distribution

```{r}
 rgn             = function(n,S.inv,nu,V,B0.initial){
  # n     - a positive integer, the number of draws to be sampled
  # S     - an NxN positive definite matrix, a parameter of the generalized-normal distribution
  # nu    - a positive scalar, degrees of freedom parameter
  # V     - an N-element list, with fixed matrices
  # B0.initial - an NxN matrix, of initial values of the parameters
  N             = nrow(B0.initial)
  no.draws      = n
  
  B0            = array(NA, c(N,N,no.draws))
  B0.aux        = B0.initial
  
for (i in 1:no.draws){
    for (n in 1:N){
      rn            = nrow(V[[n]])
      SS_tmp        = nu*solve(V[[n]]%*%S.inv%*%t(V[[n]]))
      SS_tmp        = 0.5 * (SS_tmp + t(SS_tmp))
      Un            = chol(SS_tmp)
      w             = t(orthogonal.complement.matrix.TW(t(B0.aux[-n,])))
      w1            = w %*% t(V[[n]]) %*% t(Un) / sqrt(as.numeric(w %*% t(V[[n]]) %*% t(Un) %*% Un %*% V[[n]] %*% t(w)))
      if (rn>1){
        Wn          = cbind(t(w1),orthogonal.complement.matrix.TW(t(w1)))
      } else {
        Wn          = w1
      }
      alpha         = rep(NA,rn)
      u             = rmvnorm(1,rep(0,nu+1),(1/nu)*diag(nu+1))
      alpha[1]      = sqrt(as.numeric(u%*%t(u)))
      if (runif(1)<0.5){
        alpha[1]    = -alpha[1]
      }
      if (rn>1){
        alpha[2:rn] = rmvnorm(1,rep(0,nrow(V[[n]])-1),(1/nu)*diag(rn-1))
      }
      bn            = alpha %*% Wn %*% Un
      B0.aux[n,]    = bn %*% V[[n]]
    }
    B0[,,i]         = B0.aux
  }
  
  return(B0)
}
```

The next function normalizes the matrix of the contemporaneous effects, $B_0$:

```{r}
normalization.wz2003  = function(B0,B0.hat.inv, Sigma.inv, diag.signs){
  # This function normalizes a matrix of contemporaneous effects
  # according to the algorithm by Waggoner & Zha (2003, JOE)
  # B0        - an NxN matrix, to be normalized
  # B0.hat    - an NxN matrix, a normalized matrix
  
  N                 = nrow(B0)
  K                 = 2^N
  distance          = rep(NA,K)
  for (k in 1:K){
    B0.tmp.inv      = solve(diag(diag.signs[k,]) %*% B0)
    distance[k]     = sum(
      unlist(
        lapply(1:N,
               function(n){
                 t(B0.tmp.inv - B0.hat.inv)[n,] %*%Sigma.inv %*% t(B0.tmp.inv - B0.hat.inv)[n,]
               }
        )))
  }
  B0.out            = diag(diag.signs[which.min(distance),]) %*% B0
  
  return(B0.out)
}
```

This function normalizes the output from the **rgn()** function, ensuring that we obtain a unique maximum

```{r}
normalize.Gibbs.output.parallel          = function(B0.posterior,B0.hat){
  # This function normalizes the Gibbs sampler output from function rgn
  # using function normalization.wz2003 
  # B0.posterior  - a list, output from function rgn
  # B0.hat        - an NxN matrix, a normalized matrix
  
  N                 = nrow(B0.hat)
  K                 = 2^N
  
  B0.hat.inv        = solve(B0.hat)
  Sigma.inv         = t(B0.hat)%*%B0.hat
  
  diag.signs        = matrix(NA,2^N,N)
  for (n in 1:N){
    diag.signs[,n]  = kronecker(c(-1,1),rep(1,2^(n-1)))
  }
  
  B0.posterior.n    = mclapply(1:dim(B0.posterior)[3],function(i){
    normalization.wz2003(B0=B0.posterior[,,i],B0.hat.inv, Sigma.inv, diag.signs)
  },mc.cores=1
  )
  B0.posterior.n  = simplify2array(B0.posterior.n)
  
  return(B0.posterior.n)
}
```

Lastly, a function for simulating the draws of the multivariate normal distribution of the autoregressive slope matrix, $B_+$, is needed

```{r}
rnorm.ngn       = function(B0.posterior,B,Omega){
  # This function simulates draws for the multivariate normal distribution
  # of the autoregressive slope matrix of an SVAR model
  # from a normal-generalized-normal distribution according to algorithm 
  # by Waggoner & Zha (2003, JEDC)
  # B0.posterior  - a list, output from function rgn
  # B             - an NxK matrix, a parameter determining the mean of the multivariate conditionally normal distribution given B0
  # Omega         - a KxK positive definite matrix, a covariance matrix of the multivariate normal distribution
  
  N             = nrow(B)
  K             = ncol(B)
  no.draws      = dim(B0.posterior)[3]
  L             = t(chol(Omega))
  
  Bp.posterior  = lapply(1:no.draws,function(i){
    Bp          = matrix(NA, N, K)
    for (n in 1:N){
      Bp[n,]    = as.vector(t(B0.posterior[n,,i] %*% B) + L%*%rnorm(K))
    }
    return(Bp)
  })
  Bp.posterior  = simplify2array(Bp.posterior)
  return(Bp.posterior)
}
```

Having set up all the necessary functions, I now simulate a bivariate random walk to produce artificial data.

```{r simulation of data}
#Simulation of a bivariate random walk
p.sim = 1
T.sim = 500
N.sim = 2
K.sim = 1 + N.sim*p.sim

Y.sim           = arima.sim(list(order = c(0,1,0)), n = T.sim + p.sim-1, mean = 0, sd =1)
for (i in 2:N.sim){
  Y.sim         = rbind(Y.sim, arima.sim(list(order = c(0,1,0)), n = T.sim + p.sim-1, mean = 0, sd = 1))
}

X.sim           = matrix(1,1,T.sim)
for (i in 1:p.sim){
  X.sim         = rbind(X.sim, Y.sim[,(p.sim+1-i):(ncol(Y.sim)-i)])
}
Y.sim           = Y.sim[,-p.sim]
```

Finally, the gibbs sampler for the basic model can be presented. Note that the priors have been set in regards to the specification from above. Further, I create the restriction matrix $V_n$, where one easily can see that a recursive structure is being implied in the system.

```{r gibbs sampler for basic model}
Gibbs.sampler.base <- function(p,Y,X,S1,S2){

  N       = nrow(Y)
  p       = p 
  K       = 1+N*p
  S1      = S1
  S2      = S2
  kappa0  = 10
  kappa1  = 10
  kappa2  = 0.1
  
  priors   = list(
  B        = cbind(rep(0,N), diag(N), matrix(0, N, (p-1)*N)),
  Omega    = diag(c(kappa2,kappa1*((1:p)^(-2))%x%rep(1,N))),
  S        = kappa0*diag(N),
  nu       = N
  )
  
  # create the V matrices
  FF.V           = vector("list",N)
  for (n in 1:N){
  FF.V[[n]]   = cbind(diag(n),matrix(0,n,N-n))
  }

  # create initial B0 matrix
  B0.initial = matrix(0,N,N)
  for (n in 1:N){
  unrestricted    = apply(FF.V[[n]],2,sum)==1
  B0.initial[n,unrestricted] = rnorm(sum(unrestricted))
  }

  B0.posterior    <- array(NA,c(N,N,(S1+S2)))
  Bp.posterior    <- array(NA,c(N,(1+N*p),(S1+S2)))

  for (s in 1:(S1+S2)){

    # Computing posterior parameters
    Omega.inv      = solve(priors$Omega)
    Omega.post.inv = X%*%t(X) + Omega.inv
    Omega.post     = solve(Omega.post.inv)
    B.post         = (Y%*%t(X) + priors$B%*%Omega.inv) %*% Omega.post
    S.post         = Y%*%t(Y) + solve(priors$S) + priors$B%*%Omega.inv%*%t(priors$B) -   B.post%*%Omega.post.inv%*%t(B.post) 
    nu.post        = ncol(Y) + priors$nu

    # Use B0.initial for first iteration, otherwise the latest draw from B0.posterior
    if (s==1) {
      B0.s = B0.initial
    } else {
      B0.s = B0.posterior[,,s-1]
    }

    # sampling one draw B0 from the posterior distribution using Gibbs
    # rgn.function samples from a random conditional generalized normal distribution
    B0.tmp                  = rgn(n=1, S.inv=S.post, nu=nu.post, V=FF.V, B0.initial=B0.s)
    B0.posterior[,,s]       = B0.tmp[,,1]

    # sample one draw B+ from the normal conditional posterior
    Bp.tmp              = rnorm.ngn(B0.tmp, B=B.post,Omega=Omega.post)
    Bp.posterior[,,s]   = Bp.tmp[,,1]
  }
  # END OF GIBBS
  #Discard first S1 draws
  B0.posterior <- B0.posterior[,,(S1+1):(S1+S2)]
  Bp.posterior <- Bp.posterior[,,(S1+1):(S1+S2)]

  #normalisation of B0.posterior and Bp.posterior
  B0.hat             = diag(sign(diag(B0.tmp[,,1]))) %*% B0.tmp[,,1]

  B0.posterior.N    <- array(NA,c(N,N,S2))
  Bp.posterior.N    <- array(NA,c(N,(1+N*p),S2))

  B0.posteror.N.tmp      =  normalize.Gibbs.output.parallel(B0.posterior,B0.hat=B0.hat)
  for (s in 1:S2){
    B0.posterior.N[,,s]    = B0.posteror.N.tmp[,,s]
    Bp.posterior.N[,,s]    = B0.posterior.N[,,s]%*%solve(B0.posterior[,,s])%*%Bp.posterior[,,s]
  }

  return(list(B0.posterior.N = B0.posterior.N,
              Bp.posterior.N = Bp.posterior.N))
}
```

```{r baseline show}
#| echo: false
# Run Basic function
Basic.sim = Gibbs.sampler.base(p=1,Y=Y.sim,X=X.sim,S1=500,S2=3000)
```

Since a bivariate random walk was simulated, the $B_0$ matrix should be an identity matrix. Further, the first column of $B_+$ should be zero and the matrix $B_+[,2:3]$ should also be an identity matrix. This is also approximately the case as shown in @tbl-B0posterior and @tbl-Bpposterior, which indicates the estimation procedure is correct.

```{r}
#| echo: false
#| label: tbl-B0posterior
#| tbl-cap: Matrix B[0]
df.B0 <- as.data.frame(round(apply(Basic.sim$B0.posterior.N,1:2,mean),4))
colnames(df.B0) <- c("","")
knitr::kable(df.B0, index=FALSE)
```

```{r}
#| echo: false
#| label: tbl-Bpposterior
#| tbl-cap: Matrix B[+]
df.Bp <- as.data.frame(round(apply(Basic.sim$Bp.posterior.N,1:2,mean),4))
colnames(df.Bp) <- c("","","")
knitr::kable(df.Bp, index=FALSE)
```

## The Extended Model

As part of my extended model, I estimate the shrinkage parameters $\kappa_0$ and $\kappa_+$. Estimating those parameters instead of just setting them might lead to improved efficiency and reliability. By remembering how $\kappa_0$ and $\kappa_+$ affected the posterior parameters in the basic model, we can now write up the kernel for the new conjugate-prior up for the extended model:

```{=tex}
\begin{align}
p(B_+,B_0|Y,X) &\propto L(B_+,B_0|Y,X)p(B_+,B_0|\kappa_0,\kappa_+)p(\kappa_0)p(\kappa_+)\\
\end{align}
```
```{=tex}
\begin{align}
p(\kappa_0|\underline{s}_{\kappa_0},\underline{\nu}_{\kappa_0}) &\sim \mathcal{IG}2(\underline{s}_{\kappa_0},\underline{\nu}_{\kappa_0})\\
p(\kappa_+|\underline{s}_{\kappa_+},\underline{\nu}_{\kappa_+}) &\sim \mathcal{IG}2(\underline{s}_{\kappa_+},\underline{\nu}_{\kappa_+})
\end{align}
```
The full-conditional posterior distribution of $\kappa_0$ can be found to be:

```{=tex}
\begin{align}
p(\kappa_0|Y,X,B_0,B_+,\kappa_+) &\propto p(B_0|\kappa_0)p(\kappa_0)\\
&\propto \prod_{n=1}^N\kappa_0^{\frac{r_n}{2}}\exp \left\{  -\frac{1}{2}\sum_{n=1}^N b_nV_n(\kappa_0 I_{r_n})^{-1}V_n'b_n'\right\}\kappa_0^{-\frac{\underline{\nu}_{\kappa_0}+2}{2}}\exp \left\{  -\frac{1}{2} \frac{\underline{s}_{\kappa_0}}{\kappa_0}\right\}\\
&\propto \prod_{n=1}^N\kappa_0^{\frac{r_n}{2}} \exp \left\{  -\frac{1}{2}\frac{1}{\kappa_0}\sum_{n=1}^N b_nV_n I_{r_n}V_n'b_n'\right\}\kappa_0^{-\frac{\underline{\nu}_{\kappa_0}+2}{2}}\exp \left\{  -\frac{1}{2} \frac{\underline{s}_{\kappa_0}}{\kappa_0}\right\}
\end{align}
```
Since $\underline{S}=\kappa_0I_N$ and $b_n|\kappa_0 \sim \mathcal{N}(0,\kappa_0(V_nV_n')^{-1})=\mathcal{N}_{r_n}(0_{r_n},\kappa_0I_{r_n})$. By collecting the components in an appropriate way, the full-conditional posterior can be written as:

```{=tex}
\begin{align}
p(\kappa_0|Y,X,B_0,B_+,\kappa_+) &\propto \kappa_0^{-\frac{\bar{\nu}_{\kappa_0}+2}{2}} \exp \left\{ -\frac{1}{2}\frac{\bar{s}_{\kappa_0}}{\kappa_0} \right\}\\
\bar{s}_{\kappa_0} &= \underline{s}_{\kappa_0}+\sum_{n=1}^N b_nV_nI_{r_n}V_n'b_n'\\
\bar{\nu}_{\kappa_0} &= \underline{\nu}_{\kappa_0}+\sum_{n=1}^N r_n
\end{align}
```
The same procedure goes for the full-conditional posterior distribution of $\kappa_+$:

```{=tex}
\begin{align}
p(\kappa_+|Y,X,B_0,B_+,\kappa_0) &\propto p(B_+|B_0,\kappa_+)p(\kappa_+)\\
&\propto \kappa_+^{\frac{K}{2}}\exp \left\{-\frac{1}{2}\frac{1}{\kappa_+} \sum_{n=1}^N (B_n-b_nV_n\underline{B})\underline{\Omega}^{-1}(B_n-b_nV_n\underline{B})'\right\}\kappa_+^{-\frac{\underline{\nu}_{\kappa_+}+2}{2}}\exp \left\{  -\frac{1}{2} \frac{\underline{s}_{\kappa_+}}{\kappa_+}\right\}
\end{align}
```
Since $B_n|b_n,\kappa_+ \sim \mathcal{N}_{N+1}(b_nV_n\underline{B},\kappa_+\Omega)$

Which further can be derived to:

```{=tex}
\begin{align}
p(\kappa_+|Y,X,B_0,B_+,\kappa_0) &\propto \kappa_+^{-\frac{\bar{\nu}_{\kappa_+}+2}{2}} \exp \left\{ -\frac{1}{2}\frac{\bar{s}_{\kappa_+}}{\kappa_+} \right\}\\
\bar{s}_{\kappa_+} &= \underline{s}_{\kappa_+}+\sum_{n=1}^N (B_n-b_nV_n\underline{B})\underline{\Omega}^{-1}(B_n-b_nV_n\underline{B})'\\
\bar{\nu}_{\kappa_+} &= \underline{\nu}_{\kappa_+}+NK
\end{align}
```
This facilitates writing up the gibbs sampler for the extended model. Note that new priors have to be set compared to the basic model, as the hyperparameters are now being estimated:

```{r gibbs sampler for extended model}
Gibbs.sampler.extended <- function(p,Y,X,S1,S2){
  
  N       = nrow(Y)
  p       = p # calculate from X and Y (K and N)
  K       = 1+N*p
  S1      = S1
  S2      = S2
  
  #set new priors
  
  priors = list(
  B         = cbind(rep(0,N), diag(N), matrix(0, N, (p-1)*N)),
  Omega     = diag(c(10,((1:p)^(-2))%x%rep(1,N))),
  S         = diag(N),
  nu        = N,
  S.kappa0  = 1,
  nu.kappa0 = 1,
  S.kappa1  = 1,
  nu.kappa1 = 1
  )
  
  # create the V matrices
  FF.V           = vector("list",N)
  for (n in 1:N){
  FF.V[[n]]   = cbind(diag(n),matrix(0,n,N-n))
  }

  # create initial B0 matrix
  B0.initial = matrix(0,N,N)
  for (n in 1:N){
  unrestricted    = apply(FF.V[[n]],2,sum)==1
  B0.initial[n,unrestricted] = rnorm(sum(unrestricted))
  }
  
  kappa0          <- rep(NA, S1 + S2)
  kappa1          <- rep(NA, S1 + S2)
  B0.posterior    <- array(NA,c(N,N,(S1+S2)))
  Bp.posterior    <- array(NA,c(N,(1+N*p),(S1+S2)))  
  
  kappa0[1] <- 1
  kappa1[1] <- 1 
  
  for (s in 1:(S1+S2)){
    
    # Computing posterior parameters
    # Only Omega, B and S depend on kappa1
    
    Omega.inv      = solve(priors$Omega)
    Omega.post.inv = X%*%t(X) + (1/kappa1[s])*Omega.inv
    Omega.post     = solve(Omega.post.inv)
    B.post         = (Y%*%t(X) + priors$B%*%((1/kappa1[s])*Omega.inv)) %*% Omega.post
    S.post         = Y%*%t(Y) + (1/kappa0[s])*solve(priors$S) + priors$B%*%((1/kappa1[s])*Omega.inv)%*%t(priors$B) - B.post%*%Omega.post.inv%*%t(B.post) 
    nu.post        = ncol(Y) + priors$nu
    
    # Use B0.initial for first iteration, otherwise the latest draw from B0.posterior
    
    if (s==1) {
      B0.s = B0.initial
    } else {
      B0.s = B0.posterior[,,s-1]
    }
    
    # sampling one draw B0 from the posterior distribution using Gibbs  
    # rgn.function samples from a random conditional generalized normal distribution
    
    B0.tmp                  = rgn(n=1, S.inv=S.post, nu=nu.post, V=FF.V, B0.initial=B0.s)
    B0.posterior[,,s]       = B0.tmp[,,1]
    
    # sample one draw B+ from the normal conditional posterior
    Bp.tmp              = rnorm.ngn(B0.tmp, B=B.post,Omega=Omega.post)
    Bp.posterior[,,s]   = Bp.tmp[,,1]
    
    #compute posterior for the shrinkage parameter S.kappa and nu
    S.kappa0.post = priors$S.kappa0 + sum(B0.posterior[,,s]^2)
    
    nu.kappa0.post  = priors$nu.kappa0 + sum(unlist(lapply(FF.V, nrow)))
    
    S.kappa1.post   = priors$S.kappa1
    for (i in 1:N){
    S.kappa1.post = S.kappa1.post + (Bp.posterior[i,,s]-       B0.posterior[i,,s]%*%priors$B)%*%Omega.inv%*%t(Bp.posterior[i,,s]-B0.posterior[i,,s]%*%priors$B)
    }
    
    nu.kappa1.post  = priors$nu.kappa1 + N*(p*N+1) 
    
    
    #Draw kappa0 and kappa1 from IG2
    if (s != S1+S2) {
      kappa0[s+1]    = S.kappa0.post / rchisq(1, df=nu.kappa0.post) 
      kappa1[s+1]    = S.kappa1.post / rchisq(1, df=nu.kappa1.post) 
    }
  }
  
  #Discard first S1 draws
  
  B0.posterior <- B0.posterior[,,(S1+1):(S1+S2)]
  Bp.posterior <- Bp.posterior[,,(S1+1):(S1+S2)]
  kappa0       <- kappa0[(S1+1):(S1+S2)]
  kappa1       <- kappa1[(S1+1):(S1+S2)]
  
  #normalisation of B0.posterior and Bp.posterior
  
  B0.hat             = diag(sign(diag(B0.tmp[,,1]))) %*% B0.tmp[,,1]
  # t(chol((nu.post-N)*S.post))# normalisation using this B0.hat should work
  
  B0.posterior.N    <- array(NA,c(N,N,S2))
  Bp.posterior.N    <- array(NA,c(N,(1+N*p),S2))
  
    B0.posteror.N.tmp      =  normalize.Gibbs.output.parallel(B0.posterior,B0.hat=B0.hat)
  for (s in 1:S2){
    B0.posterior.N[,,s]    = B0.posteror.N.tmp[,,s]
    Bp.posterior.N[,,s]    = B0.posterior.N[,,s]%*%solve(B0.posterior[,,s])%*%Bp.posterior[,,s]
  }
  
  return(list(B0.posterior.N = B0.posterior.N,
              Bp.posterior.N = Bp.posterior.N,
              kappa0 = kappa0,
              kappa1 = kappa1))
} 
```

```{r extended show}
#| echo: false
# Run Basic function
# Run function
extended.sim = Gibbs.sampler.extended(p=1,Y=Y.sim,X=X.sim,S1=500,S2=3000)
```

```{r}
#| echo: false
#| label: tbl-B0posteriorex
#| tbl-cap: Matrix B[0]
df.B0.e <- as.data.frame(round(apply(extended.sim$B0.posterior.N,1:2,mean),4))
colnames(df.B0.e) <- c("","")
knitr::kable(df.B0.e, index=TRUE)
```

```{r}
#| echo: false
#| label: tbl-Bpposteriorex
#| tbl-cap: Matrix B[+]
df.Bp.e <- as.data.frame(round(apply(extended.sim$Bp.posterior.N,1:2,mean),4))
colnames(df.Bp.e) <- c("","","")
knitr::kable(df.Bp.e, index=TRUE)
```

From @tbl-B0posteriorex and @tbl-Bpposteriorex, the estimation procedure for the extended model seems to be correct as well as the output aligns with the artificial data being a bivariate random walk.

I now turn to plotting the diagonal elements of $B_+[,2:3]$ in order to show whether the algorithm converges.

```{r showing converge plot}
#| echo: false
#| message: false
#| warning: false
#| label: fig-convergence-plot
#| fig-cap: "Convergence Plots"
# Plotting convergence
par(mfrow=c(1,2))
plot(extended.sim$Bp.posterior.N[,2,][1,],type='l',col="#660099",ylab="",xlab="",main=expression(B[+12]), lwd = 0.1)
plot(extended.sim$Bp.posterior.N[,3,][2,],type='l',col="#CC66CC",ylab="",xlab="",main=expression(B[+23]), lwd = 0.1)
```

This is indeed the case. The plots look like white noise processes as it fluctuates around the true value 1. This means the algorithm has converged.

## Model with $t$-distributed errors

Since macroeconomic variables tend to be characterized by fat tails, I now proceed by extending the model even further by modelling the error terms to follow a $t$-distribution. Recall from the previous section that $u_{n.t}\sim\mathcal{N}(0,1)$, where $u_{n.t}$ represents the structural shock for the $n$th row. This is now being replaced by the following assumption 

```{=tex}
\begin{align}
u_{n.t}|\lambda\sim\mathcal{N}(0,\lambda_n)
\end{align}
```
Where $\lambda_n \sim \mathcal{IG}2(\underline{s}_{\lambda},\underline{\nu}_{\lambda})$. The distribution of the error term then becomes the following: 

```{=tex}
\begin{align}
p(u_{n.t}) &= \int p(u_{n.t}|\lambda_n)p(\lambda_n)d\lambda_n \\
& = \mathcal{t}(0,\underline{s},\underline{\nu})
\end{align}
```
This obviously also alters the likelihood function in the model, which can be rewritten to:
```{=tex}
\begin{align}
L(B_+,B_0 | Y, X) \propto |\det(B_0)|^T|\det(\lambda I_t)|^N \exp \left\{-\frac{1}{2} \sum_{n=1}^N (b_nV_nY-B_nX)(\lambda I_t)^{-1}(b_nV_nY-B_nX)'  \right\}
\end{align}
```

We can now find the full conditional posterior for $\lambda$:

```{=tex}
\begin{align}
p(\lambda|Y,X,B_0,B_+) &\propto p(B_+|B_0,\lambda)p(\lambda)\\
&\propto |\det(\Omega)|^T|\det(\lambda I_t)|^N \exp \left\{-\frac{1}{2} \sum_{n=1}^N (B_n-b_nV_n\underline{B})(\lambda I_T)^{-1}\underline{\Omega}^{-1}(B_n-b_nV_n\underline{B})'\right\}\lambda^{-\frac{\underline{\nu}_{\lambda}+2}{2}}\exp \left\{  -\frac{1}{2} \frac{\underline{s}_{\lambda}}{\lambda}\right\}
\end{align}
```
Which can be shown to yield the following posterior parameters:
```{=tex}
\begin{align}
p(\lambda|Y,X,B_0,B_+) &\propto \lambda^{-\frac{\bar{\nu}_{\lambda}+2}{2}} \exp \left\{ -\frac{1}{2}\frac{\bar{s}_{\lambda}}{\lambda} \right\}\\
\bar{s}_{\lambda} &= \underline{s}_{\lambda}+\sum_{n=1}^N (B_n-b_nV_n\underline{B})\underline{\Omega}^{-1}(B_n-b_nV_n\underline{B})'\\
\bar{\nu}_{\lambda} &= \underline{\nu}_{\lambda}+NT
\end{align}
```
This facilitates writing up the gibbs sampler for this model below. Note that $\underline{s}_{\lambda}=1$ and $\underline{\nu}_{\lambda}=4$. By setting the degrees of freedom rather low this allows for fat tails.

```{r}
Gibbs.sampler.tdis <- function(p,Y,X,S1,S2){
  #
  N       = nrow(Y)
  p       = p 
  K       = 1+N*p
  S1      = S1 # S1
  S2      = S2 #S2
  
  ### Setting new priors
  priors   = list(
  B        = cbind(rep(0,N), diag(N), matrix(0, N, (p-1)*N)),
  Omega    = diag(c(10,((1:p)^(-2))%x%rep(1,N))),
  S        = diag(N),
  nu       = N,
  S.kappa0  = 1,
  nu.kappa0 = 1,
  S.kappa1  = 1,
  nu.kappa1 = 1, 
  s.lambda = 1, 
  nu.lambda = 4
  )
  
  lambda.draw = priors$s.lambda/rchisq(1, priors$nu.lambda)

  
  #Exclusions (can be changed to different exclusions then cholesky) 
FF.V           = vector("list",N)
for (n in 1:N){
  FF.V[[n]]   = cbind(diag(n),matrix(0,n,N-n))
}

# The B0.initial is used as an initial matrix used in the Gibbs sampler
B0.initial = matrix(0,N,N)
for (n in 1:N){
  unrestricted    = apply(FF.V[[n]],2,sum)==1
  B0.initial[n,unrestricted] = rnorm(sum(unrestricted))
}
  
  kappa0          <- rep(NA, S1 + S2)
  kappa1          <- rep(NA, S1 + S2)
  B0.posterior    <- array(NA,c(N,N,(S1+S2)))
  Bp.posterior    <- array(NA,c(N,(1+N*p),(S1+S2)))
  lambda          <- rep(NA, S1 + S2)
  
  #  lambda[1] <- priors$lambda.draw
  kappa0[1] <- 1
  kappa1[1] <- 1
  
  for (s in 1:(S1+S2)){
    # Computing posterior parameters
    # Only Omega, B and S depend on kappa1
    
    if (s==1) {
      B0.s = B0.initial
      lambda = lambda.draw 
    } else {
      B0.s = B0.posterior[,,s-1]
      lambda[s]=lambda[s]
    }  
    
    Omega.inv      = solve(priors$Omega)
    Omega.post.inv = (1/lambda[s])*X%*%t(X) + (1/kappa1[s])*Omega.inv
    Omega.post     = solve(Omega.post.inv)
    B.post         = ((1/lambda[s])*Y%*%t(X) + priors$B%*%((1/kappa1[s])*Omega.inv)) %*% Omega.post
    S.post         = (1/lambda[s])*Y%*%t(Y) + (1/kappa0[s])*solve(priors$S) + priors$B%*%((1/kappa1[s])*Omega.inv)%*%t(priors$B) - B.post%*%Omega.post.inv%*%t(B.post)
    nu.post        = ncol(Y) + priors$nu
    
    # sampling one draw B0 from the posterior distribution using Gibbs  
    # rgn.function samples from a random conditional generalized normal distribution
    
    B0.tmp                  = rgn(n=1, S.inv=S.post, nu=nu.post, V=FF.V, B0.initial=B0.s)
    B0.posterior[,,s]       = B0.tmp[,,1]
    
    # sample one draw B+ from the normal conditional posterior
    Bp.tmp              = rnorm.ngn(B0.tmp, B=B.post,Omega=Omega.post)
    Bp.posterior[,,s]   = Bp.tmp[,,1]
    
    #compute posterior for the shrinkage parameter S.kappa and nu
    S.kappa0.post = priors$S.kappa0 + sum(B0.posterior[,,s]^2)
    
    nu.kappa0.post  = priors$nu.kappa0 + sum(unlist(lapply(FF.V, nrow)))
    
    S.kappa1.post   = priors$S.kappa1
    for (i in 1:N){
      S.kappa1.post = S.kappa1.post + (Bp.posterior[i,,s]- B0.posterior[i,,s]%*%priors$B)%*%Omega.inv%*%t(Bp.posterior[i,,s]-B0.posterior[i,,s]%*%priors$B)
    }
    
    nu.kappa1.post  = priors$nu.kappa1 + N*(p*N+1) 
    
    #### LAMBDA
    
    S.lambda.post   = priors$s.lambda
    for (i in 1:N){
      S.lambda.post = S.lambda.post + (Bp.posterior[i,,s]- B0.posterior[i,,s]%*%priors$B)%*%Omega.inv%*%t(Bp.posterior[i,,s]-B0.posterior[i,,s]%*%priors$B)
    }
    
    nu.lambda.post  = priors$nu.lambda + N*ncol(Y) 
    

    #Draw kappa0, kappa1 and lambda from IG2
    if (s != S1+S2) {
      kappa0[s+1]    = S.kappa0.post / rchisq(1, df=nu.kappa0.post) 
      kappa1[s+1]    = S.kappa1.post / rchisq(1, df=nu.kappa1.post)
      lambda[s+1]    = S.lambda.post / rchisq(1, df=nu.lambda.post)
    }
  }
  
  #Discard first S1 draws
  
  B0.posterior <- B0.posterior[,,(S1+1):(S1+S2)]
  Bp.posterior <- Bp.posterior[,,(S1+1):(S1+S2)]
  kappa0       <- kappa0[(S1+1):(S1+S2)]
  kappa1       <- kappa1[(S1+1):(S1+S2)]
  lambda       <- lambda[(S1+1):(S1+S2)]
  
  #normalisation of B0.posterior and Bp.posterior
  
  B0.hat             = diag(sign(diag(B0.tmp[,,1]))) %*% B0.tmp[,,1]
  # t(chol((nu.post-N)*S.post))# normalisation using this B0.hat should work
  
  B0.posterior.N    <- array(NA,c(N,N,S2))
  Bp.posterior.N    <- array(NA,c(N,(1+N*p),S2))
  
  B0.posteror.N.tmp      =  normalize.Gibbs.output.parallel(B0.posterior,B0.hat=B0.hat)
  for (s in 1:S2){
    B0.posterior.N[,,s]    = B0.posteror.N.tmp[,,s]
    B0.posterior.N[,,s]    = B0.posterior.N[,,s]/sqrt(lambda[s]) 
    Bp.posterior.N[,,s]    = B0.posterior.N[,,s]%*%solve(B0.posterior[,,s])%*%Bp.posterior[,,s]
  }
  
  return(list(B0.posterior.N = B0.posterior.N,
              Bp.posterior.N = Bp.posterior.N,
              Omega.post = Omega.post, 
              kappa0 = kappa0,
              kappa1 = kappa1,
              lambda = lambda))
}
```

```{r tdis show}
#| echo: false
# Run Basic function
# Run function
extended.tdis = Gibbs.sampler.tdis(p=1,Y=Y.sim,X=X.sim,S1=500,S2=3000)
```
By running the model with the artificial data and looking at the $B_0$ and $B_+$ matrix in @tbl-B0posterior-tdis and @tbl-Bpposterior-tdis, it is clear that the output aligns with a bi-variate random walk.
```{r}
#| echo: false
#| label: tbl-B0posterior-tdis
#| tbl-cap: Matrix B[0]
df.B0.e <- as.data.frame(round(apply(extended.tdis$B0.posterior.N,1:2,mean),4))
colnames(df.B0.e) <- c("","")
knitr::kable(df.B0.e, index=TRUE)
```

```{r}
#| echo: false
#| label: tbl-Bpposterior-tdis
#| tbl-cap: Matrix B[+]
df.B0.e <- as.data.frame(round(apply(extended.tdis$Bp.posterior.N,1:2,mean),4))
colnames(df.B0.e) <- c("","")
knitr::kable(df.B0.e, index=TRUE)
```
Further, by computing a trace plot for $\lambda$ we see that the algorithm indeed also converges.
```{r showing converge plot}
#| echo: false
#| message: false
#| warning: false
#| label: fig-convergence-plot-tdis
#| fig-cap: "Convergence Plot of lambda"
# Plotting convergence
plot(extended.tdis$lambda,type='l',col="#660099",ylab="",xlab="",main=expression(lambda), lwd = 0.1)
```

# Empirical Investigation

```{r run all models with data}
#| echo: false
#| message: false
#| warning: false

basic    = Gibbs.sampler.base(p=12,Y=Y,X=X,S1=500,S2=10000)
extended = Gibbs.sampler.extended(p=12,Y=Y,X=X,S1=500,S2=10000)
tdis     = Gibbs.sampler.tdis(p=12,Y=Y,X=X,S1=500,S2=3000)
```
In this section I will present the empirical results for each model and provide the economic interpretation behind them. I will do so by computing the impulse responses for each variable from a shock to the money supply. Computations in R will be presented as well as plots of the impulse responses.

### Empirical results 

Before presenting the impulse response functions, the means and standard deviations of the posterior draws of the $B_0$ matrix for all three models are being reported below. Starting out with the basic model:

```{r}
#| echo: false
#| label: tbl-B0posterior-data
#| tbl-cap: Means of B0 for the basic model
df.B0 <- as.data.frame(round(apply(basic$B0.posterior.N,1:2,mean),4))
colnames(df.B0) <- c("","","","","","")
knitr::kable(df.B0, index=FALSE)
```

```{r}
#| echo: false
#| label: tbl-B0posterior-data-sd
#| tbl-cap: Standard deviations of B0 for the basic model
df.B0 <- as.data.frame(round(apply(basic$B0.posterior.N,1:2,sd),4))
colnames(df.B0) <- c("","","","","","")
knitr::kable(df.B0, index=FALSE)
```
Now for the extended model, where the estimated hyperparameters, $\kappa_0$ and $\kappa_+$, are plotted in a histogram:
```{r}
#| echo: false
#| label: tbl-B0posterior-data-ext
#| tbl-cap: Means of B0 for the extended model
df.B0 <- as.data.frame(round(apply(extended$B0.posterior.N,1:2,mean),4))
colnames(df.B0) <- c("","","","","","")
knitr::kable(df.B0, index=FALSE)
```

```{r}
#| echo: false
#| label: tbl-B0posterior-data-sd-ext
#| tbl-cap: Standard deviations of B0 for the extended model
df.B0 <- as.data.frame(round(apply(extended$B0.posterior.N,1:2,sd),4))
colnames(df.B0) <- c("","","","","","")
knitr::kable(df.B0, index=FALSE)
```


```{r}
#| echo: false 
#| message: false
#| warning: false
#| label: fig-histograms-kappa
#| fig-cap: "Histograms of hyperparameters"
  par(mfrow=c(1,2),mar=c(2,2,2,2))
  hist(extended$kappa0, breaks="Freedman-Diaconis", col='tomato3', main = expression(paste("Contemporaneous Hyperparameter", kappa[0])), border=F, xlim=c(0,max(extended$kappa0)))
  abline(v = mean(extended$kappa0), col='purple4', lwd = 2)
  hist(extended$kappa1, breaks="Freedman-Diaconis", col='royalblue', main = expression(paste("Autoregressive Hyperparameter", kappa[1])), border=F, xlim=c(0,max(extended$kappa1)))
  abline(v = mean(extended$kappa1), col='purple4', lwd = 3)
```
And now for the model with $t$-distributed error terms, where a trace plot of the estimated $\lambda$ is also plotted.
```{r}
#| echo: false
#| label: tbl-B0posterior-data-tdis
#| tbl-cap: Means of B0 for t-distributed model
df.B0 <- as.data.frame(round(apply(tdis$B0.posterior.N,1:2,mean),4))
colnames(df.B0) <- c("","","","","","")
knitr::kable(df.B0, index=FALSE)
```

```{r}
#| echo: false
#| label: tbl-B0posterior-data-sd-tdis
#| tbl-cap: Standard deviations of B0 for t-distributed model
df.B0 <- as.data.frame(round(apply(tdis$B0.posterior.N,1:2,sd),4))
colnames(df.B0) <- c("","","","","","")
knitr::kable(df.B0, index=FALSE)
```
```{r}
#| echo: false 
#| message: false
#| warning: false
#| label: fig-trace-lambda
#| fig-cap: "Convergence plot of lambda"
plot(tdis$lambda,type='l',col="#660099",ylab="",xlab="",main=expression(lambda), lwd = 0.1)
```
Unlike for the artificial data, it is clear that $\lambda$ does not converge for the actual data. This seems rather strange and might indicate that the implementation of $t$-distributed error terms is a failed experiment. I will therefore not proceed with this model and refrain from plotting the IRFs for this model below.

### Impulse Responses

By considering the VAR(1)-representation of the model we have:
```{=tex}
\begin{align}
Y_t &= \textbf{A}Y_{t-1} + E_t\\
&=E_t+\textbf{A}E_{t-1}+\textbf{A}^2E_{t-2}+...
\end{align}
```

Now, by making use of the matrix $J=\left[I_n\quad 0_{N\times N(p-1)}\right]$, the model can be transformed back to the VAR(p) representation:

```{=tex}
\begin{align}
y_t &=JY_t\\
&=JE_t+J\textbf{A}J'JE_{t-1}+J\textbf{A}^2J'JE_{t-2}+...\\
&=\varepsilon_t+J\textbf{A}J'\varepsilon_{t-1}+J\textbf{A}^2J'\varepsilon_{t-2}+...\\
\end{align}
```

Since we are dealing with a structural model it holds that $\varepsilon_t=Bu_t$. This implies the following:

```{=tex}
\begin{align}
y_t &=Bu_t+J\textbf{A}J'Bu_{t-1}+J\textbf{A}^2J'Bu_{t-2}+...\\
&=\Theta_0u_t+\Theta_1u_{t-1}+\Theta_2u_{t-2}+...\\
\end{align}
```
Where $\frac{\partial y_{t+i}}{\partial u_t}=\Theta_i$ is the IRF. See below the R function for computing these IRFS:

```{r computing IRFs}
IRF <- function(B0.posterior,Bp.posterior,p,h){
  
  h = h
  p = p

  #set colours for IRF plots
  mcxs1  = "#05386B"
  mcxs1.rgb   = col2rgb(mcxs1)
  mcxs1.shade1= rgb(mcxs1.rgb[1],mcxs1.rgb[2],mcxs1.rgb[3], alpha=120, maxColorValue=255)
 
  N  <- dim(B0.posterior)[2]
  S2 <- dim(B0.posterior)[3]
  K  <- 1+N*p
  
 B.posterior       = array(NA,c(N,N,S2))
 A.posterior       = array(NA,c(N,K,S2))
 for (s in 1:S2){
  B               = solve(B0.posterior[,,s])
  B.posterior[,,s]= B
  A.posterior[,,s]= B %*% Bp.posterior[,,s]
}

IRF.posterior     = array(NA,c(N,N,h+1,S2))
IRF.inf.posterior = array(NA,c(N,N,S2))
J                 = cbind(diag(N),matrix(0,N,N*(p-1)))
for (s in 1:S2){
  A.bold          = rbind(A.posterior[,2:(1+N*p),s],cbind(diag(N*(p-1)),matrix(0,N*(p-1),N)))
  IRF.inf.posterior[,,s]          = J %*% solve(diag(N*p)-A.bold) %*% t(J) %*% B.posterior[,,s]
  A.bold.power    = A.bold
  for (i in 1:(h+1)){
    if (i==1){
      IRF.posterior[,,i,s]        = B.posterior[,,s]
    } else {
      IRF.posterior[,,i,s]        = J %*% A.bold.power %*% t(J) %*% B.posterior[,,s]
      A.bold.power                = A.bold.power %*% A.bold
    }
  }
}

IRF.posterior.mps = IRF.posterior[,4,,]
IRFs.k1           = apply(IRF.posterior.mps,1:2,median)
IRFs.inf.k1       = apply(IRF.posterior.mps,1,mean)
rownames(IRFs.k1) = colnames(y)

IRFs.k1.hdi    = apply(IRF.posterior.mps,1:2,hdi, credMass=0.68)
hh             = 1:(h+1)
pl = par(mfrow=c(3,2), mar=c(4,4.5,2,2),cex.axis=1.5, cex.lab=1.5)
for (n in 1:N){
  ylims     = range(IRFs.k1[n,hh],IRFs.k1.hdi[,n,hh])
  plot(hh,IRFs.k1[n,hh], type="l", ylim=ylims, axes=FALSE, xlab="", ylab=rownames(IRFs.k1)[n])
  if (n==5 | n==6){
    axis(1,c(1,13,25),c("","1y","2y"))
  } else {
    axis(1,c(1,13,25),c("","",""))
  }
  axis(2,c(ylims[1],0,ylims[2]),round(c(ylims[1],0,ylims[2]),3))
  polygon(c(hh,(h+1):1), c(IRFs.k1.hdi[1,n,hh],IRFs.k1.hdi[2,n,(h+1):1]), col=mcxs1.shade1,border=mcxs1.shade1)
  abline(h=0)
  lines(hh, IRFs.k1[n,hh],lwd=2,col=mcxs1)
}
}
```


### Identification Procedure

In order to being able to estimate a SVAR model the $B_0$ matrix has to be identified. As already briefly mentioned, I will imply a recursive ordering on the contemporaneous effects between my variables and therefore simply apply a Cholesky decomposition:

\begin{align*}
Bu_t=\begin{bmatrix}
    b_{11} & 0 & 0 & 0 & 0 & 0 \\
    b_{21} & b_{22} & 0 & 0 & 0 & 0 \\
    b_{31} & b_{32} & b_{33} & 0 & 0 & 0 \\
    b_{41} & b_{42} & b_{43} & b_{44} & 0 & 0 \\
    b_{51} & b_{52} & b_{53} & b_{54} & b_{55} & 0 \\
    b_{61} & b_{62} & b_{63} & b_{64} & b_{65} & b_{66} \\
\end{bmatrix}\begin{bmatrix}
    u_{cpi_t}\\u_{ip_t}\\u_{ff_t}\\u_{m_t}\\u_{hp_t}\\u_{spx_t}
\end{bmatrix}
\end{align*}

These restrictions and the implied recursive system have to be economically justified. The first equation in the system I interpret as a horizontal AS curve, while the second equation I interpret as a downward sloping AD curve. The third equation represents the monetary policy reaction function of the FED that reacts on both prices and output. Having money supply as the fourth variable is justifiable as the FED doesn't react on the money supply. Note that this variable is also the shock of interest in this research project. Last but not least, house prices and stock prices are in the end of the contemporaneous causal chain.

### Impulse responses plots

The IRFs are plotted for the basic model and the extended model. Generally, the results are in line with economic theory and therefore have standard interpretations. In other words, the results are mostly as expected. A positive shock to the money supply can be seen as an expansionary and inflationary shock and should therefore be accompanied with an increase in CPI, real activity and asset prices. This is also indeed the case as all those variables react positively to the shock both contemporaneously and in subsequent periods. However, it is rather puzzling why the Fed Funds rate rises as well. Economic theory would suggest that interest rates would fall following a money supply shock, as an increased money stock makes liquidity relatively cheaper and therefore one would except the Fed Funds rate to fall.

There are no big differences between the plots across the three models. The most notable differences are that the confidence bands for the IRF of CPI are wider for the basic model than for the extended model, whereas it is the opposite case for the Fed Funds rate.

```{r show IRFs basic}
#| echo: false
#| message: false
#| warning: false
#| label: fig-irf-basic-plot
#| fig-cap: "IRFs for basic model" 

IRF(B0.posterior=basic$B0.posterior.N,Bp.posterior=basic$Bp.posterior.N,p=12,h=24)
```

```{r show IRFs extended}
#| echo: false
#| message: false
#| warning: false
#| label: fig-irf-extended-plot
#| fig-cap: "IRFs for extended model" 
IRF(B0.posterior=extended$B0.posterior.N,Bp.posterior=extended$Bp.posterior.N,p=12,h=24)
```


# Concluding Remarks

In this research project I have investigated the effect of a money supply shock on asset prices and inflation. As expected, an increase in the money supply affects both asset prices and inflation positively. It seems like there is a larger effect on asset prices compared to on inflation, but it is tough to say. Throughout the project I presented three different models, namely the basic model where the hyperparameters were set exogenously, the extended model where the hyperparameters were being estimated and the model with $t$-distributed error terms. The model with $t$-distributed error terms didn't seem to converge with the data and therefore I did not compute the IRFs for this model. When comparing the basic and extended model it is also tough to say whether the extended model enhanced the empirical results. One could argue that the confidence bands of the IRFs seemed to be a tad narrower for the extended model compared with the basic model.









# References