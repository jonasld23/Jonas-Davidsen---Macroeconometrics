---
title: "The Effect of Money Supply Shocks on Asset Prices and Inflation in the US Economy: A Bayesian SVAR Approach"
author: 
  - name: "Jonas Loopers Davidsen"
format:
  html:
    toc: true
    toc-location: left
---

<<<<<<< HEAD
> **Keywords.** money supply shocks, QE, asset prices, inflation, bsvar, gibbs sampler, impulse responses, R
=======
> Disclaimer: This document is merely a research proposal and therefore still subject to changes along the process.\
>>>>>>> 41d47063279825e9a18740578892ff6012578e11

```{r downloading and transforming data}
#| echo: false
#| message: false
#| warning: false
<<<<<<< HEAD
rm(list=ls())
=======

>>>>>>> 41d47063279825e9a18740578892ff6012578e11
#load packages
library(fredr)
library(quantmod)
library(xts)
library(ggplot2)
library(gridExtra)
library(datetimeutils)
library(mvtnorm)
library(MASS)
library(tseries)
library(tidyverse)
library(parallel)
<<<<<<< HEAD
library(HDInterval)
=======
>>>>>>> 41d47063279825e9a18740578892ff6012578e11

#input FRED key
fredr_set_key("2ffcd7e6c4f6e03de63ae1a03e4c3e6e")

<<<<<<< HEAD
#Load all data from 1987-01-01 to 2022-12-31 and transform to log for M1, CPI, HP and IP
M1   <- as.data.frame(fredr(series_id = "M1SL", observation_start = as.Date("1987-01-01"), observation_end = as.Date("2022-12-31")))
M1   <- ts(log(M1[,3]), start=c(1987,1), frequency=12)
=======
#Load all data from 1987-01-01 to 2022-12-31 and transform to log for M2, FF, CPI, HP and IP
M2   <- as.data.frame(fredr(series_id = "M2SL", observation_start = as.Date("1987-01-01"), observation_end = as.Date("2022-12-31")))
M2   <- ts(log(M2[,3]), start=c(1987,1), frequency=12)
>>>>>>> 41d47063279825e9a18740578892ff6012578e11

FF   <- as.data.frame(fredr(series_id = "FEDFUNDS", observation_start = as.Date("1987-01-01"), observation_end = as.Date("2022-12-31")))
FF   <- ts(FF[,3], start=c(1987,1), frequency=12)

CPI  <- as.data.frame(fredr(series_id = "USACPIALLMINMEI", observation_start = as.Date("1987-01-01"), observation_end = as.Date("2022-12-31")))
CPI  <- ts(log(CPI[,3]), start=c(1987,1), frequency=12)

HP   <- as.data.frame(fredr(series_id = "CSUSHPISA", observation_start = as.Date("1987-01-01"), observation_end = as.Date("2022-12-31")))
HP   <- ts(log(HP[,3]), start=c(1987,1), frequency=12)

IP   <- as.data.frame(fredr(series_id = "INDPRO", observation_start = as.Date("1987-01-01"), observation_end = as.Date("2022-12-31")))
IP   <- ts(log(IP[,3]), start=c(1987,1), frequency=12)

#SPX needs some additional modification
SPX  <- as.xts(getSymbols("^GSPC", src = "yahoo", auto.assign = FALSE, from = "1987-01-01", to = "2022-12-31"))

<<<<<<< HEAD
#Adjust SPX to use start of month data and find first date in each month
dates <- nth_day(index(SPX), period = "month", n = "first")

#Pull only values on those dates
SPX <- SPX[index(SPX) %in% dates]

#transform to log
=======
#Adjust SPX to use start of month data
#Find first date in each month
dates <- nth_day(index(SPX), period = "month", n = "first")
#Pull only values on those dates
SPX <- SPX[index(SPX) %in% dates]
#transfrom to log
>>>>>>> 41d47063279825e9a18740578892ff6012578e11
SPX <- as.data.frame(SPX)
SPX <- ts(log(SPX[,6]), start=c(1987,1), frequency=12)

#Create y matrix
<<<<<<< HEAD
y <- cbind(CPI, IP, FF, M1, HP, SPX)

#compute regressors based on 12 lags 
p=12
Y       = y[(p+1):nrow(y),]
X       = matrix(1,nrow(Y),1)
for (i in 1:p){
  X     = cbind(X,y[((p+1):nrow(y))-i,])
}

#transpose X and Y. Is needed for the algorithm
Y       = t(Y)
X       = t(X)
=======
y <- cbind(CPI, IP, FF, M2, IP, SPX)
>>>>>>> 41d47063279825e9a18740578892ff6012578e11
```

## Introduction

Despite extensive Quantitative Easing (QE) programs following the financial crisis in 2008, inflation continued to remain well under the target level in many advanced economies. Rather, the increase in the money supply primarily seemed to inflate asset prices instead of the general price level and in other words struggled to stimulate aggregate demand. However, following the Covid-19 pandemic central banks quite drastically expanded their QE programs and thereby raised the money supply to unprecedented levels in response to the economic downturn. Among other factors such as supply chain issues, surging energy prices and massive fiscal stimulus, this has been one of the drivers behind inflation reaching double digits recently in many countries. This raises questions about the effectiveness of monetary policy in stimulating the economy and simultaneously controlling inflation. Another concern regarding QE mainly inflating asset prices, is that it can lead to financial instability in terms of increased risk of assets becoming overvalued and detached from the underlying fundamentals. This can lead to asset price bubbles and increase the amount of speculation among investors. It is therefore crucial for both policy makers and investors to understand the mechanisms through which a money supply shock affects different economic variables such as asset prices and inflation in light of economic and financial stability.

**The research objective and question**

The objective of this research project is to investigate the effects of a money supply shock on asset prices and inflation in the US economy. So the research question is how does money supply affect asset prices and inflation and what are the implications for monetary policy and financial stability?

## Data

In this section I will present the data I use for this research project by plotting the time series and analyze its properties.

<<<<<<< HEAD
### Choice of variables
=======
Despite extensive Quantitative Easing (QE) programs following the financial crisis in 2008, inflation continued to remain well under the target level in many advanced economies. Rather, the increase in the money supply primarily seemed to inflate asset prices instead of the general price level and in other words struggled to stimulate aggregate demand. However, following the Covid-19 pandemic central banks quite drastically expanded their QE programs and thereby raised the money supply to unprecedented levels in response to the economic downturn. Among other factors such as supply chain issues, surging energy prices and massive fiscal stimulus, this has been one of the drivers behind inflation reaching double digits recently in many countries. This raises questions about the effectiveness of monetary policy in stimulating the economy and simultaneously controlling inflation. Another concern regarding QE mainly inflating asset prices, is that it can lead to financial instability in terms of increased risk of assets becoming overvalued and detached from the underlying fundamentals. This can lead to asset price bubbles and increase the amount of speculation among investors. It is therefore crucial for both policy makers and investors to understand the mechanisms through which a money supply shock affects different economic variables such as asset prices and inflation in light of economic and financial stability.
>>>>>>> 41d47063279825e9a18740578892ff6012578e11

Given my focus on the relationship between money supply, asset prices and inflation, a measure for those three variables are needed. As a measure for money supply the M1 aggregate is chosen as it serves as a good proxy for the availability of liquidity in the economy. As measures for asset prices, both stock prices and house prices are included. These two types of assets are big components of the total assets in the US economy and provide a way to investigate the transmission mechanism of money supply shocks to asset prices and the real economy. Further, the CPI is chosen as it is commonly used to construct the so-called headline inflation. Moreover, the effective fed funds rate and industrial production in the US is included. The effective fed funds rate is the rate at which banks lend and borrow funds from each other overnight and is obviously heavily influenced by the actual fed funds rate. Industrial production is a measure for monthly US real activity and is chosen since actual GDP data is only available quarterly. These two variables are important to include as they play a crucial role in the relationship between money supply, asset prices and inflation and therefore serve as control variables. Thus, I have the following six variables for the US economy in my SVAR model:

-   $M_t$: M1 aggregate from FRED Database

<<<<<<< HEAD
-   $SPX_t$: SP500 index from Yahoo Finance

-   $HP_t$: S&P/Case-Shiller U.S. National Home Price Index from FRED Database
=======
-   $M_t$: M2 aggregate from FRED Database

-   $SPX_t$: SP500 index from Yahoo Finance

-   $HP_t$: S&P/Case-Shiller U.S. National Home Price Index from FRED Database

-   $CPI_t$: Consumer Price Index: All Items for the US from FRED Database

    -   Motivation: Given my focus on the relationship between money supply, asset prices and inflation, a measure for those three variables are needed. As a measure for money supply the M2 aggregate is chosen as it serves as a good proxy for the availability of liquidity in the economy. As measures for asset prices, both stock prices and house prices are included. These two types of assets are big components of the total assets in the economy and provide a way to investigate the transmission mechanism of money supply shocks to asset prices and the real economy. Further, the CPI is chosen as it is commonly used to construct the so-called headline inflation.

-   $ff_t$: Effective Fed Funds Rate from FRED Database

-   $IP_t$: Industrial Production: Total Index

    -   Motivation: The effective fed funds rate is the rate at which banks lend and borrow funds from each other overnight and is obviously heavily influenced by the actual fed funds rate. Industrial production is a measure for monthly US real activity and is chosen since actual GDP data is only available for each quarter. These two variables are important to include as they play a crucial role in the relationship between money supply, asset prices and inflation and therefore serve as control variables.
>>>>>>> 41d47063279825e9a18740578892ff6012578e11

-   $CPI_t$: Consumer Price Index: All Items for the US from FRED Database

-   $ff_t$: Effective Fed Funds Rate from FRED Database

-   $IP_t$: Industrial Production: Total Index from FRED Database

Data from FRED Database is downloaded using the **fredr** package, while data from Yahoo Finance is downloaded using the **quantmod** package. My sample period will be from M1 1987 - M12 2022 as data for $HP_t$ only goes back to this period. As I am including stock prices in my model I choose the frequency of the data to be monthly as stock prices are highly volatile and liquid. By choosing quarterly data for stock price one would lose a lot of nuances and informationHence, the choice of industrial production as a proxy for GDP.

### Transformation and properties of the data

Since the effective fed funds rate, $ff_t$, is in percentages it is not being transformed. However, for the rest of the variables the log-transformation is being applied and we therefore get the following:

<<<<<<< HEAD
-   $m_t=\log(M_t)$
-   $spx_t=\log(SPX_t)$
-   $hp_t=\log(HP_t)$
-   $cpi_t=\log(CPI_t)$
-   $ip_t=\log(IP_t)$
=======
$m_t=\log(M_t)$, $spx_t=\log(SPX_t)$, $hp_t=\log(HP_t)$, $cpi_t=\log(CPI_t)$, $ip_t=\log(IP_t)$.
>>>>>>> 41d47063279825e9a18740578892ff6012578e11

This results in the following plots for the variables:

```{r visualize data}
#| echo: false
#| message: false
#| warning: false
<<<<<<< HEAD
#| label: fig-line-plot
#| fig-cap: "Time Series Plots"

#create date and variable name vector
date <- time(y)
names <- c("cpi","ip","ff","m","hp","spx")
=======

#create date and variable name vector
date <- time(y)
names <- c("CPI", "IP", "FF", "M2", "HP", "SPX")
>>>>>>> 41d47063279825e9a18740578892ff6012578e11

#plot all time series
par(mfrow=c(3,2), mar=c(2,2,2,2))
for (i in 1:6){
  plot(date, y = y[,i], type = "l", 
       main = paste(names[i]), ylab = "", xlab = "",
<<<<<<< HEAD
       col = "plum4", lwd = 2.5,
=======
       col = "plum4", lwd = 1.5,
>>>>>>> 41d47063279825e9a18740578892ff6012578e11
       ylim = c(min(y[,i]),max(y[,i])))
}
```

<<<<<<< HEAD
From a graphical inspection one can clearly see that $m_t$, $spx_t$, $hp_t$, $cpi_t$ and $ip_t$ are not stationary processes and might contain one or more unit roots. However, for $ff_t$ it is rather ambiguous whether the series is stationary or not. It is essential to know whether we are dealing with non-stationary processes or not when setting the prior distributions for the variables. By making use of the Augmented Dickey Fuller (ADF) test it can be tested formally whether the variables are unit root processes.
=======
From a graphical inspection one can clearly see that $m_t$, $spx_t$, $hp_t$, $cpi_t$ and $ip_t$ are not stationary processes and might contain one or more unit roots. However, for $ff_t$ it is rather ambiguous whether the variables are stationary or not. It is essential to know whether we are dealing with non-stationary processes or not when setting the prior distributions for the variables. By making use of the Augmented Dickey Fuller (ADF) test it can be tested formally whether the variables are unit root processes.
>>>>>>> 41d47063279825e9a18740578892ff6012578e11

```{r ADF Test}
#| echo: false
#| message: false
#| warning: false
#| results: hide

<<<<<<< HEAD
#ADF test for both levels and first diff
=======
>>>>>>> 41d47063279825e9a18740578892ff6012578e11
max_lag = 12
adf_ <- list()
adf_diff <- list()
for (i in 1:6) {
  adf_result = adf.test(y[,i], k = max_lag)
  adf_[[i]] <- adf_result
  adf_diff_result = adf.test(diff(y[,i]), k = max_lag) 
  adf_diff[[i]] <- adf_diff_result
}

#set up tables
adf_table_levels <- data.frame(Test_Statistic = numeric(length(adf_)), 
                        p_value = numeric(length(adf_)), 
                        Lags_Used = numeric(length(adf_)))

adf_table_diff <- data.frame(Test_Statistic = numeric(length(adf_diff)), 
                        p_value = numeric(length(adf_diff)), 
                        Lags_Used = numeric(length(adf_diff)))

for (i in 1:length(adf_)) {
  adf_table_levels[i, "Test_Statistic"] = round(adf_[[i]]$statistic,3)
  adf_table_levels[i, "p_value"] = round(adf_[[i]]$p.value,3)
  adf_table_levels[i, "Lags_Used"] = round(adf_[[i]]$parameter,3)
}

for (i in 1:length(adf_diff)) {
  adf_table_diff[i, "Test_Statistic"] = round(adf_diff[[i]]$statistic,3)
  adf_table_diff[i, "p_value"] = round(adf_diff[[i]]$p.value,3)
  adf_table_diff[i, "Lags_Used"] = round(adf_diff[[i]]$parameter,3)
}

# Compute tables
rownames(adf_table_levels) <- c("cpi", "ip", "ff", "m","hp","spx")
colnames(adf_table_levels)<- c("Test statistic", "P-value", "Lags")
knitr::kable(adf_table_levels, digits = 3)

rownames(adf_table_diff) <- c("\u0394cpi", "\u0394ip", "\u0394ff", "\u0394m","\u0394hp","\u0394spx")
colnames(adf_table_diff)<- c("Test statistic", "P-value", "Lags")
knitr::kable(adf_table_diff, digits = 3)
```

<<<<<<< HEAD
```{r show adf levels table}
=======
```{r test show}
>>>>>>> 41d47063279825e9a18740578892ff6012578e11
#| echo: false
#| message: false
#| warning: false
knitr::kable(adf_table_levels, digits = 3)
```

I start out by testing for unit roots for the variables in levels. By looking at the p-values it is clear that all variables are non-stationary as we cannot reject the null hypothesis of the variables being a I(1) process. However, the test statistic for $ff_t$ seems to be very sensitive to the choice of lags as we do reject the null hypothesis for other $p$. I perform the ADF test once more to check whether the variables in first difference are stationary and to establish that we are dealing with I(1) processes.

```{r show adf diff table}
#| echo: false
#| message: false
#| warning: false
knitr::kable(adf_table_diff, digits = 3)
```

It is clear that the variables in first differences are stationary as the null hypothesis can be rejected clearly.

The existence of a unit root in the time series can also be seen by plotting the Autocorrelation Functions (ACF) and Partial Autocorrelation Functions (PACF):

```{r ACF}
#| echo: false
#| message: false
#| warning: false
#| label: fig-acf-plot
#| fig-cap: "ACF Plots"
par(mfrow=c(3,2), mar=c(2,2,2,2))
for (j in 1:length(colnames(y))){
  acf(y[,j],main="")
  title(main = paste(names[j]), line = 1)
}
```

```{r PACF}
#| echo: false
#| message: false
#| warning: false
#| label: fig-pacf-plot
#| fig-cap: "PACF Plots"
par(mfrow=c(3,2), mar=c(2,2,2,2))
for (j in 1:length(colnames(y))){
  pacf(y[,j],main="")
  title(main = paste(names[j]), line = 1)
}
```

It is apparent that there is a clear memory pattern in the time series.

## The model and hypothesis

For investigating the effect of money supply on asset prices and inflation, a structural VAR model will be used in this research project. The structural VAR model with $p$ lags can written as

```{=tex}
\begin{align}
B_0y_t &= b_0 + B_1y_{t-1}+\dots+B_py_{t-p}+w_t
\end{align}
```
<<<<<<< HEAD
where $y_t=[cpi_t$ $ip_t$ $ff_t$ $m_t$ $hp_t$ $spx_t]'$ and contains the six variables presented above. The error term $w_t$ conditioned on the past is assumed to be $w_t|Y_{t-1}\sim\;iid(\textbf{0}_N,I_N)$, where $N=6$ in this case. The $B_0$ is the so-called structural matrix and contains all contemporaneous relationships between the variables, which I essentially am interested in. However, this matrix cannot just be estimated without imposing certain assumptions. Therefore, the first step is to premultiply $B_0^{-1}$ on both sides so that we obtain the reduced form of the SVAR model:
=======
where $y_t=[m_t$ $spx_t$ $hp_t$ $inf_t$ $ff_t$ $ip_t]'$ and contains the six variables presented above. The error term $u_t$ conditioned on the past is assumed to be $w_t|Y_{t-1}\sim\;iid(\textbf{0}_N,I_N)$, where $N=6$ in my case. The $B_0$ is the so-called structural matrix and contains all contemporaneous relationships between the variables, which I essentially am interested in. However, this matrix can't just be estimated without certain assumptions. Therefore, the first step is to premultiply $B_0^{-1}$ on both sides so that we obtain the reduced form of the SVAR model:
>>>>>>> 41d47063279825e9a18740578892ff6012578e11

```{=tex}
\begin{align}
y_t &= \mu_0 + A_1y_{t-1}+\dots+A_py_{t-p}+u_t
\end{align}
```
<<<<<<< HEAD
Where $A_i=B_0^{-1}B_i$ and $u_t=B_0^{-1}w_t$. It is assumed that $u_t|Y_{t-1}\sim\;iid(\textbf{0}_N,\Sigma)$, which allows us to denote $\Sigma = B_0^{-1} (B_0^{-1})'$. In order to reconstruct $B_0^{-1}$ and thereby identify the SVAR model, restrictions on the matrix need to imposed. As $B_0^{-1}$ consists of $K(K+1)/2$ variables, at least $K(K-1)/2$ restrictions need to be imposed. This can be done in multiple ways. In this project I will impose zero exclusion restrictions on $B_0^{-1}$ by implying a recursive system between the variables, which has to be economically justified. I will elaborate more on this later in this research project. It is important to note that if I choose a recursive system the ordering of $y_t$ is crucial.

The estimation output I will interpret to measure how money supply shocks affect asset prices and inflation is impulse response functions (IRFs) and forecast error variance decomposition (FEVDs). IRFs measures the dynamic response of a variable to a given shock, while FEVDs are a measure for the contribution of different shocks to the forecast error variance of a certain variable.

# Estimation Procedure

The estimation procedure in this paper is based on the Markov Chain Monte Carlo (MCMC) Gibbs sampler algorithm presented in Waggoner & Zha (2003), since I will make use of exclusion restrictions to identify the SVAR model.

## Basic Model

First, I redefine the model presented in the previous section to the following:

```{=tex}
\begin{align}
B_0y_t &= b_0 + B_1y_{t-1} + \dots + B_py_{t-p} u_t\\
       &= B_+ x_t + u_t
\end{align}
```
Where $B_+=\big[b_0\;B_1\;\dots\;B_p\big]$ and $x_t=\big[1\;y_{t-1}'\;\dots\;y_{t-p}'\big]$. As $B_0$ is the structural matrix the exclusion restrictions will be set on its rows such that $B_0=\left[b_1V_1\;\dots\;b_NV_N\right]'$ holds, where $B_{0[n\cdot]}=b_n\;V_n$ and represents the $n$th row of $B_0$. The dimension of $b_n$ is $1\times r_n$ and is a vector of the unrestricted elements of the $n$th row of $B_0$. The matrix $V_n$ is of dimension $r_n\times N$ and consists only of ones and zeroes since it is the restriction matrix. Now the structural model can be written equation-by-equation in the following way:

```{=tex}
\begin{align}
b_nV_ny_t &= B_nx_t + u_{n.t}\\
u_{n.t}   &\sim \mathcal{N}(0,1)
\end{align}
```
Which subsequently can be rewritten in matrix form as:

```{=tex}
\begin{align}
b_nV_nY &= B_nX+ U_n\\
U_n   &\sim \mathcal{N}(0_T,I_T)
\end{align}
```
where $\underset{(N \times T)}{Y}=\begin{pmatrix} y_1, \dots , y_T \end{pmatrix}$, $\underset{(K \times T)}{X}=\begin{pmatrix} x_1, \dots , x_T \end{pmatrix}$, $\underset{(1 \times T)}{U_n}=\begin{pmatrix} u_{n.1}, \dots , u_{n.T} \end{pmatrix}$ and $\underset{(1 \times K)}{B_n}= B_{+[n.]}$.

For convenience the likelihood function of $B_0$ and $B_+$ given data can be written as a $\mathcal{NGN}$ distribution:

```{=tex}
\begin{align}
L(B_+,B_0 | Y, X) \propto |\det(B_0)|^T \exp \left\{-\frac{1}{2} \sum_{n=1}^N (b_nV_nY-B_nX)(b_nV_nY-B_nX)'  \right\}
\end{align}
```
Moving to the prior distribution, the $\mathcal{NGN}$ distribution is being used as a natural-conjugate prior. Therefore, I define $p(B_+,B_0)\sim \mathcal{NGN}(\underline{B}, \underline{\Omega}, \underline{S}, \underline{\nu})$, where the following holds:

```{=tex}
\begin{align}
p(B_+,B_0)&=\left(\prod_{n=1}^N p(B_n|b_n)\right)p(b_1,\dots,b_n)\\
p(B_n|b_n)&\sim \mathcal{N}_K (b_nV_n\underline{B},\underline{\Omega})\\
p(b_1,\dots,b_n) &\propto |\det (B_0)|^{\underline{\nu}-N} \exp \left\{-\frac{1}{2}\sum_{n=1}^Nb_nV_n\underline{S}^{-1}V_n'b_n'\right\}
\end{align}
```
Which results in the following kernel of the natural-conjugate prior distribution:

```{=tex}
\begin{align}
|\det(B_0)|^{\underline{\nu}-N} \exp \left\{-\frac{1}{2} \sum_{n=1}^N b_nV_n\underline{S}^{-1}V_n'B_n'\right\} \times \exp \left\{-\frac{1}{2} \sum_{n=1}^N (B_n-b_nV_n\underline{B})\underline{\Omega}^{-1}(B_n-b_nV_n\underline{B})'\right\}
\end{align}
```
For the prior parameters the Minnesota prior parameters are being exploited:

```{=tex}
\begin{align}
\underline{B} &= \left[0_{N\times 1}\;I_N\;0_{N\times(p-1)N}\right]\\
\underline{\Omega} &= \text{diag} \left(\left[\kappa_2\;\kappa_1(\textbf{p}^{-2}\otimes I_N')\right)\right]\\
\underline{S} &= \kappa_0I_N\\
\underline{\nu} &= N
\end{align}
```
This enables us to derive the posterior distribution:

```{=tex}
\begin{align}
p(B_+,B_0|Y,X) &\propto L(B_+,B_0|Y,X)p(B_+,B_0)\\
               &\propto |\det(B_0)|^T \exp \left\{-\frac{1}{2} \sum_{n=1}^N (b_nV_nY-B_nX)(b_nV_nY-B_nX)'  \right\}\\
               &\times |\det(B_0)|^{\underline{\nu}-N} \exp \left\{-\frac{1}{2} \sum_{n=1}^N b_nV_n\underline{S}^{-1}V_n'B_n'\right\} \\ &\times \exp \left\{-\frac{1}{2} \sum_{n=1}^N (B_n-b_nV_n\underline{B})\underline{\Omega}^{-1}(B_n-b_nV_n\underline{B})'\right\}
\end{align}
```
By performing appropriate operations this can be expressed more densely the following way:

```{=tex}
\begin{align}
p(B_+,B_0|Y,X) &\propto |\det(B_0)|^{T+\underline{\nu}-N} \exp \left\{-\frac{1}{2} \sum_{n=1}^N (B_n-b_nV_n\bar{B})\bar{\Omega}^{-1}(B_n-b_nV_n\bar{B})'+b_nV_n\bar{S}^{-1}V_n'b_n'\right\}
\end{align}
```
Leading to the following posterior parameters:

```{=tex}
\begin{align}
p(B_+,B_0|Y,X) &\sim \mathcal{NGN}(\bar{B},\bar{\Omega},\bar{S},\bar{\nu})\\
\bar{\Omega}&=\left[XX'+\underline{\Omega}^{-1}\right]^{-1}\\
\bar{B}&=\left[YX'+\underline{B\Omega}^{-1}\right]\bar{\Omega}\\
\bar{S}&=\left[YY'+\underline{S}^{-1}+\underline{B\Omega}^{-1}\underline{B}'-\bar{B}\bar{\Omega}^{-1}\bar{B}'\right]^{-1}\\
\bar{\nu}&= T+\underline{\nu}
\end{align}
```
## The Gibbs Sampler

Having derived the posterior parameters, the gibbs sampler can now be scrutinized. As already outlined, the sampler is based on the $\mathcal{NGN}$ distribution. Further, the algorithm is divided into two steps. First, $B_0$ is drawn $S1+S2$ times from \begin{gather*}
    p(b_n | Y, X, b_1, \dots, b_{n-1}, b_{n+1}, \dots, b_N) 
\end{gather*} From which we get the posterior samples $\{b_1^{(s)},\dots, b_N^{(s)}\}^{S}_{s=1}$. Next step is to normalize these samples, so we subsequently can sample $B_n$ directly for each draw of $b_n^{(s)}$ from $p(B_n|Y,X,b_n)$. Based on this, the posterior draws $\left\{B_+^{(s)},B_0^{(s)}\right\}_{s=1}^{S1+S2}$ can be returned.

The gibbs sampler for $b_n^{(s)} \sim p(b_n | Y, X, b_1^{(s)}, \dots, b_{n-1}^{(s)}, b_{n+1}^{(s-1)}, \dots, b_N^{(s-1)})$ is computed by following the algorithm proposed by Waggoner & Zha 2003. To facilitate this, following is defined:

-   $U_n = \text{chol}\Big(\bar{\nu}\Big(V_n\bar{S}^{-1}V_n'\Big)^{-1}\Big)$, where $U_n$ is a $r_n \times r_n$ upper-triangular matrix.

-   $w = \left[B_{0[-n.]}^{(s)}\right]_\perp$, where $w$ is a $1 \times N$ matrix.

-   $w_1 = wV_n'U_n'\cdot \Big( wV_n'U_n'V_nU_nw'\Big)^{-\frac{1}{2}}$, where $w_1$ is a $1 \times r_n$ vector.

-   $W_n=\begin{pmatrix} w_1' & w_{1\perp}' \end{pmatrix}$, where $W_n$ is a matrix of dimensions $r_n \times r_n$.

The $1 \times r_n$ matrix $\alpha_n$ can now be constructed by drawing the first element of $\alpha_n$ by following this procedure:

-   Draw $u \sim N(0_{\nu+1},{\bar{\nu}^{-1}I_{\nu+1}})$

-   Set $\alpha_{n[\cdot 1]} = \begin{cases}\sqrt{u'u} \text{ with probability 0.5}\\-\sqrt{u'u} \text{ with probability 0.5}\end{cases}$

The remaining $r_n-1$ elements of $\alpha_n$ can be drawn from $N(0_{r_n-1},\bar{\nu}^{-1}I_{r_n-1})$, after which the draw of the full conditional distribution of $b_n$ can be computed by $b_n^{(s)}\alpha_nW_nU_n$.

As already mentioned, these samples need to be normalized in order to ensure that a unique maximum is being found. I will not go into details of this procedure here, but rather refer to Waggoner & Zha (2003) for a rigorous outline.

## R Code Snippets

This section provides the R code behind the estimation procedure. In order to facilitate this, the following R functions are being used by the courtesy of Tomasz WoznÃ­ak.

The following function computes an orthogonal complement matrix to the input x, which is used in the **rgn()** function presented below.

```{r}
orthogonal.complement.matrix.TW = function(x){
  # x is a mxn matrix and m>n
  # the function returns a mx(m-n) matrix, out, that is an orthogonal complement of x, i.e.:
  # t(x)%*%out = 0 and det(cbind(x,out))!=0
  if( dim(x)[1] == 1 & dim(x)[2] == 2){
    x = t(x)
  }
  # x <- ifelse(dim(x)[1] == 1 && dim(x)[2] == 2, t(x), x)
  N     = dim(x)
  tmp   = qr.Q(qr(x, tol = 1e-10),complete=TRUE)
  out   = as.matrix(tmp[,(N[2]+1):N[1]])
  return(out)
}
```

The **rgn()** function simulates draws for $b_n$ from a $\mathcal{NGN}$ distribution

```{r}
rgn             = function(n,S.inv,nu,V,B0.initial){
  # This function simulates draws for the unrestricted elements 
  # of the conteporaneous relationships matrix of an SVAR model
  # from a generalized-normal distribution according to algorithm 
  # by Waggoner & Zha (2003, JEDC)
  # n     - a positive integer, the number of draws to be sampled
  # S     - an NxN positive definite matrix, a parameter of the generalized-normal distribution
  # nu    - a positive scalar, degrees of freedom parameter
  # V     - an N-element list, with fixed matrices
  # B0.initial - an NxN matrix, of initial values of the parameters
  
  N             = nrow(B0.initial)
  no.draws      = n
  
  B0            = array(NA, c(N,N,no.draws))
  B0.aux        = B0.initial
  
  for (i in 1:no.draws){
    for (n in 1:N){
      rn            = nrow(V[[n]])
      Un            = chol(nu*solve(V[[n]]%*%S.inv%*%t(V[[n]])))
      w             = t(orthogonal.complement.matrix.TW(t(B0.aux[-n,])))
      w1            = w %*% t(V[[n]]) %*% t(Un) / sqrt(as.numeric(w %*% t(V[[n]]) %*% t(Un) %*% Un %*% V[[n]] %*% t(w)))
      if (rn>1){
        Wn          = cbind(t(w1),orthogonal.complement.matrix.TW(t(w1)))
      } else {
        Wn          = w1
      }
      alpha         = rep(NA,rn)
      u             = rmvnorm(1,rep(0,nu+1),(1/nu)*diag(nu+1))
      alpha[1]      = sqrt(as.numeric(u%*%t(u)))
      if (runif(1)<0.5){
        alpha[1]    = -alpha[1]
      }
      if (rn>1){
        alpha[2:rn] = rmvnorm(1,rep(0,nrow(V[[n]])-1),(1/nu)*diag(rn-1))
      }
      bn            = alpha %*% Wn %*% Un
      B0.aux[n,]    = bn %*% V[[n]]
    }
    B0[,,i]         = B0.aux
  }
  
  return(B0)
}
```

The next function normalizes the matrix of the contemporaneous effects, $B_0$:

```{r}
normalization.wz2003  = function(B0,B0.hat.inv, Sigma.inv, diag.signs){
  # This function normalizes a matrix of contemporaneous effects
  # according to the algorithm by Waggoner & Zha (2003, JOE)
  # B0        - an NxN matrix, to be normalized
  # B0.hat    - an NxN matrix, a normalized matrix
  
  N                 = nrow(B0)
  K                 = 2^N
  distance          = rep(NA,K)
  for (k in 1:K){
    B0.tmp.inv      = solve(diag(diag.signs[k,]) %*% B0)
    distance[k]     = sum(
      unlist(
        lapply(1:N,
               function(n){
                 t(B0.tmp.inv - B0.hat.inv)[n,] %*%Sigma.inv %*% t(B0.tmp.inv - B0.hat.inv)[n,]
               }
        )))
  }
  B0.out            = diag(diag.signs[which.min(distance),]) %*% B0
  
  return(B0.out)
}
```

This function normalizes the output from the **rgn()** function, ensuring that we obtain a unique maximum

```{r}
normalize.Gibbs.output.parallel          = function(B0.posterior,B0.hat){
  # This function normalizes the Gibbs sampler output from function rgn
  # using function normalization.wz2003 
  # B0.posterior  - a list, output from function rgn
  # B0.hat        - an NxN matrix, a normalized matrix
  
  N                 = nrow(B0.hat)
  K                 = 2^N
  
  B0.hat.inv        = solve(B0.hat)
  Sigma.inv         = t(B0.hat)%*%B0.hat
  
  diag.signs        = matrix(NA,2^N,N)
  for (n in 1:N){
    diag.signs[,n]  = kronecker(c(-1,1),rep(1,2^(n-1)))
  }
  
  B0.posterior.n    = mclapply(1:dim(B0.posterior)[3],function(i){
    normalization.wz2003(B0=B0.posterior[,,i],B0.hat.inv, Sigma.inv, diag.signs)
  },mc.cores=1
  )
  B0.posterior.n  = simplify2array(B0.posterior.n)
  
  return(B0.posterior.n)
}
```

Lastly, a function for simulating the draws of the multivariate normal distribution of the autoregressive slope matrix, $B_+$, is needed

```{r}
rnorm.ngn       = function(B0.posterior,B,Omega){
  # This function simulates draws for the multivariate normal distribution
  # of the autoregressive slope matrix of an SVAR model
  # from a normal-generalized-normal distribution according to algorithm 
  # by Waggoner & Zha (2003, JEDC)
  # B0.posterior  - a list, output from function rgn
  # B             - an NxK matrix, a parameter determining the mean of the multivariate conditionally normal distribution given B0
  # Omega         - a KxK positive definite matrix, a covariance matrix of the multivariate normal distribution
  
  N             = nrow(B)
  K             = ncol(B)
  no.draws      = dim(B0.posterior)[3]
  L             = t(chol(Omega))
  
  Bp.posterior  = lapply(1:no.draws,function(i){
    Bp          = matrix(NA, N, K)
    for (n in 1:N){
      Bp[n,]    = as.vector(t(B0.posterior[n,,i] %*% B) + L%*%rnorm(K))
    }
    return(Bp)
  })
  Bp.posterior  = simplify2array(Bp.posterior)
  return(Bp.posterior)
}
```

Having set up all the necessary functions, I now simulate a bivariate random walk to produce artificial data.

```{r simulation of data}
#Simulation of a bivariate random walk
p.sim = 1
T.sim = 500
N.sim = 2
K.sim = 1 + N.sim*p.sim

Y.sim           = arima.sim(list(order = c(0,1,0)), n = T.sim + p.sim-1, mean = 0, sd =1)
for (i in 2:N.sim){
  Y.sim         = rbind(Y.sim, arima.sim(list(order = c(0,1,0)), n = T.sim + p.sim-1, mean = 0, sd = 1))
}

X.sim           = matrix(1,1,T.sim)
for (i in 1:p.sim){
  X.sim         = rbind(X.sim, Y.sim[,(p.sim+1-i):(ncol(Y.sim)-i)])
}
Y.sim           = Y.sim[,-p.sim]
```

Finally, the gibbs sampler for the basic model can be presented. Note that the priors have been set in regards to the specification from above. Further, I create the restriction matrix $V_n$, where one easily can see that a recursive structure is being implied in the system.

```{r gibbs sampler for basic model}
Gibbs.sampler.base <- function(p,Y,X,S1,S2){

  N       = nrow(Y)
  p       = p # calculate from X and Y (K and N)
  K       = 1+N*p
  S1      = S1
  S2      = S2
  kappa0  = 10
  kappa1  = 10
  kappa2  = 0.1
  
  priors   = list(
  B        = cbind(rep(0,N), diag(N), matrix(0, N, (p-1)*N)),
  Omega    = diag(c(kappa2,kappa1*((1:p)^(-2))%x%rep(1,N))),
  S        = kappa0*diag(N),
  nu       = N
  )
  
  # create the V matrices
  FF.V           = vector("list",N)
  for (n in 1:N){
  FF.V[[n]]   = cbind(diag(n),matrix(0,n,N-n))
  }

  # create initial B0 matrix
  B0.initial = matrix(0,N,N)
  for (n in 1:N){
  unrestricted    = apply(FF.V[[n]],2,sum)==1
  B0.initial[n,unrestricted] = rnorm(sum(unrestricted))
  }

  B0.posterior    <- array(NA,c(N,N,(S1+S2)))
  Bp.posterior    <- array(NA,c(N,(1+N*p),(S1+S2)))

  for (s in 1:(S1+S2)){

    # Computing posterior parameters
    Omega.inv      = solve(priors$Omega)
    Omega.post.inv = X%*%t(X) + Omega.inv
    Omega.post     = solve(Omega.post.inv)
    B.post         = (Y%*%t(X) + priors$B%*%Omega.inv) %*% Omega.post
    S.post         = Y%*%t(Y) + solve(priors$S) + priors$B%*%Omega.inv%*%t(priors$B) -   B.post%*%Omega.post.inv%*%t(B.post) 
    nu.post        = ncol(Y) + priors$nu

    # Use B0.initial for first iteration, otherwise the latest draw from B0.posterior
    if (s==1) {
      B0.s = B0.initial
    } else {
      B0.s = B0.posterior[,,s-1]
    }

    # sampling one draw B0 from the posterior distribution using Gibbs
    # rgn.function samples from a random conditional generalized normal distribution
    B0.tmp                  = rgn(n=1, S.inv=S.post, nu=nu.post, V=FF.V, B0.initial=B0.s)
    B0.posterior[,,s]       = B0.tmp[,,1]

    # sample one draw B+ from the normal conditional posterior
    Bp.tmp              = rnorm.ngn(B0.tmp, B=B.post,Omega=Omega.post)
    Bp.posterior[,,s]   = Bp.tmp[,,1]
  }
  # END OF GIBBS
  #Discard first S1 draws
  B0.posterior <- B0.posterior[,,(S1+1):(S1+S2)]
  Bp.posterior <- Bp.posterior[,,(S1+1):(S1+S2)]

  #normalisation of B0.posterior and Bp.posterior
  B0.hat             = diag(sign(diag(B0.tmp[,,1]))) %*% B0.tmp[,,1]

  B0.posterior.N    <- array(NA,c(N,N,S2))
  Bp.posterior.N    <- array(NA,c(N,(1+N*p),S2))

  B0.posteror.N.tmp      =  normalize.Gibbs.output.parallel(B0.posterior,B0.hat=B0.hat)
  for (s in 1:S2){
    B0.posterior.N[,,s]    = B0.posteror.N.tmp[,,s]
    Bp.posterior.N[,,s]    = B0.posterior.N[,,s]%*%solve(B0.posterior[,,s])%*%Bp.posterior[,,s]
  }

  return(list(B0.posterior.N = B0.posterior.N,
              Bp.posterior.N = Bp.posterior.N))
}
```

```{r baseline show}
#| echo: false
#| message: false
#| warning: false
# Run Basic function
Basic.sim = Gibbs.sampler.base(p=1,Y=Y.sim,X=X.sim,S1=100,S2=1000)
apply(Basic.sim$B0.posterior.N,1:2,mean)
apply(Basic.sim$Bp.posterior.N,1:2,mean)
```

Since a bivariate random walk was simulated, the $B_0$ matrix should be an identity matrix. Further, the first column of $B_+$ should be zero and the matrix $B_+[,2:3]$ should also be an identity matrix. This is also approximately the case, which indicates the estimation procedure is correct.

## Extended Model

As part of my extended model, I estimate the shrinkage parameters $\kappa_0$ and $\kappa_+$. Estimating those parameters instead of just setting them might lead to improved efficiency and reliability. By remembering how $\kappa_0$ and $\kappa_+$ affected the posterior parameters in the basic model, we can now write up the kernel for the new conjugate-prior up for the extended model:

```{=tex}
\begin{align}
p(B_+,B_0|Y,X) &\propto L(B_+,B_0|Y,X)p(B_+,B_0|\kappa_0,\kappa_+)p(\kappa_0)p(\kappa_+)\\
\end{align}
```
```{=tex}
\begin{align}
p(\kappa_0|\underline{s}_{\kappa_0},\underline{\nu}_{\kappa_0}) &\sim \mathcal{IG}2(\underline{s}_{\kappa_0},\underline{\nu}_{\kappa_0})\\
p(\kappa_+|\underline{s}_{\kappa_+},\underline{\nu}_{\kappa_+}) &\sim \mathcal{IG}2(\underline{s}_{\kappa_+},\underline{\nu}_{\kappa_+})
\end{align}
```
The full-conditional posterior distribution of $\kappa_0$ can be found to be:

```{=tex}
\begin{align}
p(\kappa_0|Y,X,B_0,B_+,\kappa_+) &\propto p(B_0|\kappa_0)p(\kappa_0)\\
&\propto \prod_{n=1}^N\kappa_0^{\frac{r_n}{2}}\exp \left\{  -\frac{1}{2}\sum_{n=1}^N b_nV_n(\kappa_0 I_{r_n})^{-1}V_n'b_n'\right\}\kappa_0^{-\frac{\underline{\nu}_{\kappa_0}+2}{2}}\exp \left\{  -\frac{1}{2} \frac{\underline{s}_{\kappa_0}}{\kappa_0}\right\}\\
&\propto \prod_{n=1}^N\kappa_0^{\frac{r_n}{2}} \exp \left\{  -\frac{1}{2}\frac{1}{\kappa_0}\sum_{n=1}^N b_nV_n I_{r_n}V_n'b_n'\right\}\kappa_0^{-\frac{\underline{\nu}_{\kappa_0}+2}{2}}\exp \left\{  -\frac{1}{2} \frac{\underline{s}_{\kappa_0}}{\kappa_0}\right\}
\end{align}
```
Since $\underline{S}=\kappa_0I_N$ and $b_n|\kappa_0 \sim \mathcal{N}(0,\kappa_0(V_nV_n')^{-1})=\mathcal{N}_{r_n}(0_{r_n},\kappa_0I_{r_n})$. By collecting the components in an appropriate way, the full-conditional posterior can be written as:

```{=tex}
\begin{align}
p(\kappa_0|Y,X,B_0,B_+,\kappa_+) &\propto \kappa_0^{-\frac{\bar{\nu}_{\kappa_0}+2}{2}} \exp \left\{ -\frac{1}{2}\frac{\bar{s}_{\kappa_0}}{\kappa_0} \right\}\\
\bar{s}_{\kappa_0} &= \underline{s}_{\kappa_0}+\sum_{n=1}^N b_nV_nI_{r_n}V_n'b_n'\\
\bar{\nu}_{\kappa_0} &= \underline{\nu}_{\kappa_0}+\sum_{n=1}^N r_n
\end{align}
```
The same procedure goes for the full-conditional posterior distribution of $\kappa_+$:

```{=tex}
\begin{align}
p(\kappa_+|Y,X,B_0,B_+,\kappa_0) &\propto p(B_+|B_0,\kappa_+)p(\kappa_+)\\
&\propto \kappa_+^{\frac{K}{2}}\exp \left\{-\frac{1}{2}\frac{1}{\kappa_+} \sum_{n=1}^N (B_n-b_nV_n\underline{B})\underline{\Omega}^{-1}(B_n-b_nV_n\underline{B})'\right\}\kappa_+^{-\frac{\underline{\nu}_{\kappa_+}+2}{2}}\exp \left\{  -\frac{1}{2} \frac{\underline{s}_{\kappa_+}}{\kappa_+}\right\}
\end{align}
```
Since $B_n|b_n,\kappa_+ \sim \mathcal{N}_{N+1}(b_nV_n\underline{B},\kappa_+\Omega)$

Which further can be derived to:

```{=tex}
\begin{align}
p(\kappa_+|Y,X,B_0,B_+,\kappa_0) &\propto \kappa_+^{-\frac{\bar{\nu}_{\kappa_+}+2}{2}} \exp \left\{ -\frac{1}{2}\frac{\bar{s}_{\kappa_+}}{\kappa_+} \right\}\\
\bar{s}_{\kappa_+} &= \underline{s}_{\kappa_+}+\sum_{n=1}^N (B_n-b_nV_n\underline{B})\underline{\Omega}^{-1}(B_n-b_nV_n\underline{B})'\\
\bar{\nu}_{\kappa_+} &= \underline{\nu}_{\kappa_+}+NK
\end{align}
```
This facilitates writing up the gibbs sampler for the extended model. Note that new priors have to be set compared to the basic model, as the hyperparameters are now being estimated:

```{r gibbs sampler for extended model}
Gibbs.sampler.extended <- function(p,Y,X,S1,S2){
  
  N       = nrow(Y)
  p       = p # calculate from X and Y (K and N)
  K       = 1+N*p
  S1      = S1
  S2      = S2
  
  #set new priors
  
  priors = list(
  B         = cbind(rep(0,N), diag(N), matrix(0, N, (p-1)*N)),
  Omega     = diag(c(10,((1:p)^(-2))%x%rep(1,N))),
  S         = diag(N),
  nu        = N,
  S.kappa0  = 1,
  nu.kappa0 = 1,
  S.kappa1  = 1,
  nu.kappa1 = 1
  )
  
  # create the V matrices
  FF.V           = vector("list",N)
  for (n in 1:N){
  FF.V[[n]]   = cbind(diag(n),matrix(0,n,N-n))
  }

  # create initial B0 matrix
  B0.initial = matrix(0,N,N)
  for (n in 1:N){
  unrestricted    = apply(FF.V[[n]],2,sum)==1
  B0.initial[n,unrestricted] = rnorm(sum(unrestricted))
  }
  
  kappa0          <- rep(NA, S1 + S2)
  kappa1          <- rep(NA, S1 + S2)
  B0.posterior    <- array(NA,c(N,N,(S1+S2)))
  Bp.posterior    <- array(NA,c(N,(1+N*p),(S1+S2)))  
  
  kappa0[1] <- 1
  kappa1[1] <- 1 
  
  for (s in 1:(S1+S2)){
    
    # Computing posterior parameters
    # Only Omega, B and S depend on kappa1
    #cat("\n kappa0: ", kappa0[s], "kappa1: ", kappa1[s])
    
    Omega.inv      = solve(priors$Omega)
    Omega.post.inv = X%*%t(X) + (1/kappa1[s])*Omega.inv
    Omega.post     = solve(Omega.post.inv)
    B.post         = (Y%*%t(X) + priors$B%*%((1/kappa1[s])*Omega.inv)) %*% Omega.post
    S.post         = Y%*%t(Y) + (1/kappa0[s])*solve(priors$S) + priors$B%*%((1/kappa1[s])*Omega.inv)%*%t(priors$B) - B.post%*%Omega.post.inv%*%t(B.post) 
    nu.post        = ncol(Y) + priors$nu
    
    # Use B0.initial for first iteration, otherwise the latest draw from B0.posterior
    
    if (s==1) {
      B0.s = B0.initial
    } else {
      B0.s = B0.posterior[,,s-1]
    }
    
    # sampling one draw B0 from the posterior distribution using Gibbs  
    # rgn.function samples from a random conditional generalized normal distribution
    
    B0.tmp                  = rgn(n=1, S.inv=S.post, nu=nu.post, V=FF.V, B0.initial=B0.s)
    B0.posterior[,,s]       = B0.tmp[,,1]
    
    #cat("B0: ", B0.posterior[,,s],"\n")
    
    # sample one draw B+ from the normal conditional posterior
    Bp.tmp              = rnorm.ngn(B0.tmp, B=B.post,Omega=Omega.post)
    Bp.posterior[,,s]   = Bp.tmp[,,1]
    
    #compute posterior for the shrinkage parameter S.kappa and nu
    S.kappa0.post = priors$S.kappa0 + sum(B0.posterior[,,s]^2)
    
    # S.kappa.post = sum(priors$S.kappa + (B0.posterior[i,,s]-priors$B[i,])%*%Omega.inv%*%t(B0.posterior[i,,s]-priors$B[i,]))
    
    # nu.kappa0.post  = priors$nu.kappa0 + i #change outside of loop count number rows (otherwise make as a sum of i's)
    nu.kappa0.post  = priors$nu.kappa0 + sum(unlist(lapply(FF.V, nrow)))
    
    S.kappa1.post   = priors$S.kappa1
    for (i in 1:N){
      S.kappa1.post = S.kappa1.post + (Bp.posterior[i,,s]- B0.posterior[i,,s]%*%priors$B)%*%Omega.inv%*%t(Bp.posterior[i,,s]-B0.posterior[i,,s]%*%priors$B)
    }
    # S.kappa.post = sum(priors$S.kappa + (B0.posterior[i,,s]-priors$B[i,])%*%Omega.inv%*%t(B0.posterior[i,,s]-priors$B[i,]))
    
    nu.kappa1.post  = priors$nu.kappa1 + N*(p*N+1) 
    
    
    #Draw kappa0 and kappa1 from IG2
    if (s != S1+S2) {
      kappa0[s+1]    = S.kappa0.post / rchisq(1, df=nu.kappa0.post) 
      kappa1[s+1]    = S.kappa1.post / rchisq(1, df=nu.kappa1.post) 
    }
  }
  
  #Discard first S1 draws
  
  B0.posterior <- B0.posterior[,,(S1+1):(S1+S2)]
  Bp.posterior <- Bp.posterior[,,(S1+1):(S1+S2)]
  kappa0       <- kappa0[(S1+1):(S1+S2)]
  kappa1       <- kappa1[(S1+1):(S1+S2)]
  
  #normalisation of B0.posterior and Bp.posterior
  
  B0.hat             = diag(sign(diag(B0.tmp[,,1]))) %*% B0.tmp[,,1]
  # t(chol((nu.post-N)*S.post))# normalisation using this B0.hat should work
  
  B0.posterior.N    <- array(NA,c(N,N,S2))
  Bp.posterior.N    <- array(NA,c(N,(1+N*p),S2))
  
    B0.posteror.N.tmp      =  normalize.Gibbs.output.parallel(B0.posterior,B0.hat=B0.hat)
  for (s in 1:S2){
    B0.posterior.N[,,s]    = B0.posteror.N.tmp[,,1]
    Bp.posterior.N[,,s]    = B0.posterior.N[,,s]%*%solve(B0.posterior[,,s])%*%Bp.posterior[,,s]
  }
  
  return(list(B0.posterior.N = B0.posterior.N,
              Bp.posterior.N = Bp.posterior.N,
              kappa0 = kappa0,
              kappa1 = kappa1))
} 
```

=======
Where $A_i=B_0^{-1}B_i$ and $u_t=B_0^{-1}w_t$. It is assumed that $u_t|Y_{t-1}\sim\;iid(\textbf{0}_N,\Sigma)$, which allows us to denote $\Sigma = B_0^{-1} (B_0^{-1})'$. In order to reconstruct $B_0^{-1}$ and thereby identify the SVAR model restrictions on the matrix need to imposed. As $B_0^{-1}$ consists of $K(K+1)/2$ variables, at least $K(K-1)/2$ restrictions need to be imposed. This can be done in multiple ways. In this project I will impose zero exclusion restrictions on $B_0^{-1}$ by either implying recursive or a non-recursive system between the variables. It is important to note that if I choose a recursive system the ordering of $y_t$ is crucial and is therefore still subject to change.

The estimation output I will interpret to measure how money supply shocks affect asset prices and inflation will be impulse response functions (IRFs) and forecast error variance decomposition (FEVDs). IRFs measures the dynamic response of a variable to a given shock, while FEVDs are a measure for the contribution of different shocks to the forecast error variance of a certain variable.

# Estimation Procedure

The estimation procedure in this paper is based on the Markov Chain Monte Carlo (MCMC) Gibbs sampler algorithm presented in @waggoner2003gibbs, since I will make use of exclusion restrictions to identify the SVAR model. 

## Basic Model

First, I redefine the model presented in the previous section to the following:

```{=tex}
\begin{align}
B_0y_t &= b_0 + B_1y_{t-1} + \dots + B_py_{t-p} u_t\\
       &= B_+ x_t + u_t
\end{align}
```
Where $B_+=\big[b_0\;B_1\;\dots\;B_p\big]$ and $x_t=\big[1\;y_{t-1}'\;\dots\;y_{t-p}'\big]$. 
As $B_0$ is the structural matrix the exclusion restrictions will be set on its rows such that $B_0=\left[b_1V_1\;\dots\;b_NV_N\right]'$ holds, where $B_{0[n\cdot]}=b_n\;V_n$ and represents the $n$th row of $B_0$. The dimension of $b_n$ is $1\times r_n$ and is a vector of the unrestricted elements of the $n$th row of $B_0$. The matrix $V_n$ is of dimension $r_n\times N$ and consists only of ones and zeroes since it is the restriction matrix. Now the structural model can be written equation-by-equation in the following way:

```{=tex}
\begin{align}
b_nV_ny_t &= B_nx_t + u_{n.t}\\
u_{n.t}   &\sim \mathcal{N}(0,1)
\end{align}
```
Which subsequently can be rewritten in matrix form as:

```{=tex}
\begin{align}
b_nV_nY &= B_nX+ U_n\\
U_n   &\sim \mathcal{N}(0_T,I_T)
\end{align}
```
where $\underset{(N \times T)}{Y}=\begin{pmatrix}
    y_1, \dots , y_T
\end{pmatrix}$, $\underset{(K \times T)}{X}=\begin{pmatrix}
    x_1, \dots , x_T
\end{pmatrix}$, $\underset{(1 \times T)}{U_n}=\begin{pmatrix}
    u_{n.1}, \dots , u_{n.T}
\end{pmatrix}$ and $\underset{(1 \times K)}{B_n}= B_{+[n.]}$.

For convenience the likelihood function of $B_0$ and $B_+$ given data can be written as a $\mathcal{NGN}$ distribution:

```{=tex}
\begin{align}
L(B_+,B_0 | Y, X) \propto |\det(B_0)|^T \exp \left\{-\frac{1}{2} \sum_{n=1}^N (b_nV_nY-B_nX)(b_nV_nY-B_nX)'  \right\}
\end{align}
```
Moving to the prior distribution, the $\mathcal{NGN}$ distribution is being used as a natural-conjugate prior. Therefore, I define $p(B_+,B_0)\sim \mathcal{NGN}(\underline{B}, \underline{\Omega}, \underline{S}, \underline{\nu})$, where the following holds:

```{=tex}
\begin{align}
p(B_+,B_0)&=\left(\prod_{n=1}^N p(B_n|b_n)\right)p(b_1,\dots,b_n)\\
p(B_n|b_n)&\sim \mathcal{N}_K (b_nV_n\underline{B},\underline{\Omega})\\
p(b_1,\dots,b_n) &\propto |\det (B_0)|^{\underline{\nu}-N} \exp \left\{-\frac{1}{2}\sum_{n=1}^Nb_nV_n\underline{S}^{-1}V_n'b_n'\right\}
\end{align}
```
Which results in the following kernel of the natural-conjugate prior distribution:

```{=tex}
\begin{align}
|\det(B_0)|^{\underline{\nu}-N} \exp \left\{-\frac{1}{2} \sum_{n=1}^N b_nV_n\underline{S}^{-1}V_n'B_n'\right\} \times \exp \left\{-\frac{1}{2} \sum_{n=1}^N (B_n-b_nV_n\underline{B})\underline{\Omega}^{-1}(B_n-b_nV_n\underline{B})'\right\}
\end{align}
```
For the prior parameters the Minnesota prior parameters are being exploited:

```{=tex}
\begin{align}
\underline{B} &= \left[0_{N\times 1}\;I_N\;0_{N\times(p-1)N}\right]\\
\underline{\Omega} &= \text{diag} \left(\left[\kappa_2\;\kappa_1(\textbf{p}^{-2}\otimes I_N')\right)\right]\\
\underline{S} &= \kappa_0I_N\\
\underline{\nu} &= N
\end{align}
```

This enables us to derive the posterior distribution:

```{=tex}
\begin{align}
p(B_+,B_0|Y,X) &\propto L(B_+,B_0|Y,X)p(B_+,B_0)\\
               &\propto |\det(B_0)|^T \exp \left\{-\frac{1}{2} \sum_{n=1}^N (b_nV_nY-B_nX)(b_nV_nY-B_nX)'  \right\}\\
               &\times |\det(B_0)|^{\underline{\nu}-N} \exp \left\{-\frac{1}{2} \sum_{n=1}^N b_nV_n\underline{S}^{-1}V_n'B_n'\right\} \\ &\times \exp \left\{-\frac{1}{2} \sum_{n=1}^N (B_n-b_nV_n\underline{B})\underline{\Omega}^{-1}(B_n-b_nV_n\underline{B})'\right\}
\end{align}
```

By performing appropriate operations this can be expressed more densely the following way:

```{=tex}
\begin{align}
p(B_+,B_0|Y,X) &\propto |\det(B_0)|^{T+\underline{\nu}-N} \exp \left\{-\frac{1}{2} \sum_{n=1}^N (B_n-b_nV_n\bar{B})\bar{\Omega}^{-1}(B_n-b_nV_n\bar{B})'+b_nV_n\bar{S}^{-1}V_n'b_n'\right\}
\end{align}
```

Leading to the following posterior parameters:

```{=tex}
\begin{align}
p(B_+,B_0|Y,X) &\sim \mathcal{NGN}(\bar{B},\bar{\Omega},\bar{S},\bar{\nu})\\
\bar{\Omega}&=\left[XX'+\underline{\Omega}^{-1}\right]^{-1}\\
\bar{B}&=\left[YX'+\underline{B\Omega}^{-1}\right]\bar{\Omega}\\
\bar{S}&=\left[YY'+\underline{S}^{-1}+\underline{B\Omega}^{-1}\underline{B}'-\bar{B}\bar{\Omega}^{-1}\bar{B}'\right]^{-1}\\
\bar{\nu}&= T+\underline{\nu}
\end{align}
```

## The Gibbs Sampler

Having derived the posterior parameters, the gibbs sampler can now be scrutinized. As already outlined, the sampler is based on the $\mathcal{NGN}$ distribution. Further, the algorithm is divided into two steps. First, $B_0$ is drawn $S1+S2$ times from
\begin{gather*}
    p(b_n | Y, X, b_1, \dots, b_{n-1}, b_{n+1}, \dots, b_N) 
\end{gather*} 
From which we get the posterior samples $\{b_1^{(s)},\dots, b_N^{(s)}\}^{S}_{s=1}$. Next step is to normalize these samples, so we subsequently can sample $B_n$ directly for each draw of $b_n^{(s)}$ from $p(B_n|Y,X,b_n)$. Based on this, the posterior draws $\left\{B_+^{(s)},B_0^{(s)}\right\}_{s=1}^{S1+S2}$ can be returned.

The gibbs sampler for
$b_n^{(s)} \sim p(b_n | Y, X, b_1^{(s)}, \dots, b_{n-1}^{(s)}, b_{n+1}^{(s-1)}, \dots, b_N^{(s-1)})$
is computed by following the algorithm proposed by @waggoner2003gibbs. To facilitate this, following is defined:

-   $U_n = \text{chol}\Big(\bar{\nu}\Big(V_n\bar{S}^{-1}V_n'\Big)^{-1}\Big)$,
    where $U_n$ is a $r_n \times r_n$ upper-triangular matrix.
    
-   $w = \left[B_{0[-n.]}^{(s)}\right]_\perp$, where $w$ is a $1 \times N$ matrix. 

-   $w_1 = wV_n'U_n'\cdot \Big( wV_n'U_n'V_nU_nw'\Big)^{-\frac{1}{2}}$,
    where $w_1$ is a $1 \times r_n$ vector.

-   $W_n=\begin{pmatrix} w_1' & w_{1\perp}' \end{pmatrix}$, where $W_n$
    is a matrix of dimensions $r_n \times r_n$.

The $1 \times r_n$ matrix $\alpha_n$ can now be constructed by drawing the first element of $\alpha_n$ by following this procedure:

-   Draw $u \sim N(0_{\nu+1},{\bar{\nu}^{-1}I_{\nu+1}})$

-   Set $\alpha_{n[\cdot 1]} = \begin{cases}\sqrt{u'u} \text{ with probability 0.5}\\-\sqrt{u'u} \text{ with probability 0.5}\end{cases}$ 
    
The remaining $r_n-1$ elements of $\alpha_n$ can be drawn from $N(0_{r_n-1},\bar{\nu}^{-1}I_{r_n-1})$, after which the draw of the full conditional distribution of $b_n$ can be computed by $b_n^{(s)}\alpha_nW_nU_n$.

As already mentioned, these samples need to be normalized in order to ensure that a unique maximum is being found. I will not go into details of this procedure here, but rather refer to @waggoner2003likelihood for a rigorous outline.

## R Code Snippets

This section provides the R code behind the estimation procedure. In order to facilitate this, the following R functions are being used by the courtesy of Tomasz WoÅºniak.

The following function computes an orthogonal complement matrix to the input x, which is used in the **rgn()** function presented below.
```{r}
orthogonal.complement.matrix.TW = function(x){
  # x is a mxn matrix and m>n
  # the function returns a mx(m-n) matrix, out, that is an orthogonal complement of x, i.e.:
  # t(x)%*%out = 0 and det(cbind(x,out))!=0
  if( dim(x)[1] == 1 & dim(x)[2] == 2){
    x = t(x)
  }
  # x <- ifelse(dim(x)[1] == 1 && dim(x)[2] == 2, t(x), x)
  N     = dim(x)
  tmp   = qr.Q(qr(x, tol = 1e-10),complete=TRUE)
  out   = as.matrix(tmp[,(N[2]+1):N[1]])
  return(out)
}
```

The **rgn()** function simulates draws for $b_n$ from a $\mathcal{NGN}$ distribution
```{r}
rgn             = function(n,S.inv,nu,V,B0.initial){
  # This function simulates draws for the unrestricted elements 
  # of the conteporaneous relationships matrix of an SVAR model
  # from a generalized-normal distribution according to algorithm 
  # by Waggoner & Zha (2003, JEDC)
  # n     - a positive integer, the number of draws to be sampled
  # S     - an NxN positive definite matrix, a parameter of the generalized-normal distribution
  # nu    - a positive scalar, degrees of freedom parameter
  # V     - an N-element list, with fixed matrices
  # B0.initial - an NxN matrix, of initial values of the parameters
  
  N             = nrow(B0.initial)
  no.draws      = n
  
  B0            = array(NA, c(N,N,no.draws))
  B0.aux        = B0.initial
  
  for (i in 1:no.draws){
    for (n in 1:N){
      rn            = nrow(V[[n]])
      Un            = chol(nu*solve(V[[n]]%*%S.inv%*%t(V[[n]])))
      w             = t(orthogonal.complement.matrix.TW(t(B0.aux[-n,])))
      w1            = w %*% t(V[[n]]) %*% t(Un) / sqrt(as.numeric(w %*% t(V[[n]]) %*% t(Un) %*% Un %*% V[[n]] %*% t(w)))
      if (rn>1){
        Wn          = cbind(t(w1),orthogonal.complement.matrix.TW(t(w1)))
      } else {
        Wn          = w1
      }
      alpha         = rep(NA,rn)
      u             = rmvnorm(1,rep(0,nu+1),(1/nu)*diag(nu+1))
      alpha[1]      = sqrt(as.numeric(u%*%t(u)))
      if (runif(1)<0.5){
        alpha[1]    = -alpha[1]
      }
      if (rn>1){
        alpha[2:rn] = rmvnorm(1,rep(0,nrow(V[[n]])-1),(1/nu)*diag(rn-1))
      }
      bn            = alpha %*% Wn %*% Un
      B0.aux[n,]    = bn %*% V[[n]]
    }
    B0[,,i]         = B0.aux
  }
  
  return(B0)
}
```

The next function normalizes the matrix of the contemporaneous effects, $B_0$:

```{r}
normalization.wz2003  = function(B0,B0.hat.inv, Sigma.inv, diag.signs){
  # This function normalizes a matrix of contemporaneous effects
  # according to the algorithm by Waggoner & Zha (2003, JOE)
  # B0        - an NxN matrix, to be normalized
  # B0.hat    - an NxN matrix, a normalized matrix
  
  N                 = nrow(B0)
  K                 = 2^N
  distance          = rep(NA,K)
  for (k in 1:K){
    B0.tmp.inv      = solve(diag(diag.signs[k,]) %*% B0)
    distance[k]     = sum(
      unlist(
        lapply(1:N,
               function(n){
                 t(B0.tmp.inv - B0.hat.inv)[n,] %*%Sigma.inv %*% t(B0.tmp.inv - B0.hat.inv)[n,]
               }
        )))
  }
  B0.out            = diag(diag.signs[which.min(distance),]) %*% B0
  
  return(B0.out)
}
```

This function normalizes the output from the **rgn()** function, ensuring that we obtain a unique maximum
```{r}
normalize.Gibbs.output.parallel          = function(B0.posterior,B0.hat){
  # This function normalizes the Gibbs sampler output from function rgn
  # using function normalization.wz2003 
  # B0.posterior  - a list, output from function rgn
  # B0.hat        - an NxN matrix, a normalized matrix
  
  N                 = nrow(B0.hat)
  K                 = 2^N
  
  B0.hat.inv        = solve(B0.hat)
  Sigma.inv         = t(B0.hat)%*%B0.hat
  
  diag.signs        = matrix(NA,2^N,N)
  for (n in 1:N){
    diag.signs[,n]  = kronecker(c(-1,1),rep(1,2^(n-1)))
  }
  
  B0.posterior.n    = mclapply(1:dim(B0.posterior)[3],function(i){
    normalization.wz2003(B0=B0.posterior[,,i],B0.hat.inv, Sigma.inv, diag.signs)
  },mc.cores=1
  )
  B0.posterior.n  = simplify2array(B0.posterior.n)
  
  return(B0.posterior.n)
}
```

Lastly, a function for simulating the draws of the multivariate normal distribution of the autoregressive slope matrix, $B_+$, is needed
```{r}
rnorm.ngn       = function(B0.posterior,B,Omega){
  # This function simulates draws for the multivariate normal distribution
  # of the autoregressive slope matrix of an SVAR model
  # from a normal-generalized-normal distribution according to algorithm 
  # by Waggoner & Zha (2003, JEDC)
  # B0.posterior  - a list, output from function rgn
  # B             - an NxK matrix, a parameter determining the mean of the multivariate conditionally normal distribution given B0
  # Omega         - a KxK positive definite matrix, a covariance matrix of the multivariate normal distribution
  
  N             = nrow(B)
  K             = ncol(B)
  no.draws      = dim(B0.posterior)[3]
  L             = t(chol(Omega))
  
  Bp.posterior  = lapply(1:no.draws,function(i){
    Bp          = matrix(NA, N, K)
    for (n in 1:N){
      Bp[n,]    = as.vector(t(B0.posterior[n,,i] %*% B) + L%*%rnorm(K))
    }
    return(Bp)
  })
  Bp.posterior  = simplify2array(Bp.posterior)
  return(Bp.posterior)
}
```

Having set up all the necessary functions, I now simulate a bivariate random walk to produce artificial data.

```{r}
#Simulation of data

p = 1
T = 500
N = 2
K = 1 + N*p

Y           = arima.sim(list(order = c(0,1,0)), n = T + p-1, mean = 0, sd =1)
for (i in 2:N){
  Y         = rbind(Y, arima.sim(list(order = c(0,1,0)), n = T + p-1, mean = 0, sd = 1))
}

X           = matrix(1,1,T)
for (i in 1:p){
  X         = rbind(X, Y[,(p+1-i):(ncol(Y)-i)])
}
Y           = Y[,-p]
artificialdata  = list(p = p, N = N, K = K, Y = Y, X = X)

#This model requires the Y and X matrix to be transposed 
# Y       = t(Y)
# X       = t(X)
```

Next, I set the priors in regards to the specification from above. Further, I create the restriction matrix $V_n$. Note, that I imply a recursive structure in the system. 

```{r}
# set the priors
kappa0     = 10
kappa1     = .1  
kappa2     = 10

priors     = list(
  B        = cbind(rep(0,N), diag(N), matrix(0, N, (p-1)*N)),
  Omega    = diag(c(kappa2,kappa1*((1:p)^(-2))%x%rep(1,N))),
  S        = kappa0*diag(N),
  nu       = N
)

# create the V matrices
FF.V           = vector("list",N)
for (n in 1:N){
  FF.V[[n]]   = cbind(diag(n),matrix(0,n,N-n))
}

# create initial B0 matrix
B0.initial = matrix(0,N,N)
for (n in 1:N){
  unrestricted    = apply(FF.V[[n]],2,sum)==1
  B0.initial[n,unrestricted] = rnorm(sum(unrestricted))
}
```

Finally, the gibbs sampler for the basic model can be presented

```{r}
Gibbs.sampler.base <- function(p,Y,X,priors,S1,S2, FF.V, B0.initial){

  N       = nrow(Y)
  p       = 1 # calculate from X and Y (K and N)
  K       = 1+N*p
  S1      = S1
  S2      = S2
  kappa0 = 10
  kappa1 = 10
  kappa2 = 0.1

  B0.posterior    <- array(NA,c(N,N,(S1+S2)))
  Bp.posterior    <- array(NA,c(N,(1+N*p),(S1+S2)))

  for (s in 1:(S1+S2)){

    # Computing posterior parameters
    Omega.inv      = solve(priors$Omega)
    Omega.post.inv = X%*%t(X) + Omega.inv
    Omega.post     = solve(Omega.post.inv)
    B.post         = (Y%*%t(X) + priors$B%*%Omega.inv) %*% Omega.post
    S.post         = Y%*%t(Y) + solve(priors$S) + priors$B%*%Omega.inv%*%t(priors$B) -   B.post%*%Omega.post.inv%*%t(B.post) 
    nu.post        = ncol(Y) + priors$nu

    # Use B0.initial for first iteration, otherwise the latest draw from B0.posterior
    if (s==1) {
      B0.s = B0.initial
    } else {
      B0.s = B0.posterior[,,s-1]
    }

    # sampling one draw B0 from the posterior distribution using Gibbs
    # rgn.function samples from a random conditional generalized normal distribution
    B0.tmp                  = rgn(n=1, S.inv=S.post, nu=nu.post, V=FF.V, B0.initial=B0.s)
    B0.posterior[,,s]       = B0.tmp[,,1]

    # sample one draw B+ from the normal conditional posterior
    Bp.tmp              = rnorm.ngn(B0.tmp, B=B.post,Omega=Omega.post)
    Bp.posterior[,,s]   = Bp.tmp[,,1]
  }
  # END OF GIBBS
  #Discard first S1 draws
  B0.posterior <- B0.posterior[,,(S1+1):(S1+S2)]
  Bp.posterior <- Bp.posterior[,,(S1+1):(S1+S2)]

  #normalisation of B0.posterior and Bp.posterior
  B0.hat             = diag(sign(diag(B0.tmp[,,1]))) %*% B0.tmp[,,1]

  B0.posterior.N    <- array(NA,c(N,N,S2))
  Bp.posterior.N    <- array(NA,c(N,(1+N*p),S2))

  B0.posteror.N.tmp      =  normalize.Gibbs.output.parallel(B0.posterior,B0.hat=B0.hat)
  for (s in 1:S2){
    B0.posterior.N[,,s]    = B0.posteror.N.tmp[,,s]
    Bp.posterior.N[,,s]    = B0.posterior.N[,,s]%*%solve(B0.posterior[,,s])%*%Bp.posterior[,,s]
  }

  return(list(B0.posterior.N = B0.posterior.N,
              Bp.posterior.N = Bp.posterior.N))
}
```

```{r baseline show}
#| echo: true
#| message: false
#| warning: false
# Run Basic function
Basic = Gibbs.sampler.base(p=1,Y=Y,X=X,priors=priors,S1=100,S2=10000, FF.V=FF.V, B0.initial=B0.initial)
apply(Basic$B0.posterior.N,1:2,mean)
apply(Basic$Bp.posterior.N,1:2,mean)
```

Since a bivariate random walk was simulated, the $B_0$ matrix should be an identity matrix. Further, the first column of $B_+$ should be zero and the matrix $B_+[,2:3]$ should also be an identity matrix. This is also approximately the case, which indicates the estimation procedure is correct.

## Extended Model

As part of my extended model, I estimate the shrinkage parameters $\kappa_0$ and $\kappa_+$. Estimating those parameters instead of just setting them might lead to improved efficiency and reliability. By remembering how $\kappa_0$ and $\kappa_+$ affected the posterior parameters in the basic model, we can now write up the kernel for the new conjugate-prior up for the extended model:

```{=tex}
\begin{align}
p(B_+,B_0|Y,X) &\propto L(B_+,B_0|Y,X)p(B_+,B_0|\kappa_0,\kappa_+)p(\kappa_0)p(\kappa_+)\\
\end{align}
```
```{=tex}
\begin{align}
p(\kappa_0|\underline{s}_{\kappa_0},\underline{\nu}_{\kappa_0}) &\sim \mathcal{IG}2(\underline{s}_{\kappa_0},\underline{\nu}_{\kappa_0})\\
p(\kappa_+|\underline{s}_{\kappa_+},\underline{\nu}_{\kappa_+}) &\sim \mathcal{IG}2(\underline{s}_{\kappa_+},\underline{\nu}_{\kappa_+})
\end{align}
```
The full-conditional posterior distribution of $\kappa_0$ can be found to be:

```{=tex}
\begin{align}
p(\kappa_0|Y,X,B_0,B_+,\kappa_+) &\propto p(B_0|\kappa_0)p(\kappa_0)\\
&\propto \prod_{n=1}^N\kappa_0^{\frac{r_n}{2}}\exp \left\{  -\frac{1}{2}\sum_{n=1}^N b_nV_n(\kappa_0 I_{r_n})^{-1}V_n'b_n'\right\}\kappa_0^{-\frac{\underline{\nu}_{\kappa_0}+2}{2}}\exp \left\{  -\frac{1}{2} \frac{\underline{s}_{\kappa_0}}{\kappa_0}\right\}\\
&\propto \prod_{n=1}^N\kappa_0^{\frac{r_n}{2}} \exp \left\{  -\frac{1}{2}\frac{1}{\kappa_0}\sum_{n=1}^N b_nV_n I_{r_n}V_n'b_n'\right\}\kappa_0^{-\frac{\underline{\nu}_{\kappa_0}+2}{2}}\exp \left\{  -\frac{1}{2} \frac{\underline{s}_{\kappa_0}}{\kappa_0}\right\}
\end{align}
```
Since $\underline{S}=\kappa_0I_N$ and $b_n|\kappa_0 \sim \mathcal{N}(0,\kappa_0(V_nV_n')^{-1})=\mathcal{N}_{r_n}(0_{r_n},\kappa_0I_{r_n})$. By collecting the components in an appropriate way, the full-conditional posterior can be written as:

```{=tex}
\begin{align}
p(\kappa_0|Y,X,B_0,B_+,\kappa_+) &\propto \kappa_0^{-\frac{\bar{\nu}_{\kappa_0}+2}{2}} \exp \left\{ -\frac{1}{2}\frac{\bar{s}_{\kappa_0}}{\kappa_0} \right\}\\
\bar{s}_{\kappa_0} &= \underline{s}_{\kappa_0}+\sum_{n=1}^N b_nV_nI_{r_n}V_n'b_n'\\
\bar{\nu}_{\kappa_0} &= \underline{\nu}_{\kappa_0}+\sum_{n=1}^N r_n
\end{align}
```
The same procedure goes for the full-conditional posterior distribution of $\kappa_+$:

```{=tex}
\begin{align}
p(\kappa_+|Y,X,B_0,B_+,\kappa_0) &\propto p(B_+|B_0,\kappa_+)p(\kappa_+)\\
&\propto \kappa_+^{\frac{K}{2}}\exp \left\{-\frac{1}{2}\frac{1}{\kappa_+} \sum_{n=1}^N (B_n-b_nV_n\underline{B})\underline{\Omega}^{-1}(B_n-b_nV_n\underline{B})'\right\}\kappa_+^{-\frac{\underline{\nu}_{\kappa_+}+2}{2}}\exp \left\{  -\frac{1}{2} \frac{\underline{s}_{\kappa_+}}{\kappa_+}\right\}
\end{align}
```
Since $B_n|b_n,\kappa_+ \sim \mathcal{N}_{N+1}(b_nV_n\underline{B},\kappa_+\Omega)$

Which further can be derived to:

```{=tex}
\begin{align}
p(\kappa_+|Y,X,B_0,B_+,\kappa_0) &\propto \kappa_+^{-\frac{\bar{\nu}_{\kappa_+}+2}{2}} \exp \left\{ -\frac{1}{2}\frac{\bar{s}_{\kappa_+}}{\kappa_+} \right\}\\
\bar{s}_{\kappa_+} &= \underline{s}_{\kappa_+}+\sum_{n=1}^N (B_n-b_nV_n\underline{B})\underline{\Omega}^{-1}(B_n-b_nV_n\underline{B})'\\
\bar{\nu}_{\kappa_+} &= \underline{\nu}_{\kappa_+}+NK
\end{align}
```

Before I code this up in R the new priors need to be set. Note, that is just set to a constant now.

```{r}
### Setting new priors

priors   = list(
  B        = cbind(rep(0,N), diag(N), matrix(0, N, (p-1)*N)),
  Omega    = diag(c(10,((1:p)^(-2))%x%rep(1,N))),
  S        = diag(N),
  nu       = N,
  S.kappa0  = 1,
  nu.kappa0 = 1,
  S.kappa1  = 1,
  nu.kappa1 = 1
)
```

Which facilitates writing up the gibbs sampler for the extended model:

```{r}
Gibbs.sampler.extended <- function(p,Y,X,priors,S1,S2, FF.V, B0.initial){
  
  N       = nrow(Y)
  p       = 1 # calculate from X and Y (K and N)
  K       = 1+N*p
  S1      = S1
  S2      = S2
  
  kappa0          <- rep(NA, S1 + S2)
  kappa1          <- rep(NA, S1 + S2)
  B0.posterior    <- array(NA,c(N,N,(S1+S2)))
  Bp.posterior    <- array(NA,c(N,(1+N*p),(S1+S2)))  
  
  kappa0[1] <- 1
  kappa1[1] <- 1 
  
  for (s in 1:(S1+S2)){
    
    # Computing posterior parameters
    # Only Omega, B and S depend on kappa1
    #cat("\n kappa0: ", kappa0[s], "kappa1: ", kappa1[s])
    
    Omega.inv      = solve(priors$Omega)
    Omega.post.inv = X%*%t(X) + (1/kappa1[s])*Omega.inv
    Omega.post     = solve(Omega.post.inv)
    B.post         = (Y%*%t(X) + priors$B%*%((1/kappa1[s])*Omega.inv)) %*% Omega.post
    S.post         = Y%*%t(Y) + (1/kappa0[s])*solve(priors$S) + priors$B%*%((1/kappa1[s])*Omega.inv)%*%t(priors$B) - B.post%*%Omega.post.inv%*%t(B.post) 
    nu.post        = ncol(Y) + priors$nu
    
    # Use B0.initial for first iteration, otherwise the latest draw from B0.posterior
    
    if (s==1) {
      B0.s = B0.initial
    } else {
      B0.s = B0.posterior[,,s-1]
    }
    
    # sampling one draw B0 from the posterior distribution using Gibbs  
    # rgn.function samples from a random conditional generalized normal distribution
    
    B0.tmp                  = rgn(n=1, S.inv=S.post, nu=nu.post, V=FF.V, B0.initial=B0.s)
    B0.posterior[,,s]       = B0.tmp[,,1]
    
    #cat("B0: ", B0.posterior[,,s],"\n")
    
    # sample one draw B+ from the normal conditional posterior
    Bp.tmp              = rnorm.ngn(B0.tmp, B=B.post,Omega=Omega.post)
    Bp.posterior[,,s]   = Bp.tmp[,,1]
    
    #compute posterior for the shrinkage parameter S.kappa and nu
    S.kappa0.post = priors$S.kappa0 + sum(B0.posterior[,,s]^2)
    
    # S.kappa.post = sum(priors$S.kappa + (B0.posterior[i,,s]-priors$B[i,])%*%Omega.inv%*%t(B0.posterior[i,,s]-priors$B[i,]))
    
    # nu.kappa0.post  = priors$nu.kappa0 + i #change outside of loop count number rows (otherwise make as a sum of i's)
    nu.kappa0.post  = priors$nu.kappa0 + sum(unlist(lapply(FF.V, nrow)))
    
    S.kappa1.post   = priors$S.kappa1
    for (i in 1:N){
      S.kappa1.post = S.kappa1.post + (Bp.posterior[i,,s]- B0.posterior[i,,s]%*%priors$B)%*%Omega.inv%*%t(Bp.posterior[i,,s]-B0.posterior[i,,s]%*%priors$B)
    }
    # S.kappa.post = sum(priors$S.kappa + (B0.posterior[i,,s]-priors$B[i,])%*%Omega.inv%*%t(B0.posterior[i,,s]-priors$B[i,]))
    
    nu.kappa1.post  = priors$nu.kappa1 + N*(p*N+1) 
    
    
    #Draw kappa0 and kappa1 from IG2
    if (s != S1+S2) {
      kappa0[s+1]    = S.kappa0.post / rchisq(1, df=nu.kappa0.post) 
      kappa1[s+1]    = S.kappa1.post / rchisq(1, df=nu.kappa1.post) 
    }
  }
  
  #Discard first S1 draws
  
  B0.posterior <- B0.posterior[,,(S1+1):(S1+S2)]
  Bp.posterior <- Bp.posterior[,,(S1+1):(S1+S2)]
  kappa0       <- kappa0[(S1+1):(S1+S2)]
  kappa1       <- kappa1[(S1+1):(S1+S2)]
  
  #normalisation of B0.posterior and Bp.posterior
  
  B0.hat             = diag(sign(diag(B0.tmp[,,1]))) %*% B0.tmp[,,1]
  # t(chol((nu.post-N)*S.post))# normalisation using this B0.hat should work
  
  B0.posterior.N    <- array(NA,c(N,N,S2))
  Bp.posterior.N    <- array(NA,c(N,(1+N*p),S2))
  
    B0.posteror.N.tmp      =  normalize.Gibbs.output.parallel(B0.posterior,B0.hat=B0.hat)
  for (s in 1:S2){
    B0.posterior.N[,,s]    = B0.posteror.N.tmp[,,1]
    Bp.posterior.N[,,s]    = B0.posterior.N[,,s]%*%solve(B0.posterior[,,s])%*%Bp.posterior[,,s]
  }
  
  return(list(B0.posterior.N = B0.posterior.N,
              Bp.posterior.N = Bp.posterior.N,
              kappa0 = kappa0,
              kappa1 = kappa1))
} 
```


>>>>>>> 41d47063279825e9a18740578892ff6012578e11
```{r extended show}
#| echo: false
#| message: false
#| warning: false
# Run Basic function
# Run function
<<<<<<< HEAD
extended.sim = Gibbs.sampler.extended(p=1,Y=Y.sim,X=X.sim,S1=100,S2=1000)

apply(extended.sim$B0.posterior.N,1:2,mean)
apply(extended.sim$Bp.posterior.N,1:2,mean)
```

Again, the estimation procedure for the extended model seems to be correct as well as the output aligns with the artificial data being a bivariate random walk.

I now turn to plotting the diagonal elements of $B_+[,2:3]$ in order to show whether the algorithm converges.

```{r showing converge plot}
#| echo: false
#| message: false
#| warning: false
#| label: fig-convergence-plot
#| fig-cap: "Convergence Plots"
# Plotting convergence
par(mfrow=c(1,2))
plot(extended.sim$Bp.posterior.N[,2,][1,],type='l',col="#660099",ylab="",xlab="",main=expression(B[+12]), lwd = 0.1)
plot(extended.sim$Bp.posterior.N[,3,][2,],type='l',col="#CC66CC",ylab="",xlab="",main=expression(B[+23]), lwd = 0.1)

```

This is indeed the case. The plots look like white noise processes as it fluctuates around the true value 1. This means the algorithm has converged.

# Empirical Investigation

In this section I will present the empirical results and provide the economic interpretation behind them. I will do so by computing the impulse responses for each variable from a shock to the money supply. Computations in R will be presented as well as plots of the impulse responses.

### Impulse Responses

By considering the VAR(1)-representation of the model we have:
```{=tex}
\begin{align}
Y_t &= \textbf{A}Y_{t-1} + E_t\\
&=E_t+\textbf{A}E_{t-1}+\textbf{A}^2E_{t-2}+...
\end{align}
```

Now, by making use of the matrix $J=\left[I_n\quad 0_{N\times N(p-1)}\right]$, the model can be transformed back to the VAR(p) representation:

```{=tex}
\begin{align}
y_t &=JY_t\\
&=JE_t+J\textbf{A}J'JE_{t-1}+J\textbf{A}^2J'JE_{t-2}+...\\
&=\varepsilon_t+J\textbf{A}J'\varepsilon_{t-1}+J\textbf{A}^2J'\varepsilon_{t-2}+...\\
\end{align}
```

Since we are dealing with a structural model it holds that $\varepsilon_t=Bu_t$. This implies the following:

```{=tex}
\begin{align}
y_t &=Bu_t+J\textbf{A}J'Bu_{t-1}+J\textbf{A}^2J'Bu_{t-2}+...\\
&=\Theta_0u_t+\Theta_1u_{t-1}+\Theta_2u_{t-2}+...\\
\end{align}
```
Where $\frac{\partial y_{t+i}}{\partial u_t}=\Theta_i$ is the IRF. See below the R function for computing these IRFS:

```{r computing IRFs}
IRF <- function(B0.posterior,Bp.posterior,p,h){
  
  h = h
  p = p

  #set colours for IRF plots
  mcxs1  = "#05386B"
  mcxs1.rgb   = col2rgb(mcxs1)
  mcxs1.shade1= rgb(mcxs1.rgb[1],mcxs1.rgb[2],mcxs1.rgb[3], alpha=120, maxColorValue=255)
 
  N  <- dim(B0.posterior)[2]
  S2 <- dim(B0.posterior)[3]
  K  <- 1+N*p
  
 B.posterior       = array(NA,c(N,N,S2))
 A.posterior       = array(NA,c(N,K,S2))
 for (s in 1:S2){
  B               = solve(B0.posterior[,,s])
  B.posterior[,,s]= B
  A.posterior[,,s]= B %*% Bp.posterior[,,s]
}

IRF.posterior     = array(NA,c(N,N,h+1,S2))
IRF.inf.posterior = array(NA,c(N,N,S2))
FEVD.posterior    = array(NA,c(N,N,h+1,S2))
J                 = cbind(diag(N),matrix(0,N,N*(p-1)))
for (s in 1:S2){
  A.bold          = rbind(A.posterior[,2:(1+N*p),s],cbind(diag(N*(p-1)),matrix(0,N*(p-1),N)))
  IRF.inf.posterior[,,s]          = J %*% solve(diag(N*p)-A.bold) %*% t(J) %*% B.posterior[,,s]
  A.bold.power    = A.bold
  for (i in 1:(h+1)){
    if (i==1){
      IRF.posterior[,,i,s]        = B.posterior[,,s]
    } else {
      IRF.posterior[,,i,s]        = J %*% A.bold.power %*% t(J) %*% B.posterior[,,s]
      A.bold.power                = A.bold.power %*% A.bold
    }
    # for (n in 1:N){
    #   for (nn in 1:N){
    #     FEVD.posterior[n,nn,i,s]  = sum(IRF.posterior[n,nn,1:i,s]^2)
    #   }
    # }
    # FEVD.posterior[,,i,s]         = diag(1/apply(FEVD.posterior[,,i,s],1,sum))%*%FEVD.posterior[,,i,s]
  }
}
FEVD.posterior    = 100*FEVD.posterior

IRF.posterior.mps = IRF.posterior[,4,,]
IRFs.k1           = apply(IRF.posterior.mps,1:2,median)
IRFs.inf.k1       = apply(IRF.posterior.mps,1,mean)
rownames(IRFs.k1) = colnames(y)

IRFs.k1.hdi    = apply(IRF.posterior.mps,1:2,hdi, credMass=0.68)
hh             = 1:(h+1)
pl = par(mfrow=c(3,2), mar=c(4,4.5,2,2),cex.axis=1.5, cex.lab=1.5)
for (n in 1:N){
  ylims     = range(IRFs.k1[n,hh],IRFs.k1.hdi[,n,hh])
  plot(hh,IRFs.k1[n,hh], type="l", ylim=ylims, axes=FALSE, xlab="", ylab=rownames(IRFs.k1)[n])
  if (n==5 | n==6){
    axis(1,c(1,13,25),c("","1y","2y"))
  } else {
    axis(1,c(1,13,25),c("","",""))
  }
  axis(2,c(ylims[1],0,ylims[2]),round(c(ylims[1],0,ylims[2]),3))
  polygon(c(hh,(h+1):1), c(IRFs.k1.hdi[1,n,hh],IRFs.k1.hdi[2,n,(h+1):1]), col=mcxs1.shade1,border=mcxs1.shade1)
  abline(h=0)
  lines(hh, IRFs.k1[n,hh],lwd=2,col=mcxs1)
}
}
```


### Identification Procedure

In order to being able to estimate a SVAR model the $B_0$ matrix has to be identified. As already briefly mentioned, I will imply a recursive ordering on the contemporaneous effects between my variables and therefore simply apply a Cholesky decomposition:

\begin{align*}
Bu_t=\begin{bmatrix}
    b_{11} & 0 & 0 & 0 & 0 & 0 \\
    b_{21} & b_{22} & 0 & 0 & 0 & 0 \\
    b_{31} & b_{32} & b_{33} & 0 & 0 & 0 \\
    b_{41} & b_{42} & b_{43} & b_{44} & 0 & 0 \\
    b_{51} & b_{52} & b_{53} & b_{54} & b_{55} & 0 \\
    b_{61} & b_{62} & b_{63} & b_{64} & b_{65} & b_{66} \\
\end{bmatrix}=\begin{bmatrix}
    u_{cpi_t}\\u_{ip_t}\\u_{ff_t}\\u_{m_t}\\u_{hp_t}\\u_{spx_t}
\end{bmatrix}
\end{align*}

These restrictions and the implied recursive system have to be economically justified. The first first equation in the system I interpret as a horizontal AS curve, while the second equation I interpret as a downward sloping AD curve. The third equation represents the monetary policy reaction function of the FED that reacts on both prices and output. Having money supply as the fourth variable is justifiable as the FED doesn't react on the money supply. Note that this variable is also the shock of interest in this research project. Last but not least, house prices and stock prices are in the end of the contemporaneous causal chain.

### Impulse responses plots

The IRFs are plotted for the basic and extended model below. Generally, the results are in line with economic theory and therefore have standard interpretations. A positive shock to the money supply can be seen as an expansionary and inflationary shock and should therefore be accompanied with an increase in CPI, real activity and asset prices. This is also indeed the case as all those variables react positively to the shock both contemporaneously and in subsequent periods. However, it is rather puzzling why the Fed Funds rate rises as well. Economic theory would suggest that interest rates would fall following a money supply shock, as an increased money stock makes liquidity relatively cheaper and therefore one would except the Fed Funds rate to fall.

There are no big differences between the plots when comparing between the basic model and the extended model. The most notable differences are that the confidence bands for the IRF of CPI are wider for the basic model than for the extended model, whereas it is the opposite case for the Fed Funds rate.

```{r run both models with data}
#| echo: false
#| message: false
#| warning: false

basic    = Gibbs.sampler.base(p=12,Y=Y,X=X,S1=100,S2=10000)
extended = Gibbs.sampler.extended(p=12,Y=Y,X=X,S1=100,S2=10000)
```
=======
extended = Gibbs.sampler.extended(p=1,Y=Y,X=X,priors=priors,S1=100,S2=10000, FF.V=FF.V, B0.initial=B0.initial)
>>>>>>> 41d47063279825e9a18740578892ff6012578e11

apply(extended$B0.posterior.N,1:2,mean)
apply(extended$Bp.posterior.N,1:2,mean)
```
Again, the estimation procedure for the extended model seems to be correct as well as the output aligns with the artificial data being a bivariate random walk.

I now turn to plotting the diagonal elements of $B_+[,2:3]$ in order to show whether the algorithm converges. 

<<<<<<< HEAD
```{r show IRFs basic}
#| echo: false
#| message: false
#| warning: false
#| label: fig-irf-basic-plot
#| fig-cap: "IRFs for basic model" 

IRF(B0.posterior=basic$B0.posterior.N,Bp.posterior=basic$Bp.posterior.N,p=12,h=24)
```

```{r show IRFs extended}
#| echo: false
#| message: false
#| warning: false
#| label: fig-irf-extended-plot
#| fig-cap: "IRFs for extended model" 
IRF(B0.posterior=extended$B0.posterior.N,Bp.posterior=extended$Bp.posterior.N,p=12,h=24)
```
=======
```{r showing converge plot}
#| echo: false
#| message: false
#| warning: false
# Plotting convergence
par(mfrow=c(1,2))
plot(extended$Bp.posterior.N[,2,][1,],type='l',col="#660099",ylab="",xlab="",main=expression(B[+12]), lwd = 0.1)
plot(extended$Bp.posterior.N[,3,][2,],type='l',col="#CC66CC",ylab="",xlab="",main=expression(B[+23]), lwd = 0.1)

```

This is indeed the case. The plots look like white noise processes as it fluctuates around the true value 1. This means the algorithm has converged.
>>>>>>> 41d47063279825e9a18740578892ff6012578e11
