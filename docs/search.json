[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Effect of Money Supply Shocks on Asset Prices and Inflation in the US Economy: A Bayesian SVAR Approach",
    "section": "",
    "text": "This research project is part of the assessment for the subject Macroeconometrics (ECOM90003) at the University of Melbourne held by Dr. Tomasz Woźniak."
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "The Effect of Money Supply Shocks on Asset Prices and Inflation in the US Economy: A Bayesian SVAR Approach",
    "section": "Introduction",
    "text": "Introduction\nDespite extensive Quantitative Easing (QE) programs following the financial crisis in 2008, inflation continued to remain well under the target level in many advanced economies. Rather, the increase in the money supply primarily seemed to inflate asset prices instead of the general price level and in other words struggled to stimulate aggregate demand. However, following the Covid-19 pandemic central banks quite drastically expanded their QE programs and thereby raised the money supply to unprecedented levels in response to the economic downturn. Among other factors such as supply chain issues, surging energy prices and massive fiscal stimulus, this has been one of the drivers behind inflation reaching double digits recently in many countries. This raises questions about the effectiveness of monetary policy in stimulating the economy and simultaneously controlling inflation. Another concern regarding QE mainly inflating asset prices, is that it can lead to financial instability in terms of increased risk of assets becoming overvalued and detached from the underlying fundamentals. This can lead to asset price bubbles and increase the amount of speculation among investors. It is therefore crucial for both policy makers and investors to understand the mechanisms through which a money supply shock affects different economic variables such as asset prices and inflation in light of economic and financial stability.\nThe research objective and question\nThe objective of this research project is to investigate the effects of a money supply shock on asset prices and inflation in the US economy. So the research question is how does money supply affect asset prices and inflation and what are the implications for monetary policy and financial stability?\nThe structure of this research project\nThis research project is divided into several subsections. Throughout the report I will present three different models, which I define as the basic model, extended model and the model with \\(t\\)-distributed errors. In the basic model the hyperparameters for the prior distribution are being set exogenously, whereas they are being estimated in the extended model. For the third model, the error term is modelled to follow a \\(t\\)-distribution. I will start out by presenting the data and its properties. Afterwards I present the estimation procedure and the corresponding R-code, which I test on a bivariate random walk to validate that the code is running correctly. Next, I use the actual data and present the empirical results and interpretation."
  },
  {
    "objectID": "index.html#data",
    "href": "index.html#data",
    "title": "The Effect of Money Supply Shocks on Asset Prices and Inflation in the US Economy: A Bayesian SVAR Approach",
    "section": "Data",
    "text": "Data\nIn this section I present the data I use for this research project by plotting the time series and analyze its properties.\n\nChoice of variables\nGiven my focus on the relationship between money supply, asset prices and inflation, a measure for those three variables are needed. As a measure for money supply the M1 aggregate is chosen as it serves as a good proxy for the availability of liquidity in the economy. As measures for asset prices, both stock prices and house prices are included. These two types of assets are big components of the total assets in the US economy and provide a way to investigate the transmission mechanism of money supply shocks to asset prices and the real economy. Further, the CPI is chosen as it is commonly used to construct the so-called headline inflation. Moreover, the effective fed funds rate and industrial production in the US is included. The effective fed funds rate is the rate at which banks lend and borrow funds from each other overnight and is obviously heavily influenced by the actual fed funds rate. Industrial production is a measure for monthly US real activity and is chosen since actual GDP data is only available quarterly. These two variables are important to include as they play a crucial role in the relationship between money supply, asset prices and inflation and therefore serve as control variables. Thus, I have the following six variables for the US economy in my SVAR model:\n\n\\(M_t\\): M1 aggregate from FRED Database\n\\(SPX_t\\): SP500 index from Yahoo Finance\n\\(HP_t\\): S&P/Case-Shiller U.S. National Home Price Index from FRED Database\n\\(CPI_t\\): Consumer Price Index: All Items for the US from FRED Database\n\\(ff_t\\): Effective Fed Funds Rate from FRED Database\n\\(IP_t\\): Industrial Production: Total Index from FRED Database\n\nData from FRED Database is downloaded using the fredr package, while data from Yahoo Finance is downloaded using the quantmod package. My sample period will be from M1 1987 - M12 2022 as data for \\(HP_t\\) only goes back to this period. As I am including stock prices in my model I choose the frequency of the data to be monthly as stock prices are highly volatile and liquid. By choosing quarterly data for stock price one would lose a lot of nuances and informationHence, the choice of industrial production as a proxy for GDP.\n\n\nTransformation and properties of the data\nSince the effective fed funds rate, \\(ff_t\\), is in percentages it is not being transformed. However, for the rest of the variables the log-transformation is being applied and we therefore get the following:\n\n\\(m_t=\\log(M_t)\\)\n\\(spx_t=\\log(SPX_t)\\)\n\\(hp_t=\\log(HP_t)\\)\n\\(cpi_t=\\log(CPI_t)\\)\n\\(ip_t=\\log(IP_t)\\)\n\nThis results in the following plots for the variables:\n\n\n\n\n\nFigure 1: Time Series Plots\n\n\n\n\nFrom a graphical inspection one can clearly see that \\(m_t\\), \\(spx_t\\), \\(hp_t\\), \\(cpi_t\\) and \\(ip_t\\) are not stationary processes and might contain one or more unit roots. However, for \\(ff_t\\) it is rather ambiguous whether the series is stationary or not. It is essential to know whether we are dealing with non-stationary processes or not when setting the prior distributions for the variables. By making use of the Augmented Dickey Fuller (ADF) test it can be tested formally whether the variables are unit root processes.\n\n\n\n\n\n\n\n\n\nI start out by testing for unit roots for the variables in levels as in Table 1. By looking at the p-values it is clear that all variables are non-stationary as we cannot reject the null hypothesis of the variables being a I(1) process. However, the test statistic for \\(ff_t\\) seems to be very sensitive to the choice of lags as we do reject the null hypothesis for other \\(p\\). I perform the ADF test once more to check whether the variables in first difference as in Table 2 are stationary and to establish that we are dealing with I(1) processes.\n\n\n\n\nTable 1: ADF test results - Levels\n\n\n\nTest statistic\nP-value\nLags\n\n\n\n\ncpi\n-2.988\n0.160\n12\n\n\nip\n-1.552\n0.767\n12\n\n\nff\n-3.043\n0.137\n12\n\n\nm\n-0.629\n0.976\n12\n\n\nhp\n-3.032\n0.141\n12\n\n\nspx\n-2.413\n0.403\n12\n\n\n\n\n\n\n\n\n\n\nTable 2: ADF test results - First Differences\n\n\n\nTest statistic\nP-value\nLags\n\n\n\n\nΔcpi\n-3.928\n0.013\n12\n\n\nΔip\n-5.282\n0.010\n12\n\n\nΔff\n-3.934\n0.012\n12\n\n\nΔm\n-5.531\n0.010\n12\n\n\nΔhp\n-3.858\n0.016\n12\n\n\nΔspx\n-5.552\n0.010\n12\n\n\n\n\n\n\nIt is clear that the variables in first differences are stationary as the null hypothesis can be rejected clearly.\nThe existence of a unit root in the time series can also be seen by plotting the Autocorrelation Functions (ACF) and Partial Autocorrelation Functions (PACF):\n\n\n\n\n\nFigure 2: ACF Plots\n\n\n\n\n\n\n\n\n\nFigure 3: PACF Plots\n\n\n\n\nIt is apparent that there is a clear memory pattern in the time series."
  },
  {
    "objectID": "index.html#the-econometric-model",
    "href": "index.html#the-econometric-model",
    "title": "The Effect of Money Supply Shocks on Asset Prices and Inflation in the US Economy: A Bayesian SVAR Approach",
    "section": "The Econometric Model",
    "text": "The Econometric Model\nFor investigating the effect of money supply on asset prices and inflation, a structural VAR model will be used in this research project. The structural VAR model with \\(p\\) lags can written as\n\\[\\begin{align}\nB_0y_t &= b_0 + B_1y_{t-1}+\\dots+B_py_{t-p}+w_t\n\\end{align}\\]\nwhere \\(y_t=[cpi_t\\) \\(ip_t\\) \\(ff_t\\) \\(m_t\\) \\(hp_t\\) \\(spx_t]'\\) and contains the six variables presented above. The error term \\(w_t\\) conditioned on the past is assumed to be \\(w_t|Y_{t-1}\\sim\\;iid(\\textbf{0}_N,I_N)\\), where \\(N=6\\) in this case. The \\(B_0\\) is the so-called structural matrix and contains all contemporaneous relationships between the variables, which I essentially am interested in. However, this matrix cannot just be estimated without imposing certain assumptions. Therefore, the first step is to premultiply \\(B_0^{-1}\\) on both sides so that we obtain the reduced form of the SVAR model:\n\\[\\begin{align}\ny_t &= \\mu_0 + A_1y_{t-1}+\\dots+A_py_{t-p}+u_t\n\\end{align}\\]\nWhere \\(A_i=B_0^{-1}B_i\\) and \\(u_t=B_0^{-1}w_t\\). It is assumed that \\(u_t|Y_{t-1}\\sim\\;iid(\\textbf{0}_N,\\Sigma)\\), which allows us to denote \\(\\Sigma = B_0^{-1} (B_0^{-1})'\\). In order to reconstruct \\(B_0^{-1}\\) and thereby identify the SVAR model, restrictions on the matrix need to imposed. As \\(B_0^{-1}\\) consists of \\(K(K+1)/2\\) variables, at least \\(K(K-1)/2\\) restrictions need to be imposed. This can be done in multiple ways. In this project I will impose zero exclusion restrictions on \\(B_0^{-1}\\) by implying a recursive system between the variables, which has to be economically justified. I will elaborate more on this later in this research project. It is important to note that if I choose a recursive system the ordering of \\(y_t\\) is crucial. The estimation output I will interpret to measure how money supply shocks affect asset prices and inflation is impulse response functions (IRFs), which measure the dynamic response of a variable to a given shock."
  },
  {
    "objectID": "index.html#basic-model",
    "href": "index.html#basic-model",
    "title": "The Effect of Money Supply Shocks on Asset Prices and Inflation in the US Economy: A Bayesian SVAR Approach",
    "section": "Basic Model",
    "text": "Basic Model\nFirst, I redefine the model presented in the previous section to the following:\n\\[\\begin{align}\nB_0y_t &= b_0 + B_1y_{t-1} + \\dots + B_py_{t-p} u_t\\\\\n       &= B_+ x_t + u_t\n\\end{align}\\]\nWhere \\(B_+=\\big[b_0\\;B_1\\;\\dots\\;B_p\\big]\\) and \\(x_t=\\big[1\\;y_{t-1}'\\;\\dots\\;y_{t-p}'\\big]\\). As \\(B_0\\) is the structural matrix the exclusion restrictions will be set on its rows such that \\(B_0=\\left[b_1V_1\\;\\dots\\;b_NV_N\\right]'\\) holds, where \\(B_{0[n\\cdot]}=b_n\\;V_n\\) and represents the \\(n\\)th row of \\(B_0\\). The dimension of \\(b_n\\) is \\(1\\times r_n\\) and is a vector of the unrestricted elements of the \\(n\\)th row of \\(B_0\\). The matrix \\(V_n\\) is of dimension \\(r_n\\times N\\) and consists only of ones and zeroes since it is the restriction matrix. Now the structural model can be written equation-by-equation in the following way:\n\\[\\begin{align}\nb_nV_ny_t &= B_nx_t + u_{n.t}\\\\\nu_{n.t}   &\\sim \\mathcal{N}(0,1)\n\\end{align}\\]\nWhich subsequently can be rewritten in matrix form as:\n\\[\\begin{align}\nb_nV_nY &= B_nX+ U_n\\\\\nU_n   &\\sim \\mathcal{N}(0_T,I_T)\n\\end{align}\\]\nwhere \\(\\underset{(N \\times T)}{Y}=\\begin{pmatrix} y_1, \\dots , y_T \\end{pmatrix}\\), \\(\\underset{(K \\times T)}{X}=\\begin{pmatrix} x_1, \\dots , x_T \\end{pmatrix}\\), \\(\\underset{(1 \\times T)}{U_n}=\\begin{pmatrix} u_{n.1}, \\dots , u_{n.T} \\end{pmatrix}\\) and \\(\\underset{(1 \\times K)}{B_n}= B_{+[n.]}\\).\nFor convenience the likelihood function of \\(B_0\\) and \\(B_+\\) given data can be written as a \\(\\mathcal{NGN}\\) distribution:\n\\[\\begin{align}\nL(B_+,B_0 | Y, X) \\propto |\\det(B_0)|^T \\exp \\left\\{-\\frac{1}{2} \\sum_{n=1}^N (b_nV_nY-B_nX)(b_nV_nY-B_nX)'  \\right\\}\n\\end{align}\\]\nMoving to the prior distribution, the \\(\\mathcal{NGN}\\) distribution is being used as a natural-conjugate prior. Therefore, I define \\(p(B_+,B_0)\\sim \\mathcal{NGN}(\\underline{B}, \\underline{\\Omega}, \\underline{S}, \\underline{\\nu})\\), where the following holds:\n\\[\\begin{align}\np(B_+,B_0)&=\\left(\\prod_{n=1}^N p(B_n|b_n)\\right)p(b_1,\\dots,b_n)\\\\\np(B_n|b_n)&\\sim \\mathcal{N}_K (b_nV_n\\underline{B},\\underline{\\Omega})\\\\\np(b_1,\\dots,b_n) &\\propto |\\det (B_0)|^{\\underline{\\nu}-N} \\exp \\left\\{-\\frac{1}{2}\\sum_{n=1}^Nb_nV_n\\underline{S}^{-1}V_n'b_n'\\right\\}\n\\end{align}\\]\nWhich results in the following kernel of the natural-conjugate prior distribution:\n\\[\\begin{align}\n|\\det(B_0)|^{\\underline{\\nu}-N} \\exp \\left\\{-\\frac{1}{2} \\sum_{n=1}^N b_nV_n\\underline{S}^{-1}V_n'B_n'\\right\\} \\times \\exp \\left\\{-\\frac{1}{2} \\sum_{n=1}^N (B_n-b_nV_n\\underline{B})\\underline{\\Omega}^{-1}(B_n-b_nV_n\\underline{B})'\\right\\}\n\\end{align}\\]\nFor the prior parameters the Minnesota prior parameters are being exploited:\n\\[\\begin{align}\n\\underline{B} &= \\left[0_{N\\times 1}\\;I_N\\;0_{N\\times(p-1)N}\\right]\\\\\n\\underline{\\Omega} &= \\text{diag} \\left(\\left[\\kappa_2\\;\\kappa_1(\\textbf{p}^{-2}\\otimes I_N')\\right)\\right]\\\\\n\\underline{S} &= \\kappa_0I_N\\\\\n\\underline{\\nu} &= N\n\\end{align}\\]\nThis enables us to derive the posterior distribution:\n\\[\\begin{align}\np(B_+,B_0|Y,X) &\\propto L(B_+,B_0|Y,X)p(B_+,B_0)\\\\\n               &\\propto |\\det(B_0)|^T \\exp \\left\\{-\\frac{1}{2} \\sum_{n=1}^N (b_nV_nY-B_nX)(b_nV_nY-B_nX)'  \\right\\}\\\\\n               &\\times |\\det(B_0)|^{\\underline{\\nu}-N} \\exp \\left\\{-\\frac{1}{2} \\sum_{n=1}^N b_nV_n\\underline{S}^{-1}V_n'B_n'\\right\\} \\\\ &\\times \\exp \\left\\{-\\frac{1}{2} \\sum_{n=1}^N (B_n-b_nV_n\\underline{B})\\underline{\\Omega}^{-1}(B_n-b_nV_n\\underline{B})'\\right\\}\n\\end{align}\\]\nBy performing appropriate operations this can be expressed more densely the following way:\n\\[\\begin{align}\np(B_+,B_0|Y,X) &\\propto |\\det(B_0)|^{T+\\underline{\\nu}-N} \\exp \\left\\{-\\frac{1}{2} \\sum_{n=1}^N (B_n-b_nV_n\\bar{B})\\bar{\\Omega}^{-1}(B_n-b_nV_n\\bar{B})'+b_nV_n\\bar{S}^{-1}V_n'b_n'\\right\\}\n\\end{align}\\]\nLeading to the following posterior parameters:\n\\[\\begin{align}\np(B_+,B_0|Y,X) &\\sim \\mathcal{NGN}(\\bar{B},\\bar{\\Omega},\\bar{S},\\bar{\\nu})\\\\\n\\bar{\\Omega}&=\\left[XX'+\\underline{\\Omega}^{-1}\\right]^{-1}\\\\\n\\bar{B}&=\\left[YX'+\\underline{B}\\underline{\\Omega}^{-1}\\right]\\bar{\\Omega}\\\\\n\\bar{S}&=\\left[YY'+\\underline{S}^{-1}+\\underline{B}\\underline{\\Omega}^{-1}\\underline{B}'-\\bar{B}\\bar{\\Omega}^{-1}\\bar{B}'\\right]^{-1}\\\\\n\\bar{\\nu}&= T+\\underline{\\nu}\n\\end{align}\\]"
  },
  {
    "objectID": "index.html#the-gibbs-sampler",
    "href": "index.html#the-gibbs-sampler",
    "title": "The Effect of Money Supply Shocks on Asset Prices and Inflation in the US Economy: A Bayesian SVAR Approach",
    "section": "The Gibbs Sampler",
    "text": "The Gibbs Sampler\nHaving derived the posterior parameters, the gibbs sampler can now be scrutinized. As already outlined, the sampler is based on the \\(\\mathcal{NGN}\\) distribution. Further, the algorithm is divided into two steps. First, \\(B_0\\) is drawn \\(S1+S2\\) times from \\[\\begin{gather*}\n    p(b_n | Y, X, b_1, \\dots, b_{n-1}, b_{n+1}, \\dots, b_N)\n\\end{gather*}\\] From which we get the posterior samples \\(\\{b_1^{(s)},\\dots, b_N^{(s)}\\}^{S}_{s=1}\\). Next step is to normalize these samples, so we subsequently can sample \\(B_n\\) directly for each draw of \\(b_n^{(s)}\\) from \\(p(B_n|Y,X,b_n)\\). Based on this, the posterior draws \\(\\left\\{B_+^{(s)},B_0^{(s)}\\right\\}_{s=1}^{S1+S2}\\) can be returned.\nThe gibbs sampler for \\(b_n^{(s)} \\sim p(b_n | Y, X, b_1^{(s)}, \\dots, b_{n-1}^{(s)}, b_{n+1}^{(s-1)}, \\dots, b_N^{(s-1)})\\) is computed by following the algorithm proposed by Waggoner and Zha (2003a). To facilitate this, following is defined:\n\n\\(U_n = \\text{chol}\\Big(\\bar{\\nu}\\Big(V_n\\bar{S}^{-1}V_n'\\Big)^{-1}\\Big)\\), where \\(U_n\\) is a \\(r_n \\times r_n\\) upper-triangular matrix.\n\\(w = \\left[B_{0[-n.]}^{(s)}\\right]_\\perp\\), where \\(w\\) is a \\(1 \\times N\\) matrix.\n\\(w_1 = wV_n'U_n'\\cdot \\Big( wV_n'U_n'V_nU_nw'\\Big)^{-\\frac{1}{2}}\\), where \\(w_1\\) is a \\(1 \\times r_n\\) vector.\n\\(W_n=\\begin{pmatrix} w_1' & w_{1\\perp}' \\end{pmatrix}\\), where \\(W_n\\) is a matrix of dimensions \\(r_n \\times r_n\\).\n\nThe \\(1 \\times r_n\\) matrix \\(\\alpha_n\\) can now be constructed by drawing the first element of \\(\\alpha_n\\) by following this procedure:\n\nDraw \\(u \\sim N(0_{\\nu+1},{\\bar{\\nu}^{-1}I_{\\nu+1}})\\)\nSet \\(\\alpha_{n[\\cdot 1]} = \\begin{cases}\\sqrt{u'u} \\text{ with probability 0.5}\\\\-\\sqrt{u'u} \\text{ with probability 0.5}\\end{cases}\\)\n\nThe remaining \\(r_n-1\\) elements of \\(\\alpha_n\\) can be drawn from \\(N(0_{r_n-1},\\bar{\\nu}^{-1}I_{r_n-1})\\), after which the draw of the full conditional distribution of \\(b_n\\) can be computed by \\(b_n^{(s)}\\alpha_nW_nU_n\\).\nAs already mentioned, these samples need to be normalized in order to ensure that a unique maximum is being found. I will not go into details of this procedure here, but rather refer to Waggoner and Zha (2003b) for a rigorous outline."
  },
  {
    "objectID": "index.html#r-code-snippets",
    "href": "index.html#r-code-snippets",
    "title": "The Effect of Money Supply Shocks on Asset Prices and Inflation in the US Economy: A Bayesian SVAR Approach",
    "section": "R Code Snippets",
    "text": "R Code Snippets\nThis section provides the R code behind the estimation procedure. In order to facilitate this, the following R functions are being used by the courtesy of Tomasz Woźniak.\nThe following function computes an orthogonal complement matrix to the input x, which is used in the rgn() function presented below.\n\northogonal.complement.matrix.TW = function(x){\n  # x is a mxn matrix and m>n\n  # the function returns a mx(m-n) matrix, out, that is an orthogonal complement of x, i.e.:\n  # t(x)%*%out = 0 and det(cbind(x,out))!=0\n  if( dim(x)[1] == 1 & dim(x)[2] == 2){\n    x = t(x)\n  }\n  # x <- ifelse(dim(x)[1] == 1 && dim(x)[2] == 2, t(x), x)\n  N     = dim(x)\n  tmp   = qr.Q(qr(x, tol = 1e-10),complete=TRUE)\n  out   = as.matrix(tmp[,(N[2]+1):N[1]])\n  return(out)\n}\n\nThe rgn() function simulates draws for \\(b_n\\) from a \\(\\mathcal{NGN}\\) distribution\n\n rgn             = function(n,S.inv,nu,V,B0.initial){\n  # n     - a positive integer, the number of draws to be sampled\n  # S     - an NxN positive definite matrix, a parameter of the generalized-normal distribution\n  # nu    - a positive scalar, degrees of freedom parameter\n  # V     - an N-element list, with fixed matrices\n  # B0.initial - an NxN matrix, of initial values of the parameters\n  N             = nrow(B0.initial)\n  no.draws      = n\n  \n  B0            = array(NA, c(N,N,no.draws))\n  B0.aux        = B0.initial\n  \nfor (i in 1:no.draws){\n    for (n in 1:N){\n      rn            = nrow(V[[n]])\n      SS_tmp        = nu*solve(V[[n]]%*%S.inv%*%t(V[[n]]))\n      SS_tmp        = 0.5 * (SS_tmp + t(SS_tmp))\n      Un            = chol(SS_tmp)\n      w             = t(orthogonal.complement.matrix.TW(t(B0.aux[-n,])))\n      w1            = w %*% t(V[[n]]) %*% t(Un) / sqrt(as.numeric(w %*% t(V[[n]]) %*% t(Un) %*% Un %*% V[[n]] %*% t(w)))\n      if (rn>1){\n        Wn          = cbind(t(w1),orthogonal.complement.matrix.TW(t(w1)))\n      } else {\n        Wn          = w1\n      }\n      alpha         = rep(NA,rn)\n      u             = rmvnorm(1,rep(0,nu+1),(1/nu)*diag(nu+1))\n      alpha[1]      = sqrt(as.numeric(u%*%t(u)))\n      if (runif(1)<0.5){\n        alpha[1]    = -alpha[1]\n      }\n      if (rn>1){\n        alpha[2:rn] = rmvnorm(1,rep(0,nrow(V[[n]])-1),(1/nu)*diag(rn-1))\n      }\n      bn            = alpha %*% Wn %*% Un\n      B0.aux[n,]    = bn %*% V[[n]]\n    }\n    B0[,,i]         = B0.aux\n  }\n  \n  return(B0)\n}\n\nThe next function normalizes the matrix of the contemporaneous effects, \\(B_0\\):\n\nnormalization.wz2003  = function(B0,B0.hat.inv, Sigma.inv, diag.signs){\n  # This function normalizes a matrix of contemporaneous effects\n  # according to the algorithm by Waggoner & Zha (2003, JOE)\n  # B0        - an NxN matrix, to be normalized\n  # B0.hat    - an NxN matrix, a normalized matrix\n  \n  N                 = nrow(B0)\n  K                 = 2^N\n  distance          = rep(NA,K)\n  for (k in 1:K){\n    B0.tmp.inv      = solve(diag(diag.signs[k,]) %*% B0)\n    distance[k]     = sum(\n      unlist(\n        lapply(1:N,\n               function(n){\n                 t(B0.tmp.inv - B0.hat.inv)[n,] %*%Sigma.inv %*% t(B0.tmp.inv - B0.hat.inv)[n,]\n               }\n        )))\n  }\n  B0.out            = diag(diag.signs[which.min(distance),]) %*% B0\n  \n  return(B0.out)\n}\n\nThis function normalizes the output from the rgn() function, ensuring that we obtain a unique maximum\n\nnormalize.Gibbs.output.parallel          = function(B0.posterior,B0.hat){\n  # This function normalizes the Gibbs sampler output from function rgn\n  # using function normalization.wz2003 \n  # B0.posterior  - a list, output from function rgn\n  # B0.hat        - an NxN matrix, a normalized matrix\n  \n  N                 = nrow(B0.hat)\n  K                 = 2^N\n  \n  B0.hat.inv        = solve(B0.hat)\n  Sigma.inv         = t(B0.hat)%*%B0.hat\n  \n  diag.signs        = matrix(NA,2^N,N)\n  for (n in 1:N){\n    diag.signs[,n]  = kronecker(c(-1,1),rep(1,2^(n-1)))\n  }\n  \n  B0.posterior.n    = mclapply(1:dim(B0.posterior)[3],function(i){\n    normalization.wz2003(B0=B0.posterior[,,i],B0.hat.inv, Sigma.inv, diag.signs)\n  },mc.cores=1\n  )\n  B0.posterior.n  = simplify2array(B0.posterior.n)\n  \n  return(B0.posterior.n)\n}\n\nLastly, a function for simulating the draws of the multivariate normal distribution of the autoregressive slope matrix, \\(B_+\\), is needed\n\nrnorm.ngn       = function(B0.posterior,B,Omega){\n  # This function simulates draws for the multivariate normal distribution\n  # of the autoregressive slope matrix of an SVAR model\n  # from a normal-generalized-normal distribution according to algorithm \n  # by Waggoner & Zha (2003, JEDC)\n  # B0.posterior  - a list, output from function rgn\n  # B             - an NxK matrix, a parameter determining the mean of the multivariate conditionally normal distribution given B0\n  # Omega         - a KxK positive definite matrix, a covariance matrix of the multivariate normal distribution\n  \n  N             = nrow(B)\n  K             = ncol(B)\n  no.draws      = dim(B0.posterior)[3]\n  L             = t(chol(Omega))\n  \n  Bp.posterior  = lapply(1:no.draws,function(i){\n    Bp          = matrix(NA, N, K)\n    for (n in 1:N){\n      Bp[n,]    = as.vector(t(B0.posterior[n,,i] %*% B) + L%*%rnorm(K))\n    }\n    return(Bp)\n  })\n  Bp.posterior  = simplify2array(Bp.posterior)\n  return(Bp.posterior)\n}\n\nHaving set up all the necessary functions, I now simulate a bivariate random walk to produce artificial data.\n\n#Simulation of a bivariate random walk\np.sim = 1\nT.sim = 500\nN.sim = 2\nK.sim = 1 + N.sim*p.sim\n\nY.sim           = arima.sim(list(order = c(0,1,0)), n = T.sim + p.sim-1, mean = 0, sd =1)\nfor (i in 2:N.sim){\n  Y.sim         = rbind(Y.sim, arima.sim(list(order = c(0,1,0)), n = T.sim + p.sim-1, mean = 0, sd = 1))\n}\n\nX.sim           = matrix(1,1,T.sim)\nfor (i in 1:p.sim){\n  X.sim         = rbind(X.sim, Y.sim[,(p.sim+1-i):(ncol(Y.sim)-i)])\n}\nY.sim           = Y.sim[,-p.sim]\n\nFinally, the gibbs sampler for the basic model can be presented. Note that the priors have been set in regards to the specification from above. Further, I create the restriction matrix \\(V_n\\), where one easily can see that a recursive structure is being implied in the system.\n\nGibbs.sampler.base <- function(p,Y,X,S1,S2){\n\n  N       = nrow(Y)\n  p       = p \n  K       = 1+N*p\n  S1      = S1\n  S2      = S2\n  kappa0  = 10\n  kappa1  = 10\n  kappa2  = 0.1\n  \n  priors   = list(\n  B        = cbind(rep(0,N), diag(N), matrix(0, N, (p-1)*N)),\n  Omega    = diag(c(kappa2,kappa1*((1:p)^(-2))%x%rep(1,N))),\n  S        = kappa0*diag(N),\n  nu       = N\n  )\n  \n  # create the V matrices\n  FF.V           = vector(\"list\",N)\n  for (n in 1:N){\n  FF.V[[n]]   = cbind(diag(n),matrix(0,n,N-n))\n  }\n\n  # create initial B0 matrix\n  B0.initial = matrix(0,N,N)\n  for (n in 1:N){\n  unrestricted    = apply(FF.V[[n]],2,sum)==1\n  B0.initial[n,unrestricted] = rnorm(sum(unrestricted))\n  }\n\n  B0.posterior    <- array(NA,c(N,N,(S1+S2)))\n  Bp.posterior    <- array(NA,c(N,(1+N*p),(S1+S2)))\n\n  for (s in 1:(S1+S2)){\n\n    # Computing posterior parameters\n    Omega.inv      = solve(priors$Omega)\n    Omega.post.inv = X%*%t(X) + Omega.inv\n    Omega.post     = solve(Omega.post.inv)\n    B.post         = (Y%*%t(X) + priors$B%*%Omega.inv) %*% Omega.post\n    S.post         = Y%*%t(Y) + solve(priors$S) + priors$B%*%Omega.inv%*%t(priors$B) -   B.post%*%Omega.post.inv%*%t(B.post) \n    nu.post        = ncol(Y) + priors$nu\n\n    # Use B0.initial for first iteration, otherwise the latest draw from B0.posterior\n    if (s==1) {\n      B0.s = B0.initial\n    } else {\n      B0.s = B0.posterior[,,s-1]\n    }\n\n    # sampling one draw B0 from the posterior distribution using Gibbs\n    # rgn.function samples from a random conditional generalized normal distribution\n    B0.tmp                  = rgn(n=1, S.inv=S.post, nu=nu.post, V=FF.V, B0.initial=B0.s)\n    B0.posterior[,,s]       = B0.tmp[,,1]\n\n    # sample one draw B+ from the normal conditional posterior\n    Bp.tmp              = rnorm.ngn(B0.tmp, B=B.post,Omega=Omega.post)\n    Bp.posterior[,,s]   = Bp.tmp[,,1]\n  }\n  # END OF GIBBS\n  #Discard first S1 draws\n  B0.posterior <- B0.posterior[,,(S1+1):(S1+S2)]\n  Bp.posterior <- Bp.posterior[,,(S1+1):(S1+S2)]\n\n  #normalisation of B0.posterior and Bp.posterior\n  B0.hat             = diag(sign(diag(B0.tmp[,,1]))) %*% B0.tmp[,,1]\n\n  B0.posterior.N    <- array(NA,c(N,N,S2))\n  Bp.posterior.N    <- array(NA,c(N,(1+N*p),S2))\n\n  B0.posteror.N.tmp      =  normalize.Gibbs.output.parallel(B0.posterior,B0.hat=B0.hat)\n  for (s in 1:S2){\n    B0.posterior.N[,,s]    = B0.posteror.N.tmp[,,s]\n    Bp.posterior.N[,,s]    = B0.posterior.N[,,s]%*%solve(B0.posterior[,,s])%*%Bp.posterior[,,s]\n  }\n\n  return(list(B0.posterior.N = B0.posterior.N,\n              Bp.posterior.N = Bp.posterior.N))\n}\n\n\n\n\nSince a bivariate random walk was simulated, the \\(B_0\\) matrix should be an identity matrix. Further, the first column of \\(B_+\\) should be zero and the matrix \\(B_+[,2:3]\\) should also be an identity matrix. This is also approximately the case as shown in Table 3 and Table 4, which indicates the estimation procedure is correct.\n\n\n\n\nTable 3: Matrix B[0]\n\n\n0.9932\n0.0000\n\n\n-0.0433\n1.0484\n\n\n\n\n\n\n\n\n\n\nTable 4: Matrix B[+]\n\n\n0.0624\n0.9917\n0.0033\n\n\n-0.2570\n-0.0410\n1.0415"
  },
  {
    "objectID": "index.html#the-extended-model",
    "href": "index.html#the-extended-model",
    "title": "The Effect of Money Supply Shocks on Asset Prices and Inflation in the US Economy: A Bayesian SVAR Approach",
    "section": "The Extended Model",
    "text": "The Extended Model\nAs part of my extended model, I estimate the shrinkage parameters \\(\\kappa_0\\) and \\(\\kappa_+\\). Estimating those parameters instead of just setting them might lead to improved efficiency and reliability. By remembering how \\(\\kappa_0\\) and \\(\\kappa_+\\) affected the posterior parameters in the basic model, we can now write up the kernel for the new conjugate-prior up for the extended model:\n\\[\\begin{align}\np(B_+,B_0|Y,X) &\\propto L(B_+,B_0|Y,X)p(B_+,B_0|\\kappa_0,\\kappa_+)p(\\kappa_0)p(\\kappa_+)\\\\\n\\end{align}\\]\n\\[\\begin{align}\np(\\kappa_0|\\underline{s}_{\\kappa_0},\\underline{\\nu}_{\\kappa_0}) &\\sim \\mathcal{IG}2(\\underline{s}_{\\kappa_0},\\underline{\\nu}_{\\kappa_0})\\\\\np(\\kappa_+|\\underline{s}_{\\kappa_+},\\underline{\\nu}_{\\kappa_+}) &\\sim \\mathcal{IG}2(\\underline{s}_{\\kappa_+},\\underline{\\nu}_{\\kappa_+})\n\\end{align}\\]\nThe full-conditional posterior distribution of \\(\\kappa_0\\) can be found to be:\n\\[\\begin{align}\np(\\kappa_0|Y,X,B_0,B_+,\\kappa_+) &\\propto p(B_0|\\kappa_0)p(\\kappa_0)\\\\\n&\\propto \\prod_{n=1}^N\\kappa_0^{\\frac{r_n}{2}}\\exp \\left\\{  -\\frac{1}{2}\\sum_{n=1}^N b_nV_n(\\kappa_0 I_{r_n})^{-1}V_n'b_n'\\right\\}\\kappa_0^{-\\frac{\\underline{\\nu}_{\\kappa_0}+2}{2}}\\exp \\left\\{  -\\frac{1}{2} \\frac{\\underline{s}_{\\kappa_0}}{\\kappa_0}\\right\\}\\\\\n&\\propto \\prod_{n=1}^N\\kappa_0^{\\frac{r_n}{2}} \\exp \\left\\{  -\\frac{1}{2}\\frac{1}{\\kappa_0}\\sum_{n=1}^N b_nV_n I_{r_n}V_n'b_n'\\right\\}\\kappa_0^{-\\frac{\\underline{\\nu}_{\\kappa_0}+2}{2}}\\exp \\left\\{  -\\frac{1}{2} \\frac{\\underline{s}_{\\kappa_0}}{\\kappa_0}\\right\\}\n\\end{align}\\]\nSince \\(\\underline{S}=\\kappa_0I_N\\) and \\(b_n|\\kappa_0 \\sim \\mathcal{N}(0,\\kappa_0(V_nV_n')^{-1})=\\mathcal{N}_{r_n}(0_{r_n},\\kappa_0I_{r_n})\\). By collecting the components in an appropriate way, the full-conditional posterior can be written as:\n\\[\\begin{align}\np(\\kappa_0|Y,X,B_0,B_+,\\kappa_+) &\\propto \\kappa_0^{-\\frac{\\bar{\\nu}_{\\kappa_0}+2}{2}} \\exp \\left\\{ -\\frac{1}{2}\\frac{\\bar{s}_{\\kappa_0}}{\\kappa_0} \\right\\}\\\\\n\\bar{s}_{\\kappa_0} &= \\underline{s}_{\\kappa_0}+\\sum_{n=1}^N b_nV_nI_{r_n}V_n'b_n'\\\\\n\\bar{\\nu}_{\\kappa_0} &= \\underline{\\nu}_{\\kappa_0}+\\sum_{n=1}^N r_n\n\\end{align}\\]\nThe same procedure goes for the full-conditional posterior distribution of \\(\\kappa_+\\):\n\\[\\begin{align}\np(\\kappa_+|Y,X,B_0,B_+,\\kappa_0) &\\propto p(B_+|B_0,\\kappa_+)p(\\kappa_+)\\\\\n&\\propto \\kappa_+^{\\frac{K}{2}}\\exp \\left\\{-\\frac{1}{2}\\frac{1}{\\kappa_+} \\sum_{n=1}^N (B_n-b_nV_n\\underline{B})\\underline{\\Omega}^{-1}(B_n-b_nV_n\\underline{B})'\\right\\}\\kappa_+^{-\\frac{\\underline{\\nu}_{\\kappa_+}+2}{2}}\\exp \\left\\{  -\\frac{1}{2} \\frac{\\underline{s}_{\\kappa_+}}{\\kappa_+}\\right\\}\n\\end{align}\\]\nSince \\(B_n|b_n,\\kappa_+ \\sim \\mathcal{N}_{N+1}(b_nV_n\\underline{B},\\kappa_+\\Omega)\\)\nWhich further can be derived to:\n\\[\\begin{align}\np(\\kappa_+|Y,X,B_0,B_+,\\kappa_0) &\\propto \\kappa_+^{-\\frac{\\bar{\\nu}_{\\kappa_+}+2}{2}} \\exp \\left\\{ -\\frac{1}{2}\\frac{\\bar{s}_{\\kappa_+}}{\\kappa_+} \\right\\}\\\\\n\\bar{s}_{\\kappa_+} &= \\underline{s}_{\\kappa_+}+\\sum_{n=1}^N (B_n-b_nV_n\\underline{B})\\underline{\\Omega}^{-1}(B_n-b_nV_n\\underline{B})'\\\\\n\\bar{\\nu}_{\\kappa_+} &= \\underline{\\nu}_{\\kappa_+}+NK\n\\end{align}\\]\nThis facilitates writing up the gibbs sampler for the extended model. Note that new priors have to be set compared to the basic model, as the hyperparameters are now being estimated:\n\nGibbs.sampler.extended <- function(p,Y,X,S1,S2){\n  \n  N       = nrow(Y)\n  p       = p # calculate from X and Y (K and N)\n  K       = 1+N*p\n  S1      = S1\n  S2      = S2\n  \n  #set new priors\n  \n  priors = list(\n  B         = cbind(rep(0,N), diag(N), matrix(0, N, (p-1)*N)),\n  Omega     = diag(c(10,((1:p)^(-2))%x%rep(1,N))),\n  S         = diag(N),\n  nu        = N,\n  S.kappa0  = 1,\n  nu.kappa0 = 1,\n  S.kappa1  = 1,\n  nu.kappa1 = 1\n  )\n  \n  # create the V matrices\n  FF.V           = vector(\"list\",N)\n  for (n in 1:N){\n  FF.V[[n]]   = cbind(diag(n),matrix(0,n,N-n))\n  }\n\n  # create initial B0 matrix\n  B0.initial = matrix(0,N,N)\n  for (n in 1:N){\n  unrestricted    = apply(FF.V[[n]],2,sum)==1\n  B0.initial[n,unrestricted] = rnorm(sum(unrestricted))\n  }\n  \n  kappa0          <- rep(NA, S1 + S2)\n  kappa1          <- rep(NA, S1 + S2)\n  B0.posterior    <- array(NA,c(N,N,(S1+S2)))\n  Bp.posterior    <- array(NA,c(N,(1+N*p),(S1+S2)))  \n  \n  kappa0[1] <- 1\n  kappa1[1] <- 1 \n  \n  for (s in 1:(S1+S2)){\n    \n    # Computing posterior parameters\n    # Only Omega, B and S depend on kappa1\n    \n    Omega.inv      = solve(priors$Omega)\n    Omega.post.inv = X%*%t(X) + (1/kappa1[s])*Omega.inv\n    Omega.post     = solve(Omega.post.inv)\n    B.post         = (Y%*%t(X) + priors$B%*%((1/kappa1[s])*Omega.inv)) %*% Omega.post\n    S.post         = Y%*%t(Y) + (1/kappa0[s])*solve(priors$S) + priors$B%*%((1/kappa1[s])*Omega.inv)%*%t(priors$B) - B.post%*%Omega.post.inv%*%t(B.post) \n    nu.post        = ncol(Y) + priors$nu\n    \n    # Use B0.initial for first iteration, otherwise the latest draw from B0.posterior\n    \n    if (s==1) {\n      B0.s = B0.initial\n    } else {\n      B0.s = B0.posterior[,,s-1]\n    }\n    \n    # sampling one draw B0 from the posterior distribution using Gibbs  \n    # rgn.function samples from a random conditional generalized normal distribution\n    \n    B0.tmp                  = rgn(n=1, S.inv=S.post, nu=nu.post, V=FF.V, B0.initial=B0.s)\n    B0.posterior[,,s]       = B0.tmp[,,1]\n    \n    # sample one draw B+ from the normal conditional posterior\n    Bp.tmp              = rnorm.ngn(B0.tmp, B=B.post,Omega=Omega.post)\n    Bp.posterior[,,s]   = Bp.tmp[,,1]\n    \n    #compute posterior for the shrinkage parameter S.kappa and nu\n    S.kappa0.post = priors$S.kappa0 + sum(B0.posterior[,,s]^2)\n    \n    nu.kappa0.post  = priors$nu.kappa0 + sum(unlist(lapply(FF.V, nrow)))\n    \n    S.kappa1.post   = priors$S.kappa1\n    for (i in 1:N){\n    S.kappa1.post = S.kappa1.post + (Bp.posterior[i,,s]-       B0.posterior[i,,s]%*%priors$B)%*%Omega.inv%*%t(Bp.posterior[i,,s]-B0.posterior[i,,s]%*%priors$B)\n    }\n    \n    nu.kappa1.post  = priors$nu.kappa1 + N*(p*N+1) \n    \n    \n    #Draw kappa0 and kappa1 from IG2\n    if (s != S1+S2) {\n      kappa0[s+1]    = S.kappa0.post / rchisq(1, df=nu.kappa0.post) \n      kappa1[s+1]    = S.kappa1.post / rchisq(1, df=nu.kappa1.post) \n    }\n  }\n  \n  #Discard first S1 draws\n  \n  B0.posterior <- B0.posterior[,,(S1+1):(S1+S2)]\n  Bp.posterior <- Bp.posterior[,,(S1+1):(S1+S2)]\n  kappa0       <- kappa0[(S1+1):(S1+S2)]\n  kappa1       <- kappa1[(S1+1):(S1+S2)]\n  \n  #normalisation of B0.posterior and Bp.posterior\n  \n  B0.hat             = diag(sign(diag(B0.tmp[,,1]))) %*% B0.tmp[,,1]\n  # t(chol((nu.post-N)*S.post))# normalisation using this B0.hat should work\n  \n  B0.posterior.N    <- array(NA,c(N,N,S2))\n  Bp.posterior.N    <- array(NA,c(N,(1+N*p),S2))\n  \n    B0.posteror.N.tmp      =  normalize.Gibbs.output.parallel(B0.posterior,B0.hat=B0.hat)\n  for (s in 1:S2){\n    B0.posterior.N[,,s]    = B0.posteror.N.tmp[,,s]\n    Bp.posterior.N[,,s]    = B0.posterior.N[,,s]%*%solve(B0.posterior[,,s])%*%Bp.posterior[,,s]\n  }\n  \n  return(list(B0.posterior.N = B0.posterior.N,\n              Bp.posterior.N = Bp.posterior.N,\n              kappa0 = kappa0,\n              kappa1 = kappa1))\n} \n\n\n\n\n\n\n\n\nTable 5: Matrix B[0]\n\n\n0.9914\n0.0000\n\n\n-0.0419\n1.0456\n\n\n\n\n\n\n\n\n\n\nTable 6: Matrix B[+]\n\n\n0.0687\n0.9900\n0.0036\n\n\n-0.2821\n-0.0394\n1.0380\n\n\n\n\n\n\nFrom Table 5 and Table 6, the estimation procedure for the extended model seems to be correct as well as the output aligns with the artificial data being a bivariate random walk.\nI now turn to plotting the diagonal elements of \\(B_+[,2:3]\\) in order to show whether the algorithm converges.\n\n\n\n\n\nFigure 4: Convergence Plots\n\n\n\n\nThis is indeed the case. The plots look like white noise processes as it fluctuates around the true value 1. This means the algorithm has converged."
  },
  {
    "objectID": "index.html#model-with-t-distributed-errors",
    "href": "index.html#model-with-t-distributed-errors",
    "title": "The Effect of Money Supply Shocks on Asset Prices and Inflation in the US Economy: A Bayesian SVAR Approach",
    "section": "Model with \\(t\\)-distributed errors",
    "text": "Model with \\(t\\)-distributed errors\nSince macroeconomic variables tend to be characterized by fat tails, I now proceed by extending the model even further by modelling the error terms to follow a \\(t\\)-distribution. Recall from the previous section that \\(u_{n.t}\\sim\\mathcal{N}(0,1)\\), where \\(u_{n.t}\\) represents the structural shock for the \\(n\\)th row. This is now being replaced by the following assumption\n\\[\\begin{align}\nu_{n.t}|\\lambda\\sim\\mathcal{N}(0,\\lambda_n)\n\\end{align}\\]\nWhere \\(\\lambda_n \\sim \\mathcal{IG}2(\\underline{s}_{\\lambda},\\underline{\\nu}_{\\lambda})\\). The distribution of the error term then becomes the following:\n\\[\\begin{align}\np(u_{n.t}) &= \\int p(u_{n.t}|\\lambda_n)p(\\lambda_n)d\\lambda_n \\\\\n& = \\mathcal{t}(0,\\underline{s},\\underline{\\nu})\n\\end{align}\\]\nThis obviously also alters the likelihood function in the model, which can be rewritten to:\n\\[\\begin{align}\nL(B_+,B_0 | Y, X) \\propto |\\det(B_0)|^T|\\det(\\lambda I_t)|^N \\exp \\left\\{-\\frac{1}{2} \\sum_{n=1}^N (b_nV_nY-B_nX)(\\lambda I_t)^{-1}(b_nV_nY-B_nX)'  \\right\\}\n\\end{align}\\]\nWe can now find the full conditional posterior for \\(\\lambda\\):\n\\[\\begin{align}\np(\\lambda|Y,X,B_0,B_+) &\\propto p(B_+|B_0,\\lambda)p(\\lambda)\\\\\n&\\propto |\\det(\\Omega)|^T|\\det(\\lambda I_t)|^N \\exp \\left\\{-\\frac{1}{2} \\sum_{n=1}^N (B_n-b_nV_n\\underline{B})(\\lambda I_T)^{-1}\\underline{\\Omega}^{-1}(B_n-b_nV_n\\underline{B})'\\right\\}\\lambda^{-\\frac{\\underline{\\nu}_{\\lambda}+2}{2}}\\exp \\left\\{  -\\frac{1}{2} \\frac{\\underline{s}_{\\lambda}}{\\lambda}\\right\\}\n\\end{align}\\]\nWhich can be shown to yield the following posterior parameters:\n\\[\\begin{align}\np(\\lambda|Y,X,B_0,B_+) &\\propto \\lambda^{-\\frac{\\bar{\\nu}_{\\lambda}+2}{2}} \\exp \\left\\{ -\\frac{1}{2}\\frac{\\bar{s}_{\\lambda}}{\\lambda} \\right\\}\\\\\n\\bar{s}_{\\lambda} &= \\underline{s}_{\\lambda}+\\sum_{n=1}^N (B_n-b_nV_n\\underline{B})\\underline{\\Omega}^{-1}(B_n-b_nV_n\\underline{B})'\\\\\n\\bar{\\nu}_{\\lambda} &= \\underline{\\nu}_{\\lambda}+NT\n\\end{align}\\]\nThis facilitates writing up the gibbs sampler for this model below. Note that \\(\\underline{s}_{\\lambda}=1\\) and \\(\\underline{\\nu}_{\\lambda}=4\\). By setting the degrees of freedom rather low this allows for fat tails.\n\nGibbs.sampler.tdis <- function(p,Y,X,S1,S2){\n  #\n  N       = nrow(Y)\n  p       = p \n  K       = 1+N*p\n  S1      = S1 # S1\n  S2      = S2 #S2\n  \n  ### Setting new priors\n  priors   = list(\n  B        = cbind(rep(0,N), diag(N), matrix(0, N, (p-1)*N)),\n  Omega    = diag(c(10,((1:p)^(-2))%x%rep(1,N))),\n  S        = diag(N),\n  nu       = N,\n  S.kappa0  = 1,\n  nu.kappa0 = 1,\n  S.kappa1  = 1,\n  nu.kappa1 = 1, \n  s.lambda = 1, \n  nu.lambda = 4\n  )\n  \n  lambda.draw = priors$s.lambda/rchisq(1, priors$nu.lambda)\n\n  \n  #Exclusions (can be changed to different exclusions then cholesky) \nFF.V           = vector(\"list\",N)\nfor (n in 1:N){\n  FF.V[[n]]   = cbind(diag(n),matrix(0,n,N-n))\n}\n\n# The B0.initial is used as an initial matrix used in the Gibbs sampler\nB0.initial = matrix(0,N,N)\nfor (n in 1:N){\n  unrestricted    = apply(FF.V[[n]],2,sum)==1\n  B0.initial[n,unrestricted] = rnorm(sum(unrestricted))\n}\n  \n  kappa0          <- rep(NA, S1 + S2)\n  kappa1          <- rep(NA, S1 + S2)\n  B0.posterior    <- array(NA,c(N,N,(S1+S2)))\n  Bp.posterior    <- array(NA,c(N,(1+N*p),(S1+S2)))\n  lambda          <- rep(NA, S1 + S2)\n  \n  #  lambda[1] <- priors$lambda.draw\n  kappa0[1] <- 1\n  kappa1[1] <- 1\n  \n  for (s in 1:(S1+S2)){\n    # Computing posterior parameters\n    # Only Omega, B and S depend on kappa1\n    \n    if (s==1) {\n      B0.s = B0.initial\n      lambda = lambda.draw \n    } else {\n      B0.s = B0.posterior[,,s-1]\n      lambda[s]=lambda[s]\n    }  \n    \n    Omega.inv      = solve(priors$Omega)\n    Omega.post.inv = (1/lambda[s])*X%*%t(X) + (1/kappa1[s])*Omega.inv\n    Omega.post     = solve(Omega.post.inv)\n    B.post         = ((1/lambda[s])*Y%*%t(X) + priors$B%*%((1/kappa1[s])*Omega.inv)) %*% Omega.post\n    S.post         = (1/lambda[s])*Y%*%t(Y) + (1/kappa0[s])*solve(priors$S) + priors$B%*%((1/kappa1[s])*Omega.inv)%*%t(priors$B) - B.post%*%Omega.post.inv%*%t(B.post)\n    nu.post        = ncol(Y) + priors$nu\n    \n    # sampling one draw B0 from the posterior distribution using Gibbs  \n    # rgn.function samples from a random conditional generalized normal distribution\n    \n    B0.tmp                  = rgn(n=1, S.inv=S.post, nu=nu.post, V=FF.V, B0.initial=B0.s)\n    B0.posterior[,,s]       = B0.tmp[,,1]\n    \n    # sample one draw B+ from the normal conditional posterior\n    Bp.tmp              = rnorm.ngn(B0.tmp, B=B.post,Omega=Omega.post)\n    Bp.posterior[,,s]   = Bp.tmp[,,1]\n    \n    #compute posterior for the shrinkage parameter S.kappa and nu\n    S.kappa0.post = priors$S.kappa0 + sum(B0.posterior[,,s]^2)\n    \n    nu.kappa0.post  = priors$nu.kappa0 + sum(unlist(lapply(FF.V, nrow)))\n    \n    S.kappa1.post   = priors$S.kappa1\n    for (i in 1:N){\n      S.kappa1.post = S.kappa1.post + (Bp.posterior[i,,s]- B0.posterior[i,,s]%*%priors$B)%*%Omega.inv%*%t(Bp.posterior[i,,s]-B0.posterior[i,,s]%*%priors$B)\n    }\n    \n    nu.kappa1.post  = priors$nu.kappa1 + N*(p*N+1) \n    \n    #### LAMBDA\n    \n    S.lambda.post   = priors$s.lambda\n    for (i in 1:N){\n      S.lambda.post = S.lambda.post + (Bp.posterior[i,,s]- B0.posterior[i,,s]%*%priors$B)%*%Omega.inv%*%t(Bp.posterior[i,,s]-B0.posterior[i,,s]%*%priors$B)\n    }\n    \n    nu.lambda.post  = priors$nu.lambda + N*ncol(Y) \n    \n\n    #Draw kappa0, kappa1 and lambda from IG2\n    if (s != S1+S2) {\n      kappa0[s+1]    = S.kappa0.post / rchisq(1, df=nu.kappa0.post) \n      kappa1[s+1]    = S.kappa1.post / rchisq(1, df=nu.kappa1.post)\n      lambda[s+1]    = S.lambda.post / rchisq(1, df=nu.lambda.post)\n    }\n  }\n  \n  #Discard first S1 draws\n  \n  B0.posterior <- B0.posterior[,,(S1+1):(S1+S2)]\n  Bp.posterior <- Bp.posterior[,,(S1+1):(S1+S2)]\n  kappa0       <- kappa0[(S1+1):(S1+S2)]\n  kappa1       <- kappa1[(S1+1):(S1+S2)]\n  lambda       <- lambda[(S1+1):(S1+S2)]\n  \n  #normalisation of B0.posterior and Bp.posterior\n  \n  B0.hat             = diag(sign(diag(B0.tmp[,,1]))) %*% B0.tmp[,,1]\n  # t(chol((nu.post-N)*S.post))# normalisation using this B0.hat should work\n  \n  B0.posterior.N    <- array(NA,c(N,N,S2))\n  Bp.posterior.N    <- array(NA,c(N,(1+N*p),S2))\n  \n  B0.posteror.N.tmp      =  normalize.Gibbs.output.parallel(B0.posterior,B0.hat=B0.hat)\n  for (s in 1:S2){\n    B0.posterior.N[,,s]    = B0.posteror.N.tmp[,,s]\n    B0.posterior.N[,,s]    = B0.posterior.N[,,s]/sqrt(lambda[s]) \n    Bp.posterior.N[,,s]    = B0.posterior.N[,,s]%*%solve(B0.posterior[,,s])%*%Bp.posterior[,,s]\n  }\n  \n  return(list(B0.posterior.N = B0.posterior.N,\n              Bp.posterior.N = Bp.posterior.N,\n              Omega.post = Omega.post, \n              kappa0 = kappa0,\n              kappa1 = kappa1,\n              lambda = lambda))\n}\n\n\n\n\nBy running the model with the artificial data and looking at the \\(B_0\\) and \\(B_+\\) matrix in Table 7 and Table 8, it is clear that the output aligns with a bi-variate random walk.\n\n\n\n\nTable 7: Matrix B[0]\n\n\n0.9944\n0.0000\n\n\n-0.0440\n1.0483\n\n\n\n\n\n\n\n\n\n\nTable 8: Matrix B[+]\n\n\n\n\nNA\n\n\n\n\n0.0689\n0.9928\n0.0035\n\n\n-0.2877\n-0.0414\n1.0405\n\n\n\n\n\n\nFurther, by computing a trace plot for \\(\\lambda\\) we see that the algorithm indeed also converges.\n\n\n\n\n\nFigure 5: Convergence Plot of lambda"
  }
]