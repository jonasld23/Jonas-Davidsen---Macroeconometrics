[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Effect of Money Supply Shocks on Asset Prices and Inflation in the US Economy: A Bayesian SVAR Approach",
    "section": "",
    "text": "Disclaimer: This document is merely a research proposal and therefore still subject to changes along the process."
  },
  {
    "objectID": "index.html#the-question-objective-and-motivation",
    "href": "index.html#the-question-objective-and-motivation",
    "title": "The Effect of Money Supply Shocks on Asset Prices and Inflation in the US Economy: A Bayesian SVAR Approach",
    "section": "The question, objective and motivation",
    "text": "The question, objective and motivation\nThe objective\nThe objective of this research project is to investigate the effects of a money supply shock on asset prices and inflation in the US economy.\nThe research question\nHow does money supply affect asset prices and inflation and what are the implications for monetary policy and financial stability?\nMotivation\nDespite extensive Quantitative Easing (QE) programs following the financial crisis in 2008, inflation continued to remain well under the target level in many advanced economies. Rather, the increase in the money supply primarily seemed to inflate asset prices instead of the general price level and in other words struggled to stimulate aggregate demand. However, following the Covid-19 pandemic central banks quite drastically expanded their QE programs and thereby raised the money supply to unprecedented levels in response to the economic downturn. Among other factors such as supply chain issues, surging energy prices and massive fiscal stimulus, this has been one of the drivers behind inflation reaching double digits recently in many countries. This raises questions about the effectiveness of monetary policy in stimulating the economy and simultaneously controlling inflation. Another concern regarding QE mainly inflating asset prices, is that it can lead to financial instability in terms of increased risk of assets becoming overvalued and detached from the underlying fundamentals. This can lead to asset price bubbles and increase the amount of speculation among investors. It is therefore crucial for both policy makers and investors to understand the mechanisms through which a money supply shock affects different economic variables such as asset prices and inflation in light of economic and financial stability."
  },
  {
    "objectID": "index.html#data-and-their-properties",
    "href": "index.html#data-and-their-properties",
    "title": "The Effect of Money Supply Shocks on Asset Prices and Inflation in the US Economy: A Bayesian SVAR Approach",
<<<<<<< Updated upstream
    "section": "Data and their properties",
    "text": "Data and their properties\nSo far the intention is to include the following six variables for the US economy in the SVAR model.\n\n\\(M_t\\): M2 aggregate from FRED Database\n\\(SPX_t\\): SP500 index from Yahoo Finance\n\\(HP_t\\): S&P/Case-Shiller U.S. National Home Price Index from FRED Database\n\\(CPI_t\\): Consumer Price Index: All Items for the US from FRED Database\n\nMotivation: Given my focus on the relationship between money supply, asset prices and inflation, a measure for those three variables are needed. As a measure for money supply the M2 aggregate is chosen as it serves as a good proxy for the availability of liquidity in the economy. As measures for asset prices, both stock prices and house prices are included. These two types of assets are big components of the total assets in the economy and provide a way to investigate the transmission mechanism of money supply shocks to asset prices and the real economy. Further, the CPI is chosen as it is commonly used to construct the so-called headline inflation.\n\n\\(ff_t\\): Effective Fed Funds Rate from FRED Database\n\\(IP_t\\): Industrial Production: Total Index\n\nMotivation: The effective fed funds rate is the rate at which banks lend and borrow funds from each other overnight and is obviously heavily influenced by the actual fed funds rate. Industrial production is a measure for monthly US real activity and is chosen since actual GDP data is only available for each quarter. These two variables are important to include as they play a crucial role in the relationship between money supply, asset prices and inflation and therefore serve as control variables.\n\n\nData from FRED Database is downloaded using the fredr package, while data from Yahoo Finance is downloaded using the quantmod package. My sample period will be from M1 1987 - M12 2022 as data for \\(HP_t\\) only goes back to this period. As I am including stock prices in my model I choose the frequency of the data to be monthly and not quarterly as stocks are highly volatile and liquid. Hence, the choice of industrial production as a proxy for GDP.\nTransformation and visualisation of the variables\nSince the effective fed funds rate, \\(ff_t\\), is in percentages it is not being transformed. However, for the rest of the variables the log-transformation is being applied and we therefore get the following:\n\\(m_t=\\log(M_t)\\), \\(spx_t=\\log(SPX_t)\\), \\(hp_t=\\log(HP_t)\\), \\(cpi_t=\\log(CPI_t)\\), \\(ip_t=\\log(IP_t)\\).\nThis results in the following plots for the variables:\n\n\n\n\n\nFrom a graphical inspection one can clearly see that \\(m_t\\), \\(spx_t\\), \\(hp_t\\), \\(cpi_t\\) and \\(ip_t\\) are not stationary processes and might contain one or more unit roots. However, for \\(ff_t\\) it is rather ambiguous whether the variables are stationary or not. It is essential to know whether we are dealing with non-stationary processes or not when setting the prior distributions for the variables. By making use of the Augmented Dickey Fuller (ADF) test it can be tested formally whether the variables are unit root processes.\n\n\n\n\n\n                         Test statistic P-value Lags\nMoney Supply                     -2.988   0.160   12\nSP500 Index                      -1.552   0.767   12\nHouse Price Index                -3.043   0.137   12\nCPI                              -2.425   0.398   12\nEffective Fed Funds Rate         -1.552   0.767   12\nIndustrial Production            -2.413   0.403   12\n\n\nBy looking at the p-values it is clear that all variables are non-stationary as we cannot reject the null hypothesis of the variables being a I(1) process. However, the test statistic for \\(ff_t\\) seems to be very sensitive to the choice of lags as we do reject the null hypothesis for other \\(p\\). I will proceed by treating all variables as unit root non stationary."
=======
<<<<<<< HEAD
    "section": "Data",
    "text": "Data\nIn this section I will present the data I use for this research project by plotting the time series and analyze its properties.\n\nChoice of variables\nGiven my focus on the relationship between money supply, asset prices and inflation, a measure for those three variables are needed. As a measure for money supply the M1 aggregate is chosen as it serves as a good proxy for the availability of liquidity in the economy. As measures for asset prices, both stock prices and house prices are included. These two types of assets are big components of the total assets in the US economy and provide a way to investigate the transmission mechanism of money supply shocks to asset prices and the real economy. Further, the CPI is chosen as it is commonly used to construct the so-called headline inflation. Moreover, the effective fed funds rate and industrial production in the US is included. The effective fed funds rate is the rate at which banks lend and borrow funds from each other overnight and is obviously heavily influenced by the actual fed funds rate. Industrial production is a measure for monthly US real activity and is chosen since actual GDP data is only available quarterly. These two variables are important to include as they play a crucial role in the relationship between money supply, asset prices and inflation and therefore serve as control variables. Thus, I have the following six variables for the US economy in my SVAR model:\n\n\\(M_t\\): M1 aggregate from FRED Database\n\\(SPX_t\\): SP500 index from Yahoo Finance\n\\(HP_t\\): S&P/Case-Shiller U.S. National Home Price Index from FRED Database\n\\(CPI_t\\): Consumer Price Index: All Items for the US from FRED Database\n\\(ff_t\\): Effective Fed Funds Rate from FRED Database\n\\(IP_t\\): Industrial Production: Total Index from FRED Database\n\nData from FRED Database is downloaded using the fredr package, while data from Yahoo Finance is downloaded using the quantmod package. My sample period will be from M1 1987 - M12 2022 as data for \\(HP_t\\) only goes back to this period. As I am including stock prices in my model I choose the frequency of the data to be monthly as stock prices are highly volatile and liquid. By choosing quarterly data for stock price one would lose a lot of nuances and informationHence, the choice of industrial production as a proxy for GDP.\n\n\nTransformation and properties of the data\nSince the effective fed funds rate, \\(ff_t\\), is in percentages it is not being transformed. However, for the rest of the variables the log-transformation is being applied and we therefore get the following:\n\n\\(m_t=\\log(M_t)\\)\n\\(spx_t=\\log(SPX_t)\\)\n\\(hp_t=\\log(HP_t)\\)\n\\(cpi_t=\\log(CPI_t)\\)\n\\(ip_t=\\log(IP_t)\\)\n\nThis results in the following plots for the variables:\n\n\n\n\n\nFigure 1: Time Series Plots\n\n\n\n\nFrom a graphical inspection one can clearly see that \\(m_t\\), \\(spx_t\\), \\(hp_t\\), \\(cpi_t\\) and \\(ip_t\\) are not stationary processes and might contain one or more unit roots. However, for \\(ff_t\\) it is rather ambiguous whether the series is stationary or not. It is essential to know whether we are dealing with non-stationary processes or not when setting the prior distributions for the variables. By making use of the Augmented Dickey Fuller (ADF) test it can be tested formally whether the variables are unit root processes.\n\n\n\n\n\n\n\n\n\nTest statistic\nP-value\nLags\n\n\n\n\ncpi\n-2.988\n0.160\n12\n\n\nip\n-1.552\n0.767\n12\n\n\nff\n-3.043\n0.137\n12\n\n\nm\n-0.629\n0.976\n12\n\n\nhp\n-3.032\n0.141\n12\n\n\nspx\n-2.413\n0.403\n12\n\n\n\n\n\nI start out by testing for unit roots for the variables in levels. By looking at the p-values it is clear that all variables are non-stationary as we cannot reject the null hypothesis of the variables being a I(1) process. However, the test statistic for \\(ff_t\\) seems to be very sensitive to the choice of lags as we do reject the null hypothesis for other \\(p\\). I perform the ADF test once more to check whether the variables in first difference are stationary and to establish that we are dealing with I(1) processes.\n\n\n\n\n\n\nTest statistic\nP-value\nLags\n\n\n\n\nΔcpi\n-3.928\n0.013\n12\n\n\nΔip\n-5.283\n0.010\n12\n\n\nΔff\n-3.934\n0.012\n12\n\n\nΔm\n-5.531\n0.010\n12\n\n\nΔhp\n-3.858\n0.016\n12\n\n\nΔspx\n-5.552\n0.010\n12\n\n\n\n\n\nIt is clear that the variables in first differences are stationary as the null hypothesis can be rejected clearly.\nThe existence of a unit root in the time series can also be seen by plotting the Autocorrelation Functions (ACF) and Partial Autocorrelation Functions (PACF):\n\n\n\n\n\nFigure 2: ACF Plots\n\n\n\n\n\n\n\n\n\nFigure 3: PACF Plots\n\n\n\n\nIt is apparent that there is a clear memory pattern in the time series."
=======
    "section": "Data and their properties",
    "text": "Data and their properties\nSo far the intention is to include the following six variables for the US economy in the SVAR model.\n\n\\(M_t\\): M2 aggregate from FRED Database\n\\(SPX_t\\): SP500 index from Yahoo Finance\n\\(HP_t\\): S&P/Case-Shiller U.S. National Home Price Index from FRED Database\n\\(CPI_t\\): Consumer Price Index: All Items for the US from FRED Database\n\nMotivation: Given my focus on the relationship between money supply, asset prices and inflation, a measure for those three variables are needed. As a measure for money supply the M2 aggregate is chosen as it serves as a good proxy for the availability of liquidity in the economy. As measures for asset prices, both stock prices and house prices are included. These two types of assets are big components of the total assets in the economy and provide a way to investigate the transmission mechanism of money supply shocks to asset prices and the real economy. Further, the CPI is chosen as it is commonly used to construct the so-called headline inflation.\n\n\\(ff_t\\): Effective Fed Funds Rate from FRED Database\n\\(IP_t\\): Industrial Production: Total Index\n\nMotivation: The effective fed funds rate is the rate at which banks lend and borrow funds from each other overnight and is obviously heavily influenced by the actual fed funds rate. Industrial production is a measure for monthly US real activity and is chosen since actual GDP data is only available for each quarter. These two variables are important to include as they play a crucial role in the relationship between money supply, asset prices and inflation and therefore serve as control variables.\n\n\nData from FRED Database is downloaded using the fredr package, while data from Yahoo Finance is downloaded using the quantmod package. My sample period will be from M1 1987 - M12 2022 as data for \\(HP_t\\) only goes back to this period. As I am including stock prices in my model I choose the frequency of the data to be monthly and not quarterly as stocks are highly volatile and liquid. Hence, the choice of industrial production as a proxy for GDP.\nTransformation and visualisation of the variables\nSince the effective fed funds rate, \\(ff_t\\), is in percentages it is not being transformed. However, for the rest of the variables the log-transformation is being applied and we therefore get the following:\n\\(m_t=\\log(M_t)\\), \\(spx_t=\\log(SPX_t)\\), \\(hp_t=\\log(HP_t)\\), \\(cpi_t=\\log(CPI_t)\\), \\(ip_t=\\log(IP_t)\\).\nThis results in the following plots for the variables:\n\n\n\n\n\nFrom a graphical inspection one can clearly see that \\(m_t\\), \\(spx_t\\), \\(hp_t\\), \\(cpi_t\\) and \\(ip_t\\) are not stationary processes and might contain one or more unit roots. However, for \\(ff_t\\) it is rather ambiguous whether the variables are stationary or not. It is essential to know whether we are dealing with non-stationary processes or not when setting the prior distributions for the variables. By making use of the Augmented Dickey Fuller (ADF) test it can be tested formally whether the variables are unit root processes.\n\n\n\n\n\n                         Test statistic P-value Lags\nMoney Supply                     -2.988   0.160   12\nSP500 Index                      -1.552   0.767   12\nHouse Price Index                -3.043   0.137   12\nCPI                              -2.425   0.398   12\nEffective Fed Funds Rate         -1.552   0.767   12\nIndustrial Production            -2.413   0.403   12\n\n\nBy looking at the p-values it is clear that all variables are non-stationary as we cannot reject the null hypothesis of the variables being a I(1) process. However, the test statistic for \\(ff_t\\) seems to be very sensitive to the choice of lags as we do reject the null hypothesis for other \\(p\\). I will proceed by treating all variables as unit root non stationary."
>>>>>>> 41d47063279825e9a18740578892ff6012578e11
>>>>>>> Stashed changes
  },
  {
    "objectID": "index.html#the-model-and-hypothesis",
    "href": "index.html#the-model-and-hypothesis",
    "title": "The Effect of Money Supply Shocks on Asset Prices and Inflation in the US Economy: A Bayesian SVAR Approach",
    "section": "The model and hypothesis",
<<<<<<< Updated upstream
    "text": "The model and hypothesis\nFor investigating the effect of money supply on asset prices and inflation a structural VAR model will be used in this research project. The structural VAR model with \\(p\\) lags can written as\n\\[\\begin{align}\nB_0y_t &= b_0 + B_1y_{t-1}+\\dots+B_py_{t-p}+w_t\n\\end{align}\\]\nwhere \\(y_t=[m_t\\) \\(spx_t\\) \\(hp_t\\) \\(inf_t\\) \\(ff_t\\) \\(ip_t]'\\) and contains the six variables presented above. The error term \\(u_t\\) conditioned on the past is assumed to be \\(w_t|Y_{t-1}\\sim\\;iid(\\textbf{0}_N,I_N)\\), where \\(N=6\\) in my case. The \\(B_0\\) is the so-called structural matrix and contains all contemporaneous relationships between the variables, which I essentially am interested in. However, this matrix can’t just be estimated without certain assumptions. Therefore, the first step is to premultiply \\(B_0^{-1}\\) on both sides so that we obtain the reduced form of the SVAR model:\n\\[\\begin{align}\ny_t &= \\mu_0 + A_1y_{t-1}+\\dots+A_py_{t-p}+u_t\n\\end{align}\\]\nWhere \\(A_i=B_0^{-1}B_i\\) and \\(u_t=B_0^{-1}w_t\\). It is assumed that \\(u_t|Y_{t-1}\\sim\\;iid(\\textbf{0}_N,\\Sigma)\\), which allows us to denote \\(\\Sigma = B_0^{-1} (B_0^{-1})'\\). In order to reconstruct \\(B_0^{-1}\\) and thereby identify the SVAR model restrictions on the matrix need to imposed. As \\(B_0^{-1}\\) consists of \\(K(K+1)/2\\) variables, at least \\(K(K-1)/2\\) restrictions need to be imposed. This can be done in multiple ways. In this project I will impose zero exclusion restrictions on \\(B_0^{-1}\\) by either implying recursive or a non-recursive system between the variables. It is important to note that if I choose a recursive system the ordering of \\(y_t\\) is crucial and is therefore still subject to change.\nThe estimation output I will interpret to measure how money supply shocks affect asset prices and inflation will be impulse response functions (IRFs) and forecast error variance decomposition (FEVDs). IRFs measures the dynamic response of a variable to a given shock, while FEVDs are a measure for the contribution of different shocks to the forecast error variance of a certain variable."
=======
<<<<<<< HEAD
    "text": "The model and hypothesis\nFor investigating the effect of money supply on asset prices and inflation, a structural VAR model will be used in this research project. The structural VAR model with \\(p\\) lags can written as\n\\[\\begin{align}\nB_0y_t &= b_0 + B_1y_{t-1}+\\dots+B_py_{t-p}+w_t\n\\end{align}\\]\nwhere \\(y_t=[cpi_t\\) \\(ip_t\\) \\(ff_t\\) \\(m_t\\) \\(hp_t\\) \\(spx_t]'\\) and contains the six variables presented above. The error term \\(w_t\\) conditioned on the past is assumed to be \\(w_t|Y_{t-1}\\sim\\;iid(\\textbf{0}_N,I_N)\\), where \\(N=6\\) in this case. The \\(B_0\\) is the so-called structural matrix and contains all contemporaneous relationships between the variables, which I essentially am interested in. However, this matrix cannot just be estimated without imposing certain assumptions. Therefore, the first step is to premultiply \\(B_0^{-1}\\) on both sides so that we obtain the reduced form of the SVAR model:\n\\[\\begin{align}\ny_t &= \\mu_0 + A_1y_{t-1}+\\dots+A_py_{t-p}+u_t\n\\end{align}\\]\nWhere \\(A_i=B_0^{-1}B_i\\) and \\(u_t=B_0^{-1}w_t\\). It is assumed that \\(u_t|Y_{t-1}\\sim\\;iid(\\textbf{0}_N,\\Sigma)\\), which allows us to denote \\(\\Sigma = B_0^{-1} (B_0^{-1})'\\). In order to reconstruct \\(B_0^{-1}\\) and thereby identify the SVAR model, restrictions on the matrix need to imposed. As \\(B_0^{-1}\\) consists of \\(K(K+1)/2\\) variables, at least \\(K(K-1)/2\\) restrictions need to be imposed. This can be done in multiple ways. In this project I will impose zero exclusion restrictions on \\(B_0^{-1}\\) by implying a recursive system between the variables, which has to be economically justified. I will elaborate more on this later in this research project. It is important to note that if I choose a recursive system the ordering of \\(y_t\\) is crucial.\nThe estimation output I will interpret to measure how money supply shocks affect asset prices and inflation is impulse response functions (IRFs) and forecast error variance decomposition (FEVDs). IRFs measures the dynamic response of a variable to a given shock, while FEVDs are a measure for the contribution of different shocks to the forecast error variance of a certain variable."
=======
    "text": "The model and hypothesis\nFor investigating the effect of money supply on asset prices and inflation a structural VAR model will be used in this research project. The structural VAR model with \\(p\\) lags can written as\n\\[\\begin{align}\nB_0y_t &= b_0 + B_1y_{t-1}+\\dots+B_py_{t-p}+w_t\n\\end{align}\\]\nwhere \\(y_t=[m_t\\) \\(spx_t\\) \\(hp_t\\) \\(inf_t\\) \\(ff_t\\) \\(ip_t]'\\) and contains the six variables presented above. The error term \\(u_t\\) conditioned on the past is assumed to be \\(w_t|Y_{t-1}\\sim\\;iid(\\textbf{0}_N,I_N)\\), where \\(N=6\\) in my case. The \\(B_0\\) is the so-called structural matrix and contains all contemporaneous relationships between the variables, which I essentially am interested in. However, this matrix can’t just be estimated without certain assumptions. Therefore, the first step is to premultiply \\(B_0^{-1}\\) on both sides so that we obtain the reduced form of the SVAR model:\n\\[\\begin{align}\ny_t &= \\mu_0 + A_1y_{t-1}+\\dots+A_py_{t-p}+u_t\n\\end{align}\\]\nWhere \\(A_i=B_0^{-1}B_i\\) and \\(u_t=B_0^{-1}w_t\\). It is assumed that \\(u_t|Y_{t-1}\\sim\\;iid(\\textbf{0}_N,\\Sigma)\\), which allows us to denote \\(\\Sigma = B_0^{-1} (B_0^{-1})'\\). In order to reconstruct \\(B_0^{-1}\\) and thereby identify the SVAR model restrictions on the matrix need to imposed. As \\(B_0^{-1}\\) consists of \\(K(K+1)/2\\) variables, at least \\(K(K-1)/2\\) restrictions need to be imposed. This can be done in multiple ways. In this project I will impose zero exclusion restrictions on \\(B_0^{-1}\\) by either implying recursive or a non-recursive system between the variables. It is important to note that if I choose a recursive system the ordering of \\(y_t\\) is crucial and is therefore still subject to change.\nThe estimation output I will interpret to measure how money supply shocks affect asset prices and inflation will be impulse response functions (IRFs) and forecast error variance decomposition (FEVDs). IRFs measures the dynamic response of a variable to a given shock, while FEVDs are a measure for the contribution of different shocks to the forecast error variance of a certain variable."
>>>>>>> 41d47063279825e9a18740578892ff6012578e11
>>>>>>> Stashed changes
  },
  {
    "objectID": "index.html#basic-model",
    "href": "index.html#basic-model",
    "title": "The Effect of Money Supply Shocks on Asset Prices and Inflation in the US Economy: A Bayesian SVAR Approach",
    "section": "Basic Model",
<<<<<<< Updated upstream
    "text": "Basic Model\nFirst, I redefine the model presented in the previous section to the following:\n\\[\\begin{align}\nB_0y_t &= b_0 + B_1y_{t-1} + \\dots + B_py_{t-p} u_t\\\\\n       &= B_+ x_t + u_t\n\\end{align}\\]\nWhere \\(B_+=\\big[b_0\\;B_1\\;\\dots\\;B_p\\big]\\) and \\(x_t=\\big[1\\;y_{t-1}'\\;\\dots\\;y_{t-p}'\\big]\\). As \\(B_0\\) is the structural matrix the exclusion restrictions will be set on its rows such that \\(B_0=\\left[b_1V_1\\;\\dots\\;b_NV_N\\right]'\\) holds, where \\(B_{0[n\\cdot]}=b_n\\;V_n\\) and represents the \\(n\\)th row of \\(B_0\\). The dimension of \\(b_n\\) is \\(1\\times r_n\\) and is a vector of the unrestricted elements of the \\(n\\)th row of \\(B_0\\). The matrix \\(V_n\\) is of dimension \\(r_n\\times N\\) and consists only of ones and zeroes since it is the restriction matrix. Now the structural model can be written equation-by-equation in the following way:\n\\[\\begin{align}\nb_nV_ny_t &= B_nx_t + u_{n.t}\\\\\nu_{n.t}   &\\sim \\mathcal{N}(0,1)\n\\end{align}\\]\nWhich subsequently can be rewritten in matrix form as:\n\\[\\begin{align}\nb_nV_nY &= B_nX+ U_n\\\\\nU_n   &\\sim \\mathcal{N}(0_T,I_T)\n\\end{align}\\]\nwhere \\(\\underset{(N \\times T)}{Y}=\\begin{pmatrix}  y_1, \\dots , y_T \\end{pmatrix}\\), \\(\\underset{(K \\times T)}{X}=\\begin{pmatrix}  x_1, \\dots , x_T \\end{pmatrix}\\), \\(\\underset{(1 \\times T)}{U_n}=\\begin{pmatrix}  u_{n.1}, \\dots , u_{n.T} \\end{pmatrix}\\) and \\(\\underset{(1 \\times K)}{B_n}= B_{+[n.]}\\).\nFor convenience the likelihood function of \\(B_0\\) and \\(B_+\\) given data can be written as a \\(\\mathcal{NGN}\\) distribution:\n\\[\\begin{align}\nL(B_+,B_0 | Y, X) \\propto |\\det(B_0)|^T \\exp \\left\\{-\\frac{1}{2} \\sum_{n=1}^N (b_nV_nY-B_nX)(b_nV_nY-B_nX)'  \\right\\}\n\\end{align}\\]\nMoving to the prior distribution, the \\(\\mathcal{NGN}\\) distribution is being used as a natural-conjugate prior. Therefore, I define \\(p(B_+,B_0)\\sim \\mathcal{NGN}(\\underline{B}, \\underline{\\Omega}, \\underline{S}, \\underline{\\nu})\\), where the following holds:\n\\[\\begin{align}\np(B_+,B_0)&=\\left(\\prod_{n=1}^N p(B_n|b_n)\\right)p(b_1,\\dots,b_n)\\\\\np(B_n|b_n)&\\sim \\mathcal{N}_K (b_nV_n\\underline{B},\\underline{\\Omega})\\\\\np(b_1,\\dots,b_n) &\\propto |\\det (B_0)|^{\\underline{\\nu}-N} \\exp \\left\\{-\\frac{1}{2}\\sum_{n=1}^Nb_nV_n\\underline{S}^{-1}V_n'b_n'\\right\\}\n\\end{align}\\]\nWhich results in the following kernel of the natural-conjugate prior distribution:\n\\[\\begin{align}\n|\\det(B_0)|^{\\underline{\\nu}-N} \\exp \\left\\{-\\frac{1}{2} \\sum_{n=1}^N b_nV_n\\underline{S}^{-1}V_n'B_n'\\right\\} \\times \\exp \\left\\{-\\frac{1}{2} \\sum_{n=1}^N (B_n-b_nV_n\\underline{B})\\underline{\\Omega}^{-1}(B_n-b_nV_n\\underline{B})'\\right\\}\n\\end{align}\\]\nFor the prior parameters the Minnesota prior parameters are being exploited:\n\\[\\begin{align}\n\\underline{B} &= \\left[0_{N\\times 1}\\;I_N\\;0_{N\\times(p-1)N}\\right]\\\\\n\\underline{\\Omega} &= \\text{diag} \\left(\\left[\\kappa_2\\;\\kappa_1(\\textbf{p}^{-2}\\otimes I_N')\\right)\\right]\\\\\n\\underline{S} &= \\kappa_0I_N\\\\\n\\underline{\\nu} &= N\n\\end{align}\\]\nThis enables us to derive the posterior distribution:\n\\[\\begin{align}\np(B_+,B_0|Y,X) &\\propto L(B_+,B_0|Y,X)p(B_+,B_0)\\\\\n               &\\propto |\\det(B_0)|^T \\exp \\left\\{-\\frac{1}{2} \\sum_{n=1}^N (b_nV_nY-B_nX)(b_nV_nY-B_nX)'  \\right\\}\\\\\n               &\\times |\\det(B_0)|^{\\underline{\\nu}-N} \\exp \\left\\{-\\frac{1}{2} \\sum_{n=1}^N b_nV_n\\underline{S}^{-1}V_n'B_n'\\right\\} \\\\ &\\times \\exp \\left\\{-\\frac{1}{2} \\sum_{n=1}^N (B_n-b_nV_n\\underline{B})\\underline{\\Omega}^{-1}(B_n-b_nV_n\\underline{B})'\\right\\}\n\\end{align}\\]\nBy performing appropriate operations this can be expressed more densely the following way:\n\\[\\begin{align}\np(B_+,B_0|Y,X) &\\propto |\\det(B_0)|^{T+\\underline{\\nu}-N} \\exp \\left\\{-\\frac{1}{2} \\sum_{n=1}^N (B_n-b_nV_n\\bar{B})\\bar{\\Omega}^{-1}(B_n-b_nV_n\\bar{B})'+b_nV_n\\bar{S}^{-1}V_n'b_n'\\right\\}\n\\end{align}\\]\nLeading to the following posterior parameters:\n\\[\\begin{align}\np(B_+,B_0|Y,X) &\\sim \\mathcal{NGN}(\\bar{B},\\bar{\\Omega},\\bar{S},\\bar{\\nu})\\\\\n\\bar{\\Omega}&=\\left[XX'+\\underline{\\Omega}^{-1}\\right]^{-1}\\\\\n\\bar{B}&=\\left[YX'+\\underline{B\\Omega}^{-1}\\right]\\bar{\\Omega}\\\\\n\\bar{S}&=\\left[YY'+\\underline{S}^{-1}+\\underline{B\\Omega}^{-1}\\underline{B}'-\\bar{B}\\bar{\\Omega}^{-1}\\bar{B}'\\right]^{-1}\\\\\n\\bar{\\nu}&= T+\\underline{\\nu}\n\\end{align}\\]"
=======
<<<<<<< HEAD
    "text": "Basic Model\nFirst, I redefine the model presented in the previous section to the following:\n\\[\\begin{align}\nB_0y_t &= b_0 + B_1y_{t-1} + \\dots + B_py_{t-p} u_t\\\\\n       &= B_+ x_t + u_t\n\\end{align}\\]\nWhere \\(B_+=\\big[b_0\\;B_1\\;\\dots\\;B_p\\big]\\) and \\(x_t=\\big[1\\;y_{t-1}'\\;\\dots\\;y_{t-p}'\\big]\\). As \\(B_0\\) is the structural matrix the exclusion restrictions will be set on its rows such that \\(B_0=\\left[b_1V_1\\;\\dots\\;b_NV_N\\right]'\\) holds, where \\(B_{0[n\\cdot]}=b_n\\;V_n\\) and represents the \\(n\\)th row of \\(B_0\\). The dimension of \\(b_n\\) is \\(1\\times r_n\\) and is a vector of the unrestricted elements of the \\(n\\)th row of \\(B_0\\). The matrix \\(V_n\\) is of dimension \\(r_n\\times N\\) and consists only of ones and zeroes since it is the restriction matrix. Now the structural model can be written equation-by-equation in the following way:\n\\[\\begin{align}\nb_nV_ny_t &= B_nx_t + u_{n.t}\\\\\nu_{n.t}   &\\sim \\mathcal{N}(0,1)\n\\end{align}\\]\nWhich subsequently can be rewritten in matrix form as:\n\\[\\begin{align}\nb_nV_nY &= B_nX+ U_n\\\\\nU_n   &\\sim \\mathcal{N}(0_T,I_T)\n\\end{align}\\]\nwhere \\(\\underset{(N \\times T)}{Y}=\\begin{pmatrix} y_1, \\dots , y_T \\end{pmatrix}\\), \\(\\underset{(K \\times T)}{X}=\\begin{pmatrix} x_1, \\dots , x_T \\end{pmatrix}\\), \\(\\underset{(1 \\times T)}{U_n}=\\begin{pmatrix} u_{n.1}, \\dots , u_{n.T} \\end{pmatrix}\\) and \\(\\underset{(1 \\times K)}{B_n}= B_{+[n.]}\\).\nFor convenience the likelihood function of \\(B_0\\) and \\(B_+\\) given data can be written as a \\(\\mathcal{NGN}\\) distribution:\n\\[\\begin{align}\nL(B_+,B_0 | Y, X) \\propto |\\det(B_0)|^T \\exp \\left\\{-\\frac{1}{2} \\sum_{n=1}^N (b_nV_nY-B_nX)(b_nV_nY-B_nX)'  \\right\\}\n\\end{align}\\]\nMoving to the prior distribution, the \\(\\mathcal{NGN}\\) distribution is being used as a natural-conjugate prior. Therefore, I define \\(p(B_+,B_0)\\sim \\mathcal{NGN}(\\underline{B}, \\underline{\\Omega}, \\underline{S}, \\underline{\\nu})\\), where the following holds:\n\\[\\begin{align}\np(B_+,B_0)&=\\left(\\prod_{n=1}^N p(B_n|b_n)\\right)p(b_1,\\dots,b_n)\\\\\np(B_n|b_n)&\\sim \\mathcal{N}_K (b_nV_n\\underline{B},\\underline{\\Omega})\\\\\np(b_1,\\dots,b_n) &\\propto |\\det (B_0)|^{\\underline{\\nu}-N} \\exp \\left\\{-\\frac{1}{2}\\sum_{n=1}^Nb_nV_n\\underline{S}^{-1}V_n'b_n'\\right\\}\n\\end{align}\\]\nWhich results in the following kernel of the natural-conjugate prior distribution:\n\\[\\begin{align}\n|\\det(B_0)|^{\\underline{\\nu}-N} \\exp \\left\\{-\\frac{1}{2} \\sum_{n=1}^N b_nV_n\\underline{S}^{-1}V_n'B_n'\\right\\} \\times \\exp \\left\\{-\\frac{1}{2} \\sum_{n=1}^N (B_n-b_nV_n\\underline{B})\\underline{\\Omega}^{-1}(B_n-b_nV_n\\underline{B})'\\right\\}\n\\end{align}\\]\nFor the prior parameters the Minnesota prior parameters are being exploited:\n\\[\\begin{align}\n\\underline{B} &= \\left[0_{N\\times 1}\\;I_N\\;0_{N\\times(p-1)N}\\right]\\\\\n\\underline{\\Omega} &= \\text{diag} \\left(\\left[\\kappa_2\\;\\kappa_1(\\textbf{p}^{-2}\\otimes I_N')\\right)\\right]\\\\\n\\underline{S} &= \\kappa_0I_N\\\\\n\\underline{\\nu} &= N\n\\end{align}\\]\nThis enables us to derive the posterior distribution:\n\\[\\begin{align}\np(B_+,B_0|Y,X) &\\propto L(B_+,B_0|Y,X)p(B_+,B_0)\\\\\n               &\\propto |\\det(B_0)|^T \\exp \\left\\{-\\frac{1}{2} \\sum_{n=1}^N (b_nV_nY-B_nX)(b_nV_nY-B_nX)'  \\right\\}\\\\\n               &\\times |\\det(B_0)|^{\\underline{\\nu}-N} \\exp \\left\\{-\\frac{1}{2} \\sum_{n=1}^N b_nV_n\\underline{S}^{-1}V_n'B_n'\\right\\} \\\\ &\\times \\exp \\left\\{-\\frac{1}{2} \\sum_{n=1}^N (B_n-b_nV_n\\underline{B})\\underline{\\Omega}^{-1}(B_n-b_nV_n\\underline{B})'\\right\\}\n\\end{align}\\]\nBy performing appropriate operations this can be expressed more densely the following way:\n\\[\\begin{align}\np(B_+,B_0|Y,X) &\\propto |\\det(B_0)|^{T+\\underline{\\nu}-N} \\exp \\left\\{-\\frac{1}{2} \\sum_{n=1}^N (B_n-b_nV_n\\bar{B})\\bar{\\Omega}^{-1}(B_n-b_nV_n\\bar{B})'+b_nV_n\\bar{S}^{-1}V_n'b_n'\\right\\}\n\\end{align}\\]\nLeading to the following posterior parameters:\n\\[\\begin{align}\np(B_+,B_0|Y,X) &\\sim \\mathcal{NGN}(\\bar{B},\\bar{\\Omega},\\bar{S},\\bar{\\nu})\\\\\n\\bar{\\Omega}&=\\left[XX'+\\underline{\\Omega}^{-1}\\right]^{-1}\\\\\n\\bar{B}&=\\left[YX'+\\underline{B\\Omega}^{-1}\\right]\\bar{\\Omega}\\\\\n\\bar{S}&=\\left[YY'+\\underline{S}^{-1}+\\underline{B\\Omega}^{-1}\\underline{B}'-\\bar{B}\\bar{\\Omega}^{-1}\\bar{B}'\\right]^{-1}\\\\\n\\bar{\\nu}&= T+\\underline{\\nu}\n\\end{align}\\]"
=======
    "text": "Basic Model\nFirst, I redefine the model presented in the previous section to the following:\n\\[\\begin{align}\nB_0y_t &= b_0 + B_1y_{t-1} + \\dots + B_py_{t-p} u_t\\\\\n       &= B_+ x_t + u_t\n\\end{align}\\]\nWhere \\(B_+=\\big[b_0\\;B_1\\;\\dots\\;B_p\\big]\\) and \\(x_t=\\big[1\\;y_{t-1}'\\;\\dots\\;y_{t-p}'\\big]\\). As \\(B_0\\) is the structural matrix the exclusion restrictions will be set on its rows such that \\(B_0=\\left[b_1V_1\\;\\dots\\;b_NV_N\\right]'\\) holds, where \\(B_{0[n\\cdot]}=b_n\\;V_n\\) and represents the \\(n\\)th row of \\(B_0\\). The dimension of \\(b_n\\) is \\(1\\times r_n\\) and is a vector of the unrestricted elements of the \\(n\\)th row of \\(B_0\\). The matrix \\(V_n\\) is of dimension \\(r_n\\times N\\) and consists only of ones and zeroes since it is the restriction matrix. Now the structural model can be written equation-by-equation in the following way:\n\\[\\begin{align}\nb_nV_ny_t &= B_nx_t + u_{n.t}\\\\\nu_{n.t}   &\\sim \\mathcal{N}(0,1)\n\\end{align}\\]\nWhich subsequently can be rewritten in matrix form as:\n\\[\\begin{align}\nb_nV_nY &= B_nX+ U_n\\\\\nU_n   &\\sim \\mathcal{N}(0_T,I_T)\n\\end{align}\\]\nwhere \\(\\underset{(N \\times T)}{Y}=\\begin{pmatrix}  y_1, \\dots , y_T \\end{pmatrix}\\), \\(\\underset{(K \\times T)}{X}=\\begin{pmatrix}  x_1, \\dots , x_T \\end{pmatrix}\\), \\(\\underset{(1 \\times T)}{U_n}=\\begin{pmatrix}  u_{n.1}, \\dots , u_{n.T} \\end{pmatrix}\\) and \\(\\underset{(1 \\times K)}{B_n}= B_{+[n.]}\\).\nFor convenience the likelihood function of \\(B_0\\) and \\(B_+\\) given data can be written as a \\(\\mathcal{NGN}\\) distribution:\n\\[\\begin{align}\nL(B_+,B_0 | Y, X) \\propto |\\det(B_0)|^T \\exp \\left\\{-\\frac{1}{2} \\sum_{n=1}^N (b_nV_nY-B_nX)(b_nV_nY-B_nX)'  \\right\\}\n\\end{align}\\]\nMoving to the prior distribution, the \\(\\mathcal{NGN}\\) distribution is being used as a natural-conjugate prior. Therefore, I define \\(p(B_+,B_0)\\sim \\mathcal{NGN}(\\underline{B}, \\underline{\\Omega}, \\underline{S}, \\underline{\\nu})\\), where the following holds:\n\\[\\begin{align}\np(B_+,B_0)&=\\left(\\prod_{n=1}^N p(B_n|b_n)\\right)p(b_1,\\dots,b_n)\\\\\np(B_n|b_n)&\\sim \\mathcal{N}_K (b_nV_n\\underline{B},\\underline{\\Omega})\\\\\np(b_1,\\dots,b_n) &\\propto |\\det (B_0)|^{\\underline{\\nu}-N} \\exp \\left\\{-\\frac{1}{2}\\sum_{n=1}^Nb_nV_n\\underline{S}^{-1}V_n'b_n'\\right\\}\n\\end{align}\\]\nWhich results in the following kernel of the natural-conjugate prior distribution:\n\\[\\begin{align}\n|\\det(B_0)|^{\\underline{\\nu}-N} \\exp \\left\\{-\\frac{1}{2} \\sum_{n=1}^N b_nV_n\\underline{S}^{-1}V_n'B_n'\\right\\} \\times \\exp \\left\\{-\\frac{1}{2} \\sum_{n=1}^N (B_n-b_nV_n\\underline{B})\\underline{\\Omega}^{-1}(B_n-b_nV_n\\underline{B})'\\right\\}\n\\end{align}\\]\nFor the prior parameters the Minnesota prior parameters are being exploited:\n\\[\\begin{align}\n\\underline{B} &= \\left[0_{N\\times 1}\\;I_N\\;0_{N\\times(p-1)N}\\right]\\\\\n\\underline{\\Omega} &= \\text{diag} \\left(\\left[\\kappa_2\\;\\kappa_1(\\textbf{p}^{-2}\\otimes I_N')\\right)\\right]\\\\\n\\underline{S} &= \\kappa_0I_N\\\\\n\\underline{\\nu} &= N\n\\end{align}\\]\nThis enables us to derive the posterior distribution:\n\\[\\begin{align}\np(B_+,B_0|Y,X) &\\propto L(B_+,B_0|Y,X)p(B_+,B_0)\\\\\n               &\\propto |\\det(B_0)|^T \\exp \\left\\{-\\frac{1}{2} \\sum_{n=1}^N (b_nV_nY-B_nX)(b_nV_nY-B_nX)'  \\right\\}\\\\\n               &\\times |\\det(B_0)|^{\\underline{\\nu}-N} \\exp \\left\\{-\\frac{1}{2} \\sum_{n=1}^N b_nV_n\\underline{S}^{-1}V_n'B_n'\\right\\} \\\\ &\\times \\exp \\left\\{-\\frac{1}{2} \\sum_{n=1}^N (B_n-b_nV_n\\underline{B})\\underline{\\Omega}^{-1}(B_n-b_nV_n\\underline{B})'\\right\\}\n\\end{align}\\]\nBy performing appropriate operations this can be expressed more densely the following way:\n\\[\\begin{align}\np(B_+,B_0|Y,X) &\\propto |\\det(B_0)|^{T+\\underline{\\nu}-N} \\exp \\left\\{-\\frac{1}{2} \\sum_{n=1}^N (B_n-b_nV_n\\bar{B})\\bar{\\Omega}^{-1}(B_n-b_nV_n\\bar{B})'+b_nV_n\\bar{S}^{-1}V_n'b_n'\\right\\}\n\\end{align}\\]\nLeading to the following posterior parameters:\n\\[\\begin{align}\np(B_+,B_0|Y,X) &\\sim \\mathcal{NGN}(\\bar{B},\\bar{\\Omega},\\bar{S},\\bar{\\nu})\\\\\n\\bar{\\Omega}&=\\left[XX'+\\underline{\\Omega}^{-1}\\right]^{-1}\\\\\n\\bar{B}&=\\left[YX'+\\underline{B\\Omega}^{-1}\\right]\\bar{\\Omega}\\\\\n\\bar{S}&=\\left[YY'+\\underline{S}^{-1}+\\underline{B\\Omega}^{-1}\\underline{B}'-\\bar{B}\\bar{\\Omega}^{-1}\\bar{B}'\\right]^{-1}\\\\\n\\bar{\\nu}&= T+\\underline{\\nu}\n\\end{align}\\]"
>>>>>>> 41d47063279825e9a18740578892ff6012578e11
>>>>>>> Stashed changes
  },
  {
    "objectID": "index.html#the-gibbs-sampler",
    "href": "index.html#the-gibbs-sampler",
    "title": "The Effect of Money Supply Shocks on Asset Prices and Inflation in the US Economy: A Bayesian SVAR Approach",
    "section": "The Gibbs Sampler",
    "text": "The Gibbs Sampler\nHaving derived the posterior parameters, the gibbs sampler can now be scrutinized. As already outlined, the sampler is based on the \\(\\mathcal{NGN}\\) distribution. Further, the algorithm is divided into two steps. First, \\(B_0\\) is drawn \\(S1+S2\\) times from \\[\\begin{gather*}\n    p(b_n | Y, X, b_1, \\dots, b_{n-1}, b_{n+1}, \\dots, b_N)\n\\end{gather*}\\] From which we get the posterior samples \\(\\{b_1^{(s)},\\dots, b_N^{(s)}\\}^{S}_{s=1}\\). Next step is to normalize these samples, so we subsequently can sample \\(B_n\\) directly for each draw of \\(b_n^{(s)}\\) from \\(p(B_n|Y,X,b_n)\\). Based on this, the posterior draws \\(\\left\\{B_+^{(s)},B_0^{(s)}\\right\\}_{s=1}^{S1+S2}\\) can be returned.\nThe gibbs sampler for \\(b_n^{(s)} \\sim p(b_n | Y, X, b_1^{(s)}, \\dots, b_{n-1}^{(s)}, b_{n+1}^{(s-1)}, \\dots, b_N^{(s-1)})\\) is computed by following the algorithm proposed by Waggoner & Zha 2003. To facilitate this, following is defined:\n\n\\(U_n = \\text{chol}\\Big(\\bar{\\nu}\\Big(V_n\\bar{S}^{-1}V_n'\\Big)^{-1}\\Big)\\), where \\(U_n\\) is a \\(r_n \\times r_n\\) upper-triangular matrix.\n\\(w = \\left[B_{0[-n.]}^{(s)}\\right]_\\perp\\), where \\(w\\) is a \\(1 \\times N\\) matrix.\n\\(w_1 = wV_n'U_n'\\cdot \\Big( wV_n'U_n'V_nU_nw'\\Big)^{-\\frac{1}{2}}\\), where \\(w_1\\) is a \\(1 \\times r_n\\) vector.\n\\(W_n=\\begin{pmatrix} w_1' & w_{1\\perp}' \\end{pmatrix}\\), where \\(W_n\\) is a matrix of dimensions \\(r_n \\times r_n\\).\n\nThe \\(1 \\times r_n\\) matrix \\(\\alpha_n\\) can now be constructed by drawing the first element of \\(\\alpha_n\\) by following this procedure:\n\nDraw \\(u \\sim N(0_{\\nu+1},{\\bar{\\nu}^{-1}I_{\\nu+1}})\\)\nSet \\(\\alpha_{n[\\cdot 1]} = \\begin{cases}\\sqrt{u'u} \\text{ with probability 0.5}\\\\-\\sqrt{u'u} \\text{ with probability 0.5}\\end{cases}\\)\n\nThe remaining \\(r_n-1\\) elements of \\(\\alpha_n\\) can be drawn from \\(N(0_{r_n-1},\\bar{\\nu}^{-1}I_{r_n-1})\\), after which the draw of the full conditional distribution of \\(b_n\\) can be computed by \\(b_n^{(s)}\\alpha_nW_nU_n\\).\nAs already mentioned, these samples need to be normalized in order to ensure that a unique maximum is being found. I will not go into details of this procedure here, but rather refer to Waggoner & Zha (2003) for a rigorous outline."
  },
  {
    "objectID": "index.html#r-code-snippets",
    "href": "index.html#r-code-snippets",
    "title": "The Effect of Money Supply Shocks on Asset Prices and Inflation in the US Economy: A Bayesian SVAR Approach",
    "section": "R Code Snippets",
<<<<<<< Updated upstream
    "text": "R Code Snippets\nThis section provides the R code behind the estimation procedure. In order to facilitate this, the following R functions are being used by the courtesy of Tomasz Wozníak.\nThe following function computes an orthogonal complement matrix to the input x, which is used in the rgn() function presented below.\n\northogonal.complement.matrix.TW = function(x){\n  # x is a mxn matrix and m>n\n  # the function returns a mx(m-n) matrix, out, that is an orthogonal complement of x, i.e.:\n  # t(x)%*%out = 0 and det(cbind(x,out))!=0\n  if( dim(x)[1] == 1 & dim(x)[2] == 2){\n    x = t(x)\n  }\n  # x <- ifelse(dim(x)[1] == 1 && dim(x)[2] == 2, t(x), x)\n  N     = dim(x)\n  tmp   = qr.Q(qr(x, tol = 1e-10),complete=TRUE)\n  out   = as.matrix(tmp[,(N[2]+1):N[1]])\n  return(out)\n}\n\nThe rgn() function simulates draws for \\(b_n\\) from a \\(\\mathcal{NGN}\\) distribution\n\nrgn             = function(n,S.inv,nu,V,B0.initial){\n  # This function simulates draws for the unrestricted elements \n  # of the conteporaneous relationships matrix of an SVAR model\n  # from a generalized-normal distribution according to algorithm \n  # by Waggoner & Zha (2003, JEDC)\n  # n     - a positive integer, the number of draws to be sampled\n  # S     - an NxN positive definite matrix, a parameter of the generalized-normal distribution\n  # nu    - a positive scalar, degrees of freedom parameter\n  # V     - an N-element list, with fixed matrices\n  # B0.initial - an NxN matrix, of initial values of the parameters\n  \n  N             = nrow(B0.initial)\n  no.draws      = n\n  \n  B0            = array(NA, c(N,N,no.draws))\n  B0.aux        = B0.initial\n  \n  for (i in 1:no.draws){\n    for (n in 1:N){\n      rn            = nrow(V[[n]])\n      Un            = chol(nu*solve(V[[n]]%*%S.inv%*%t(V[[n]])))\n      w             = t(orthogonal.complement.matrix.TW(t(B0.aux[-n,])))\n      w1            = w %*% t(V[[n]]) %*% t(Un) / sqrt(as.numeric(w %*% t(V[[n]]) %*% t(Un) %*% Un %*% V[[n]] %*% t(w)))\n      if (rn>1){\n        Wn          = cbind(t(w1),orthogonal.complement.matrix.TW(t(w1)))\n      } else {\n        Wn          = w1\n      }\n      alpha         = rep(NA,rn)\n      u             = rmvnorm(1,rep(0,nu+1),(1/nu)*diag(nu+1))\n      alpha[1]      = sqrt(as.numeric(u%*%t(u)))\n      if (runif(1)<0.5){\n        alpha[1]    = -alpha[1]\n      }\n      if (rn>1){\n        alpha[2:rn] = rmvnorm(1,rep(0,nrow(V[[n]])-1),(1/nu)*diag(rn-1))\n      }\n      bn            = alpha %*% Wn %*% Un\n      B0.aux[n,]    = bn %*% V[[n]]\n    }\n    B0[,,i]         = B0.aux\n  }\n  \n  return(B0)\n}\n\nThe next function normalizes the matrix of the contemporaneous effects, \\(B_0\\):\n\nnormalization.wz2003  = function(B0,B0.hat.inv, Sigma.inv, diag.signs){\n  # This function normalizes a matrix of contemporaneous effects\n  # according to the algorithm by Waggoner & Zha (2003, JOE)\n  # B0        - an NxN matrix, to be normalized\n  # B0.hat    - an NxN matrix, a normalized matrix\n  \n  N                 = nrow(B0)\n  K                 = 2^N\n  distance          = rep(NA,K)\n  for (k in 1:K){\n    B0.tmp.inv      = solve(diag(diag.signs[k,]) %*% B0)\n    distance[k]     = sum(\n      unlist(\n        lapply(1:N,\n               function(n){\n                 t(B0.tmp.inv - B0.hat.inv)[n,] %*%Sigma.inv %*% t(B0.tmp.inv - B0.hat.inv)[n,]\n               }\n        )))\n  }\n  B0.out            = diag(diag.signs[which.min(distance),]) %*% B0\n  \n  return(B0.out)\n}\n\nThis function normalizes the output from the rgn() function, ensuring that we obtain a unique maximum\n\nnormalize.Gibbs.output.parallel          = function(B0.posterior,B0.hat){\n  # This function normalizes the Gibbs sampler output from function rgn\n  # using function normalization.wz2003 \n  # B0.posterior  - a list, output from function rgn\n  # B0.hat        - an NxN matrix, a normalized matrix\n  \n  N                 = nrow(B0.hat)\n  K                 = 2^N\n  \n  B0.hat.inv        = solve(B0.hat)\n  Sigma.inv         = t(B0.hat)%*%B0.hat\n  \n  diag.signs        = matrix(NA,2^N,N)\n  for (n in 1:N){\n    diag.signs[,n]  = kronecker(c(-1,1),rep(1,2^(n-1)))\n  }\n  \n  B0.posterior.n    = mclapply(1:dim(B0.posterior)[3],function(i){\n    normalization.wz2003(B0=B0.posterior[,,i],B0.hat.inv, Sigma.inv, diag.signs)\n  },mc.cores=1\n  )\n  B0.posterior.n  = simplify2array(B0.posterior.n)\n  \n  return(B0.posterior.n)\n}\n\nLastly, a function for simulating the draws of the multivariate normal distribution of the autoregressive slope matrix, \\(B_+\\), is needed\n\nrnorm.ngn       = function(B0.posterior,B,Omega){\n  # This function simulates draws for the multivariate normal distribution\n  # of the autoregressive slope matrix of an SVAR model\n  # from a normal-generalized-normal distribution according to algorithm \n  # by Waggoner & Zha (2003, JEDC)\n  # B0.posterior  - a list, output from function rgn\n  # B             - an NxK matrix, a parameter determining the mean of the multivariate conditionally normal distribution given B0\n  # Omega         - a KxK positive definite matrix, a covariance matrix of the multivariate normal distribution\n  \n  N             = nrow(B)\n  K             = ncol(B)\n  no.draws      = dim(B0.posterior)[3]\n  L             = t(chol(Omega))\n  \n  Bp.posterior  = lapply(1:no.draws,function(i){\n    Bp          = matrix(NA, N, K)\n    for (n in 1:N){\n      Bp[n,]    = as.vector(t(B0.posterior[n,,i] %*% B) + L%*%rnorm(K))\n    }\n    return(Bp)\n  })\n  Bp.posterior  = simplify2array(Bp.posterior)\n  return(Bp.posterior)\n}\n\nHaving set up all the necessary functions, I now simulate a bivariate random walk to produce artificial data.\n\n#Simulation of data\n\np = 1\nT = 500\nN = 2\nK = 1 + N*p\n\nY           = arima.sim(list(order = c(0,1,0)), n = T + p-1, mean = 0, sd =1)\nfor (i in 2:N){\n  Y         = rbind(Y, arima.sim(list(order = c(0,1,0)), n = T + p-1, mean = 0, sd = 1))\n}\n\nX           = matrix(1,1,T)\nfor (i in 1:p){\n  X         = rbind(X, Y[,(p+1-i):(ncol(Y)-i)])\n}\nY           = Y[,-p]\nartificialdata  = list(p = p, N = N, K = K, Y = Y, X = X)\n\n#This model requires the Y and X matrix to be transposed \n# Y       = t(Y)\n# X       = t(X)\n\nNext, I set the priors in regards to the specification from above. Further, I create the restriction matrix \\(V_n\\). Note, that I imply a recursive structure in the system.\n\n# set the priors\nkappa0     = 10\nkappa1     = .1  \nkappa2     = 10\n\npriors     = list(\n  B        = cbind(rep(0,N), diag(N), matrix(0, N, (p-1)*N)),\n  Omega    = diag(c(kappa2,kappa1*((1:p)^(-2))%x%rep(1,N))),\n  S        = kappa0*diag(N),\n  nu       = N\n)\n\n# create the V matrices\nFF.V           = vector(\"list\",N)\nfor (n in 1:N){\n  FF.V[[n]]   = cbind(diag(n),matrix(0,n,N-n))\n}\n\n# create initial B0 matrix\nB0.initial = matrix(0,N,N)\nfor (n in 1:N){\n  unrestricted    = apply(FF.V[[n]],2,sum)==1\n  B0.initial[n,unrestricted] = rnorm(sum(unrestricted))\n}\n\nFinally, the gibbs sampler for the basic model can be presented\n\nGibbs.sampler.base <- function(p,Y,X,priors,S1,S2, FF.V, B0.initial){\n\n  N       = nrow(Y)\n  p       = 1 # calculate from X and Y (K and N)\n  K       = 1+N*p\n  S1      = S1\n  S2      = S2\n  kappa0 = 10\n  kappa1 = 10\n  kappa2 = 0.1\n\n  B0.posterior    <- array(NA,c(N,N,(S1+S2)))\n  Bp.posterior    <- array(NA,c(N,(1+N*p),(S1+S2)))\n\n  for (s in 1:(S1+S2)){\n\n    # Computing posterior parameters\n    Omega.inv      = solve(priors$Omega)\n    Omega.post.inv = X%*%t(X) + Omega.inv\n    Omega.post     = solve(Omega.post.inv)\n    B.post         = (Y%*%t(X) + priors$B%*%Omega.inv) %*% Omega.post\n    S.post         = Y%*%t(Y) + solve(priors$S) + priors$B%*%Omega.inv%*%t(priors$B) -   B.post%*%Omega.post.inv%*%t(B.post) \n    nu.post        = ncol(Y) + priors$nu\n\n    # Use B0.initial for first iteration, otherwise the latest draw from B0.posterior\n    if (s==1) {\n      B0.s = B0.initial\n    } else {\n      B0.s = B0.posterior[,,s-1]\n    }\n\n    # sampling one draw B0 from the posterior distribution using Gibbs\n    # rgn.function samples from a random conditional generalized normal distribution\n    B0.tmp                  = rgn(n=1, S.inv=S.post, nu=nu.post, V=FF.V, B0.initial=B0.s)\n    B0.posterior[,,s]       = B0.tmp[,,1]\n\n    # sample one draw B+ from the normal conditional posterior\n    Bp.tmp              = rnorm.ngn(B0.tmp, B=B.post,Omega=Omega.post)\n    Bp.posterior[,,s]   = Bp.tmp[,,1]\n  }\n  # END OF GIBBS\n  #Discard first S1 draws\n  B0.posterior <- B0.posterior[,,(S1+1):(S1+S2)]\n  Bp.posterior <- Bp.posterior[,,(S1+1):(S1+S2)]\n\n  #normalisation of B0.posterior and Bp.posterior\n  B0.hat             = diag(sign(diag(B0.tmp[,,1]))) %*% B0.tmp[,,1]\n\n  B0.posterior.N    <- array(NA,c(N,N,S2))\n  Bp.posterior.N    <- array(NA,c(N,(1+N*p),S2))\n\n  B0.posteror.N.tmp      =  normalize.Gibbs.output.parallel(B0.posterior,B0.hat=B0.hat)\n  for (s in 1:S2){\n    B0.posterior.N[,,s]    = B0.posteror.N.tmp[,,s]\n    Bp.posterior.N[,,s]    = B0.posterior.N[,,s]%*%solve(B0.posterior[,,s])%*%Bp.posterior[,,s]\n  }\n\n  return(list(B0.posterior.N = B0.posterior.N,\n              Bp.posterior.N = Bp.posterior.N))\n}\n\n\n\n            [,1]      [,2]\n[1,]  1.02005371 0.0000000\n[2,] -0.01974985 0.9780032\n\n\n           [,1]        [,2]       [,3]\n[1,]  0.1242882  0.99930421 0.01297839\n[2,] -0.2482267 -0.02849457 0.96587384\n\n\nSince a bivariate random walk was simulated, the \\(B_0\\) matrix should be an identity matrix. Further, the first column of \\(B_+\\) should be zero and the matrix \\(B_+[,2:3]\\) should also be an identity matrix. This is also approximately the case, which indicates the estimation procedure is correct."
=======
<<<<<<< HEAD
    "text": "R Code Snippets\nThis section provides the R code behind the estimation procedure. In order to facilitate this, the following R functions are being used by the courtesy of Tomasz Wozníak.\nThe following function computes an orthogonal complement matrix to the input x, which is used in the rgn() function presented below.\n\northogonal.complement.matrix.TW = function(x){\n  # x is a mxn matrix and m>n\n  # the function returns a mx(m-n) matrix, out, that is an orthogonal complement of x, i.e.:\n  # t(x)%*%out = 0 and det(cbind(x,out))!=0\n  if( dim(x)[1] == 1 & dim(x)[2] == 2){\n    x = t(x)\n  }\n  # x <- ifelse(dim(x)[1] == 1 && dim(x)[2] == 2, t(x), x)\n  N     = dim(x)\n  tmp   = qr.Q(qr(x, tol = 1e-10),complete=TRUE)\n  out   = as.matrix(tmp[,(N[2]+1):N[1]])\n  return(out)\n}\n\nThe rgn() function simulates draws for \\(b_n\\) from a \\(\\mathcal{NGN}\\) distribution\n\nrgn             = function(n,S.inv,nu,V,B0.initial){\n  # This function simulates draws for the unrestricted elements \n  # of the conteporaneous relationships matrix of an SVAR model\n  # from a generalized-normal distribution according to algorithm \n  # by Waggoner & Zha (2003, JEDC)\n  # n     - a positive integer, the number of draws to be sampled\n  # S     - an NxN positive definite matrix, a parameter of the generalized-normal distribution\n  # nu    - a positive scalar, degrees of freedom parameter\n  # V     - an N-element list, with fixed matrices\n  # B0.initial - an NxN matrix, of initial values of the parameters\n  \n  N             = nrow(B0.initial)\n  no.draws      = n\n  \n  B0            = array(NA, c(N,N,no.draws))\n  B0.aux        = B0.initial\n  \n  for (i in 1:no.draws){\n    for (n in 1:N){\n      rn            = nrow(V[[n]])\n      Un            = chol(nu*solve(V[[n]]%*%S.inv%*%t(V[[n]])))\n      w             = t(orthogonal.complement.matrix.TW(t(B0.aux[-n,])))\n      w1            = w %*% t(V[[n]]) %*% t(Un) / sqrt(as.numeric(w %*% t(V[[n]]) %*% t(Un) %*% Un %*% V[[n]] %*% t(w)))\n      if (rn>1){\n        Wn          = cbind(t(w1),orthogonal.complement.matrix.TW(t(w1)))\n      } else {\n        Wn          = w1\n      }\n      alpha         = rep(NA,rn)\n      u             = rmvnorm(1,rep(0,nu+1),(1/nu)*diag(nu+1))\n      alpha[1]      = sqrt(as.numeric(u%*%t(u)))\n      if (runif(1)<0.5){\n        alpha[1]    = -alpha[1]\n      }\n      if (rn>1){\n        alpha[2:rn] = rmvnorm(1,rep(0,nrow(V[[n]])-1),(1/nu)*diag(rn-1))\n      }\n      bn            = alpha %*% Wn %*% Un\n      B0.aux[n,]    = bn %*% V[[n]]\n    }\n    B0[,,i]         = B0.aux\n  }\n  \n  return(B0)\n}\n\nThe next function normalizes the matrix of the contemporaneous effects, \\(B_0\\):\n\nnormalization.wz2003  = function(B0,B0.hat.inv, Sigma.inv, diag.signs){\n  # This function normalizes a matrix of contemporaneous effects\n  # according to the algorithm by Waggoner & Zha (2003, JOE)\n  # B0        - an NxN matrix, to be normalized\n  # B0.hat    - an NxN matrix, a normalized matrix\n  \n  N                 = nrow(B0)\n  K                 = 2^N\n  distance          = rep(NA,K)\n  for (k in 1:K){\n    B0.tmp.inv      = solve(diag(diag.signs[k,]) %*% B0)\n    distance[k]     = sum(\n      unlist(\n        lapply(1:N,\n               function(n){\n                 t(B0.tmp.inv - B0.hat.inv)[n,] %*%Sigma.inv %*% t(B0.tmp.inv - B0.hat.inv)[n,]\n               }\n        )))\n  }\n  B0.out            = diag(diag.signs[which.min(distance),]) %*% B0\n  \n  return(B0.out)\n}\n\nThis function normalizes the output from the rgn() function, ensuring that we obtain a unique maximum\n\nnormalize.Gibbs.output.parallel          = function(B0.posterior,B0.hat){\n  # This function normalizes the Gibbs sampler output from function rgn\n  # using function normalization.wz2003 \n  # B0.posterior  - a list, output from function rgn\n  # B0.hat        - an NxN matrix, a normalized matrix\n  \n  N                 = nrow(B0.hat)\n  K                 = 2^N\n  \n  B0.hat.inv        = solve(B0.hat)\n  Sigma.inv         = t(B0.hat)%*%B0.hat\n  \n  diag.signs        = matrix(NA,2^N,N)\n  for (n in 1:N){\n    diag.signs[,n]  = kronecker(c(-1,1),rep(1,2^(n-1)))\n  }\n  \n  B0.posterior.n    = mclapply(1:dim(B0.posterior)[3],function(i){\n    normalization.wz2003(B0=B0.posterior[,,i],B0.hat.inv, Sigma.inv, diag.signs)\n  },mc.cores=1\n  )\n  B0.posterior.n  = simplify2array(B0.posterior.n)\n  \n  return(B0.posterior.n)\n}\n\nLastly, a function for simulating the draws of the multivariate normal distribution of the autoregressive slope matrix, \\(B_+\\), is needed\n\nrnorm.ngn       = function(B0.posterior,B,Omega){\n  # This function simulates draws for the multivariate normal distribution\n  # of the autoregressive slope matrix of an SVAR model\n  # from a normal-generalized-normal distribution according to algorithm \n  # by Waggoner & Zha (2003, JEDC)\n  # B0.posterior  - a list, output from function rgn\n  # B             - an NxK matrix, a parameter determining the mean of the multivariate conditionally normal distribution given B0\n  # Omega         - a KxK positive definite matrix, a covariance matrix of the multivariate normal distribution\n  \n  N             = nrow(B)\n  K             = ncol(B)\n  no.draws      = dim(B0.posterior)[3]\n  L             = t(chol(Omega))\n  \n  Bp.posterior  = lapply(1:no.draws,function(i){\n    Bp          = matrix(NA, N, K)\n    for (n in 1:N){\n      Bp[n,]    = as.vector(t(B0.posterior[n,,i] %*% B) + L%*%rnorm(K))\n    }\n    return(Bp)\n  })\n  Bp.posterior  = simplify2array(Bp.posterior)\n  return(Bp.posterior)\n}\n\nHaving set up all the necessary functions, I now simulate a bivariate random walk to produce artificial data.\n\n#Simulation of a bivariate random walk\np.sim = 1\nT.sim = 500\nN.sim = 2\nK.sim = 1 + N.sim*p.sim\n\nY.sim           = arima.sim(list(order = c(0,1,0)), n = T.sim + p.sim-1, mean = 0, sd =1)\nfor (i in 2:N.sim){\n  Y.sim         = rbind(Y.sim, arima.sim(list(order = c(0,1,0)), n = T.sim + p.sim-1, mean = 0, sd = 1))\n}\n\nX.sim           = matrix(1,1,T.sim)\nfor (i in 1:p.sim){\n  X.sim         = rbind(X.sim, Y.sim[,(p.sim+1-i):(ncol(Y.sim)-i)])\n}\nY.sim           = Y.sim[,-p.sim]\n\nFinally, the gibbs sampler for the basic model can be presented. Note that the priors have been set in regards to the specification from above. Further, I create the restriction matrix \\(V_n\\), where one easily can see that a recursive structure is being implied in the system.\n\nGibbs.sampler.base <- function(p,Y,X,S1,S2){\n\n  N       = nrow(Y)\n  p       = p # calculate from X and Y (K and N)\n  K       = 1+N*p\n  S1      = S1\n  S2      = S2\n  kappa0  = 10\n  kappa1  = 10\n  kappa2  = 0.1\n  \n  priors   = list(\n  B        = cbind(rep(0,N), diag(N), matrix(0, N, (p-1)*N)),\n  Omega    = diag(c(kappa2,kappa1*((1:p)^(-2))%x%rep(1,N))),\n  S        = kappa0*diag(N),\n  nu       = N\n  )\n  \n  # create the V matrices\n  FF.V           = vector(\"list\",N)\n  for (n in 1:N){\n  FF.V[[n]]   = cbind(diag(n),matrix(0,n,N-n))\n  }\n\n  # create initial B0 matrix\n  B0.initial = matrix(0,N,N)\n  for (n in 1:N){\n  unrestricted    = apply(FF.V[[n]],2,sum)==1\n  B0.initial[n,unrestricted] = rnorm(sum(unrestricted))\n  }\n\n  B0.posterior    <- array(NA,c(N,N,(S1+S2)))\n  Bp.posterior    <- array(NA,c(N,(1+N*p),(S1+S2)))\n\n  for (s in 1:(S1+S2)){\n\n    # Computing posterior parameters\n    Omega.inv      = solve(priors$Omega)\n    Omega.post.inv = X%*%t(X) + Omega.inv\n    Omega.post     = solve(Omega.post.inv)\n    B.post         = (Y%*%t(X) + priors$B%*%Omega.inv) %*% Omega.post\n    S.post         = Y%*%t(Y) + solve(priors$S) + priors$B%*%Omega.inv%*%t(priors$B) -   B.post%*%Omega.post.inv%*%t(B.post) \n    nu.post        = ncol(Y) + priors$nu\n\n    # Use B0.initial for first iteration, otherwise the latest draw from B0.posterior\n    if (s==1) {\n      B0.s = B0.initial\n    } else {\n      B0.s = B0.posterior[,,s-1]\n    }\n\n    # sampling one draw B0 from the posterior distribution using Gibbs\n    # rgn.function samples from a random conditional generalized normal distribution\n    B0.tmp                  = rgn(n=1, S.inv=S.post, nu=nu.post, V=FF.V, B0.initial=B0.s)\n    B0.posterior[,,s]       = B0.tmp[,,1]\n\n    # sample one draw B+ from the normal conditional posterior\n    Bp.tmp              = rnorm.ngn(B0.tmp, B=B.post,Omega=Omega.post)\n    Bp.posterior[,,s]   = Bp.tmp[,,1]\n  }\n  # END OF GIBBS\n  #Discard first S1 draws\n  B0.posterior <- B0.posterior[,,(S1+1):(S1+S2)]\n  Bp.posterior <- Bp.posterior[,,(S1+1):(S1+S2)]\n\n  #normalisation of B0.posterior and Bp.posterior\n  B0.hat             = diag(sign(diag(B0.tmp[,,1]))) %*% B0.tmp[,,1]\n\n  B0.posterior.N    <- array(NA,c(N,N,S2))\n  Bp.posterior.N    <- array(NA,c(N,(1+N*p),S2))\n\n  B0.posteror.N.tmp      =  normalize.Gibbs.output.parallel(B0.posterior,B0.hat=B0.hat)\n  for (s in 1:S2){\n    B0.posterior.N[,,s]    = B0.posteror.N.tmp[,,s]\n    Bp.posterior.N[,,s]    = B0.posterior.N[,,s]%*%solve(B0.posterior[,,s])%*%Bp.posterior[,,s]\n  }\n\n  return(list(B0.posterior.N = B0.posterior.N,\n              Bp.posterior.N = Bp.posterior.N))\n}\n\n\n\n            [,1]      [,2]\n[1,]  0.96663844 0.0000000\n[2,] -0.04125522 0.9906706\n\n\n            [,1]        [,2]          [,3]\n[1,] -0.07648103  0.94845064 -0.0008053905\n[2,]  0.31399128 -0.03069482  0.9846780309\n\n\nSince a bivariate random walk was simulated, the \\(B_0\\) matrix should be an identity matrix. Further, the first column of \\(B_+\\) should be zero and the matrix \\(B_+[,2:3]\\) should also be an identity matrix. This is also approximately the case, which indicates the estimation procedure is correct."
=======
    "text": "R Code Snippets\nThis section provides the R code behind the estimation procedure. In order to facilitate this, the following R functions are being used by the courtesy of Tomasz Wozníak.\nThe following function computes an orthogonal complement matrix to the input x, which is used in the rgn() function presented below.\n\northogonal.complement.matrix.TW = function(x){\n  # x is a mxn matrix and m>n\n  # the function returns a mx(m-n) matrix, out, that is an orthogonal complement of x, i.e.:\n  # t(x)%*%out = 0 and det(cbind(x,out))!=0\n  if( dim(x)[1] == 1 & dim(x)[2] == 2){\n    x = t(x)\n  }\n  # x <- ifelse(dim(x)[1] == 1 && dim(x)[2] == 2, t(x), x)\n  N     = dim(x)\n  tmp   = qr.Q(qr(x, tol = 1e-10),complete=TRUE)\n  out   = as.matrix(tmp[,(N[2]+1):N[1]])\n  return(out)\n}\n\nThe rgn() function simulates draws for \\(b_n\\) from a \\(\\mathcal{NGN}\\) distribution\n\nrgn             = function(n,S.inv,nu,V,B0.initial){\n  # This function simulates draws for the unrestricted elements \n  # of the conteporaneous relationships matrix of an SVAR model\n  # from a generalized-normal distribution according to algorithm \n  # by Waggoner & Zha (2003, JEDC)\n  # n     - a positive integer, the number of draws to be sampled\n  # S     - an NxN positive definite matrix, a parameter of the generalized-normal distribution\n  # nu    - a positive scalar, degrees of freedom parameter\n  # V     - an N-element list, with fixed matrices\n  # B0.initial - an NxN matrix, of initial values of the parameters\n  \n  N             = nrow(B0.initial)\n  no.draws      = n\n  \n  B0            = array(NA, c(N,N,no.draws))\n  B0.aux        = B0.initial\n  \n  for (i in 1:no.draws){\n    for (n in 1:N){\n      rn            = nrow(V[[n]])\n      Un            = chol(nu*solve(V[[n]]%*%S.inv%*%t(V[[n]])))\n      w             = t(orthogonal.complement.matrix.TW(t(B0.aux[-n,])))\n      w1            = w %*% t(V[[n]]) %*% t(Un) / sqrt(as.numeric(w %*% t(V[[n]]) %*% t(Un) %*% Un %*% V[[n]] %*% t(w)))\n      if (rn>1){\n        Wn          = cbind(t(w1),orthogonal.complement.matrix.TW(t(w1)))\n      } else {\n        Wn          = w1\n      }\n      alpha         = rep(NA,rn)\n      u             = rmvnorm(1,rep(0,nu+1),(1/nu)*diag(nu+1))\n      alpha[1]      = sqrt(as.numeric(u%*%t(u)))\n      if (runif(1)<0.5){\n        alpha[1]    = -alpha[1]\n      }\n      if (rn>1){\n        alpha[2:rn] = rmvnorm(1,rep(0,nrow(V[[n]])-1),(1/nu)*diag(rn-1))\n      }\n      bn            = alpha %*% Wn %*% Un\n      B0.aux[n,]    = bn %*% V[[n]]\n    }\n    B0[,,i]         = B0.aux\n  }\n  \n  return(B0)\n}\n\nThe next function normalizes the matrix of the contemporaneous effects, \\(B_0\\):\n\nnormalization.wz2003  = function(B0,B0.hat.inv, Sigma.inv, diag.signs){\n  # This function normalizes a matrix of contemporaneous effects\n  # according to the algorithm by Waggoner & Zha (2003, JOE)\n  # B0        - an NxN matrix, to be normalized\n  # B0.hat    - an NxN matrix, a normalized matrix\n  \n  N                 = nrow(B0)\n  K                 = 2^N\n  distance          = rep(NA,K)\n  for (k in 1:K){\n    B0.tmp.inv      = solve(diag(diag.signs[k,]) %*% B0)\n    distance[k]     = sum(\n      unlist(\n        lapply(1:N,\n               function(n){\n                 t(B0.tmp.inv - B0.hat.inv)[n,] %*%Sigma.inv %*% t(B0.tmp.inv - B0.hat.inv)[n,]\n               }\n        )))\n  }\n  B0.out            = diag(diag.signs[which.min(distance),]) %*% B0\n  \n  return(B0.out)\n}\n\nThis function normalizes the output from the rgn() function, ensuring that we obtain a unique maximum\n\nnormalize.Gibbs.output.parallel          = function(B0.posterior,B0.hat){\n  # This function normalizes the Gibbs sampler output from function rgn\n  # using function normalization.wz2003 \n  # B0.posterior  - a list, output from function rgn\n  # B0.hat        - an NxN matrix, a normalized matrix\n  \n  N                 = nrow(B0.hat)\n  K                 = 2^N\n  \n  B0.hat.inv        = solve(B0.hat)\n  Sigma.inv         = t(B0.hat)%*%B0.hat\n  \n  diag.signs        = matrix(NA,2^N,N)\n  for (n in 1:N){\n    diag.signs[,n]  = kronecker(c(-1,1),rep(1,2^(n-1)))\n  }\n  \n  B0.posterior.n    = mclapply(1:dim(B0.posterior)[3],function(i){\n    normalization.wz2003(B0=B0.posterior[,,i],B0.hat.inv, Sigma.inv, diag.signs)\n  },mc.cores=1\n  )\n  B0.posterior.n  = simplify2array(B0.posterior.n)\n  \n  return(B0.posterior.n)\n}\n\nLastly, a function for simulating the draws of the multivariate normal distribution of the autoregressive slope matrix, \\(B_+\\), is needed\n\nrnorm.ngn       = function(B0.posterior,B,Omega){\n  # This function simulates draws for the multivariate normal distribution\n  # of the autoregressive slope matrix of an SVAR model\n  # from a normal-generalized-normal distribution according to algorithm \n  # by Waggoner & Zha (2003, JEDC)\n  # B0.posterior  - a list, output from function rgn\n  # B             - an NxK matrix, a parameter determining the mean of the multivariate conditionally normal distribution given B0\n  # Omega         - a KxK positive definite matrix, a covariance matrix of the multivariate normal distribution\n  \n  N             = nrow(B)\n  K             = ncol(B)\n  no.draws      = dim(B0.posterior)[3]\n  L             = t(chol(Omega))\n  \n  Bp.posterior  = lapply(1:no.draws,function(i){\n    Bp          = matrix(NA, N, K)\n    for (n in 1:N){\n      Bp[n,]    = as.vector(t(B0.posterior[n,,i] %*% B) + L%*%rnorm(K))\n    }\n    return(Bp)\n  })\n  Bp.posterior  = simplify2array(Bp.posterior)\n  return(Bp.posterior)\n}\n\nHaving set up all the necessary functions, I now simulate a bivariate random walk to produce artificial data.\n\n#Simulation of data\n\np = 1\nT = 500\nN = 2\nK = 1 + N*p\n\nY           = arima.sim(list(order = c(0,1,0)), n = T + p-1, mean = 0, sd =1)\nfor (i in 2:N){\n  Y         = rbind(Y, arima.sim(list(order = c(0,1,0)), n = T + p-1, mean = 0, sd = 1))\n}\n\nX           = matrix(1,1,T)\nfor (i in 1:p){\n  X         = rbind(X, Y[,(p+1-i):(ncol(Y)-i)])\n}\nY           = Y[,-p]\nartificialdata  = list(p = p, N = N, K = K, Y = Y, X = X)\n\n#This model requires the Y and X matrix to be transposed \n# Y       = t(Y)\n# X       = t(X)\n\nNext, I set the priors in regards to the specification from above. Further, I create the restriction matrix \\(V_n\\). Note, that I imply a recursive structure in the system.\n\n# set the priors\nkappa0     = 10\nkappa1     = .1  \nkappa2     = 10\n\npriors     = list(\n  B        = cbind(rep(0,N), diag(N), matrix(0, N, (p-1)*N)),\n  Omega    = diag(c(kappa2,kappa1*((1:p)^(-2))%x%rep(1,N))),\n  S        = kappa0*diag(N),\n  nu       = N\n)\n\n# create the V matrices\nFF.V           = vector(\"list\",N)\nfor (n in 1:N){\n  FF.V[[n]]   = cbind(diag(n),matrix(0,n,N-n))\n}\n\n# create initial B0 matrix\nB0.initial = matrix(0,N,N)\nfor (n in 1:N){\n  unrestricted    = apply(FF.V[[n]],2,sum)==1\n  B0.initial[n,unrestricted] = rnorm(sum(unrestricted))\n}\n\nFinally, the gibbs sampler for the basic model can be presented\n\nGibbs.sampler.base <- function(p,Y,X,priors,S1,S2, FF.V, B0.initial){\n\n  N       = nrow(Y)\n  p       = 1 # calculate from X and Y (K and N)\n  K       = 1+N*p\n  S1      = S1\n  S2      = S2\n  kappa0 = 10\n  kappa1 = 10\n  kappa2 = 0.1\n\n  B0.posterior    <- array(NA,c(N,N,(S1+S2)))\n  Bp.posterior    <- array(NA,c(N,(1+N*p),(S1+S2)))\n\n  for (s in 1:(S1+S2)){\n\n    # Computing posterior parameters\n    Omega.inv      = solve(priors$Omega)\n    Omega.post.inv = X%*%t(X) + Omega.inv\n    Omega.post     = solve(Omega.post.inv)\n    B.post         = (Y%*%t(X) + priors$B%*%Omega.inv) %*% Omega.post\n    S.post         = Y%*%t(Y) + solve(priors$S) + priors$B%*%Omega.inv%*%t(priors$B) -   B.post%*%Omega.post.inv%*%t(B.post) \n    nu.post        = ncol(Y) + priors$nu\n\n    # Use B0.initial for first iteration, otherwise the latest draw from B0.posterior\n    if (s==1) {\n      B0.s = B0.initial\n    } else {\n      B0.s = B0.posterior[,,s-1]\n    }\n\n    # sampling one draw B0 from the posterior distribution using Gibbs\n    # rgn.function samples from a random conditional generalized normal distribution\n    B0.tmp                  = rgn(n=1, S.inv=S.post, nu=nu.post, V=FF.V, B0.initial=B0.s)\n    B0.posterior[,,s]       = B0.tmp[,,1]\n\n    # sample one draw B+ from the normal conditional posterior\n    Bp.tmp              = rnorm.ngn(B0.tmp, B=B.post,Omega=Omega.post)\n    Bp.posterior[,,s]   = Bp.tmp[,,1]\n  }\n  # END OF GIBBS\n  #Discard first S1 draws\n  B0.posterior <- B0.posterior[,,(S1+1):(S1+S2)]\n  Bp.posterior <- Bp.posterior[,,(S1+1):(S1+S2)]\n\n  #normalisation of B0.posterior and Bp.posterior\n  B0.hat             = diag(sign(diag(B0.tmp[,,1]))) %*% B0.tmp[,,1]\n\n  B0.posterior.N    <- array(NA,c(N,N,S2))\n  Bp.posterior.N    <- array(NA,c(N,(1+N*p),S2))\n\n  B0.posteror.N.tmp      =  normalize.Gibbs.output.parallel(B0.posterior,B0.hat=B0.hat)\n  for (s in 1:S2){\n    B0.posterior.N[,,s]    = B0.posteror.N.tmp[,,s]\n    Bp.posterior.N[,,s]    = B0.posterior.N[,,s]%*%solve(B0.posterior[,,s])%*%Bp.posterior[,,s]\n  }\n\n  return(list(B0.posterior.N = B0.posterior.N,\n              Bp.posterior.N = Bp.posterior.N))\n}\n\n\n\n            [,1]      [,2]\n[1,]  1.02005371 0.0000000\n[2,] -0.01974985 0.9780032\n\n\n           [,1]        [,2]       [,3]\n[1,]  0.1242882  0.99930421 0.01297839\n[2,] -0.2482267 -0.02849457 0.96587384\n\n\nSince a bivariate random walk was simulated, the \\(B_0\\) matrix should be an identity matrix. Further, the first column of \\(B_+\\) should be zero and the matrix \\(B_+[,2:3]\\) should also be an identity matrix. This is also approximately the case, which indicates the estimation procedure is correct."
>>>>>>> 41d47063279825e9a18740578892ff6012578e11
>>>>>>> Stashed changes
  },
  {
    "objectID": "index.html#extended-model",
    "href": "index.html#extended-model",
    "title": "The Effect of Money Supply Shocks on Asset Prices and Inflation in the US Economy: A Bayesian SVAR Approach",
    "section": "Extended Model",
<<<<<<< Updated upstream
    "text": "Extended Model\nAs part of my extended model, I estimate the shrinkage parameters \\(\\kappa_0\\) and \\(\\kappa_+\\). Estimating those parameters instead of just setting them might lead to improved efficiency and reliability. By remembering how \\(\\kappa_0\\) and \\(\\kappa_+\\) affected the posterior parameters in the basic model, we can now write up the kernel for the new conjugate-prior up for the extended model:\n\\[\\begin{align}\np(B_+,B_0|Y,X) &\\propto L(B_+,B_0|Y,X)p(B_+,B_0|\\kappa_0,\\kappa_+)p(\\kappa_0)p(\\kappa_+)\\\\\n\\end{align}\\]\n\\[\\begin{align}\np(\\kappa_0|\\underline{s}_{\\kappa_0},\\underline{\\nu}_{\\kappa_0}) &\\sim \\mathcal{IG}2(\\underline{s}_{\\kappa_0},\\underline{\\nu}_{\\kappa_0})\\\\\np(\\kappa_+|\\underline{s}_{\\kappa_+},\\underline{\\nu}_{\\kappa_+}) &\\sim \\mathcal{IG}2(\\underline{s}_{\\kappa_+},\\underline{\\nu}_{\\kappa_+})\n\\end{align}\\]\nThe full-conditional posterior distribution of \\(\\kappa_0\\) can be found to be:\n\\[\\begin{align}\np(\\kappa_0|Y,X,B_0,B_+,\\kappa_+) &\\propto p(B_0|\\kappa_0)p(\\kappa_0)\\\\\n&\\propto \\prod_{n=1}^N\\kappa_0^{\\frac{r_n}{2}}\\exp \\left\\{  -\\frac{1}{2}\\sum_{n=1}^N b_nV_n(\\kappa_0 I_{r_n})^{-1}V_n'b_n'\\right\\}\\kappa_0^{-\\frac{\\underline{\\nu}_{\\kappa_0}+2}{2}}\\exp \\left\\{  -\\frac{1}{2} \\frac{\\underline{s}_{\\kappa_0}}{\\kappa_0}\\right\\}\\\\\n&\\propto \\prod_{n=1}^N\\kappa_0^{\\frac{r_n}{2}} \\exp \\left\\{  -\\frac{1}{2}\\frac{1}{\\kappa_0}\\sum_{n=1}^N b_nV_n I_{r_n}V_n'b_n'\\right\\}\\kappa_0^{-\\frac{\\underline{\\nu}_{\\kappa_0}+2}{2}}\\exp \\left\\{  -\\frac{1}{2} \\frac{\\underline{s}_{\\kappa_0}}{\\kappa_0}\\right\\}\n\\end{align}\\]\nSince \\(\\underline{S}=\\kappa_0I_N\\) and \\(b_n|\\kappa_0 \\sim \\mathcal{N}(0,\\kappa_0(V_nV_n')^{-1})=\\mathcal{N}_{r_n}(0_{r_n},\\kappa_0I_{r_n})\\). By collecting the components in an appropriate way, the full-conditional posterior can be written as:\n\\[\\begin{align}\np(\\kappa_0|Y,X,B_0,B_+,\\kappa_+) &\\propto \\kappa_0^{-\\frac{\\bar{\\nu}_{\\kappa_0}+2}{2}} \\exp \\left\\{ -\\frac{1}{2}\\frac{\\bar{s}_{\\kappa_0}}{\\kappa_0} \\right\\}\\\\\n\\bar{s}_{\\kappa_0} &= \\underline{s}_{\\kappa_0}+\\sum_{n=1}^N b_nV_nI_{r_n}V_n'b_n'\\\\\n\\bar{\\nu}_{\\kappa_0} &= \\underline{\\nu}_{\\kappa_0}+\\sum_{n=1}^N r_n\n\\end{align}\\]\nThe same procedure goes for the full-conditional posterior distribution of \\(\\kappa_+\\):\n\\[\\begin{align}\np(\\kappa_+|Y,X,B_0,B_+,\\kappa_0) &\\propto p(B_+|B_0,\\kappa_+)p(\\kappa_+)\\\\\n&\\propto \\kappa_+^{\\frac{K}{2}}\\exp \\left\\{-\\frac{1}{2}\\frac{1}{\\kappa_+} \\sum_{n=1}^N (B_n-b_nV_n\\underline{B})\\underline{\\Omega}^{-1}(B_n-b_nV_n\\underline{B})'\\right\\}\\kappa_+^{-\\frac{\\underline{\\nu}_{\\kappa_+}+2}{2}}\\exp \\left\\{  -\\frac{1}{2} \\frac{\\underline{s}_{\\kappa_+}}{\\kappa_+}\\right\\}\n\\end{align}\\]\nSince \\(B_n|b_n,\\kappa_+ \\sim \\mathcal{N}_{N+1}(b_nV_n\\underline{B},\\kappa_+\\Omega)\\)\nWhich further can be derived to:\n\\[\\begin{align}\np(\\kappa_+|Y,X,B_0,B_+,\\kappa_0) &\\propto \\kappa_+^{-\\frac{\\bar{\\nu}_{\\kappa_+}+2}{2}} \\exp \\left\\{ -\\frac{1}{2}\\frac{\\bar{s}_{\\kappa_+}}{\\kappa_+} \\right\\}\\\\\n\\bar{s}_{\\kappa_+} &= \\underline{s}_{\\kappa_+}+\\sum_{n=1}^N (B_n-b_nV_n\\underline{B})\\underline{\\Omega}^{-1}(B_n-b_nV_n\\underline{B})'\\\\\n\\bar{\\nu}_{\\kappa_+} &= \\underline{\\nu}_{\\kappa_+}+NK\n\\end{align}\\]\nBefore I code this up in R the new priors need to be set. Note, that is just set to a constant now.\n\n### Setting new priors\n\npriors   = list(\n  B        = cbind(rep(0,N), diag(N), matrix(0, N, (p-1)*N)),\n  Omega    = diag(c(10,((1:p)^(-2))%x%rep(1,N))),\n  S        = diag(N),\n  nu       = N,\n  S.kappa0  = 1,\n  nu.kappa0 = 1,\n  S.kappa1  = 1,\n  nu.kappa1 = 1\n)\n\nWhich facilitates writing up the gibbs sampler for the extended model:\n\nGibbs.sampler.extended <- function(p,Y,X,priors,S1,S2, FF.V, B0.initial){\n  \n  N       = nrow(Y)\n  p       = 1 # calculate from X and Y (K and N)\n  K       = 1+N*p\n  S1      = S1\n  S2      = S2\n  \n  kappa0          <- rep(NA, S1 + S2)\n  kappa1          <- rep(NA, S1 + S2)\n  B0.posterior    <- array(NA,c(N,N,(S1+S2)))\n  Bp.posterior    <- array(NA,c(N,(1+N*p),(S1+S2)))  \n  \n  kappa0[1] <- 1\n  kappa1[1] <- 1 \n  \n  for (s in 1:(S1+S2)){\n    \n    # Computing posterior parameters\n    # Only Omega, B and S depend on kappa1\n    #cat(\"\\n kappa0: \", kappa0[s], \"kappa1: \", kappa1[s])\n    \n    Omega.inv      = solve(priors$Omega)\n    Omega.post.inv = X%*%t(X) + (1/kappa1[s])*Omega.inv\n    Omega.post     = solve(Omega.post.inv)\n    B.post         = (Y%*%t(X) + priors$B%*%((1/kappa1[s])*Omega.inv)) %*% Omega.post\n    S.post         = Y%*%t(Y) + (1/kappa0[s])*solve(priors$S) + priors$B%*%((1/kappa1[s])*Omega.inv)%*%t(priors$B) - B.post%*%Omega.post.inv%*%t(B.post) \n    nu.post        = ncol(Y) + priors$nu\n    \n    # Use B0.initial for first iteration, otherwise the latest draw from B0.posterior\n    \n    if (s==1) {\n      B0.s = B0.initial\n    } else {\n      B0.s = B0.posterior[,,s-1]\n    }\n    \n    # sampling one draw B0 from the posterior distribution using Gibbs  \n    # rgn.function samples from a random conditional generalized normal distribution\n    \n    B0.tmp                  = rgn(n=1, S.inv=S.post, nu=nu.post, V=FF.V, B0.initial=B0.s)\n    B0.posterior[,,s]       = B0.tmp[,,1]\n    \n    #cat(\"B0: \", B0.posterior[,,s],\"\\n\")\n    \n    # sample one draw B+ from the normal conditional posterior\n    Bp.tmp              = rnorm.ngn(B0.tmp, B=B.post,Omega=Omega.post)\n    Bp.posterior[,,s]   = Bp.tmp[,,1]\n    \n    #compute posterior for the shrinkage parameter S.kappa and nu\n    S.kappa0.post = priors$S.kappa0 + sum(B0.posterior[,,s]^2)\n    \n    # S.kappa.post = sum(priors$S.kappa + (B0.posterior[i,,s]-priors$B[i,])%*%Omega.inv%*%t(B0.posterior[i,,s]-priors$B[i,]))\n    \n    # nu.kappa0.post  = priors$nu.kappa0 + i #change outside of loop count number rows (otherwise make as a sum of i's)\n    nu.kappa0.post  = priors$nu.kappa0 + sum(unlist(lapply(FF.V, nrow)))\n    \n    S.kappa1.post   = priors$S.kappa1\n    for (i in 1:N){\n      S.kappa1.post = S.kappa1.post + (Bp.posterior[i,,s]- B0.posterior[i,,s]%*%priors$B)%*%Omega.inv%*%t(Bp.posterior[i,,s]-B0.posterior[i,,s]%*%priors$B)\n    }\n    # S.kappa.post = sum(priors$S.kappa + (B0.posterior[i,,s]-priors$B[i,])%*%Omega.inv%*%t(B0.posterior[i,,s]-priors$B[i,]))\n    \n    nu.kappa1.post  = priors$nu.kappa1 + N*(p*N+1) \n    \n    \n    #Draw kappa0 and kappa1 from IG2\n    if (s != S1+S2) {\n      kappa0[s+1]    = S.kappa0.post / rchisq(1, df=nu.kappa0.post) \n      kappa1[s+1]    = S.kappa1.post / rchisq(1, df=nu.kappa1.post) \n    }\n  }\n  \n  #Discard first S1 draws\n  \n  B0.posterior <- B0.posterior[,,(S1+1):(S1+S2)]\n  Bp.posterior <- Bp.posterior[,,(S1+1):(S1+S2)]\n  kappa0       <- kappa0[(S1+1):(S1+S2)]\n  kappa1       <- kappa1[(S1+1):(S1+S2)]\n  \n  #normalisation of B0.posterior and Bp.posterior\n  \n  B0.hat             = diag(sign(diag(B0.tmp[,,1]))) %*% B0.tmp[,,1]\n  # t(chol((nu.post-N)*S.post))# normalisation using this B0.hat should work\n  \n  B0.posterior.N    <- array(NA,c(N,N,S2))\n  Bp.posterior.N    <- array(NA,c(N,(1+N*p),S2))\n  \n    B0.posteror.N.tmp      =  normalize.Gibbs.output.parallel(B0.posterior,B0.hat=B0.hat)\n  for (s in 1:S2){\n    B0.posterior.N[,,s]    = B0.posteror.N.tmp[,,1]\n    Bp.posterior.N[,,s]    = B0.posterior.N[,,s]%*%solve(B0.posterior[,,s])%*%Bp.posterior[,,s]\n  }\n  \n  return(list(B0.posterior.N = B0.posterior.N,\n              Bp.posterior.N = Bp.posterior.N,\n              kappa0 = kappa0,\n              kappa1 = kappa1))\n} \n\n\n\n           [,1]     [,2]\n[1,] 1.02123929 0.000000\n[2,] 0.02458385 1.026761\n\n\n           [,1]       [,2]       [,3]\n[1,]  0.1228352 1.00056214 0.01287995\n[2,] -0.2534007 0.01442736 1.01473022\n\n\nAgain, the estimation procedure for the extended model seems to be correct as well as the output aligns with the artificial data being a bivariate random walk.\nI now turn to plotting the diagonal elements of \\(B_+[,2:3]\\) in order to show whether the algorithm converges.\n\n\n\n\n\nThis is indeed the case. The plots look like white noise processes as it fluctuates around the true value 1. This means the algorithm has converged."
=======
<<<<<<< HEAD
    "text": "Extended Model\nAs part of my extended model, I estimate the shrinkage parameters \\(\\kappa_0\\) and \\(\\kappa_+\\). Estimating those parameters instead of just setting them might lead to improved efficiency and reliability. By remembering how \\(\\kappa_0\\) and \\(\\kappa_+\\) affected the posterior parameters in the basic model, we can now write up the kernel for the new conjugate-prior up for the extended model:\n\\[\\begin{align}\np(B_+,B_0|Y,X) &\\propto L(B_+,B_0|Y,X)p(B_+,B_0|\\kappa_0,\\kappa_+)p(\\kappa_0)p(\\kappa_+)\\\\\n\\end{align}\\]\n\\[\\begin{align}\np(\\kappa_0|\\underline{s}_{\\kappa_0},\\underline{\\nu}_{\\kappa_0}) &\\sim \\mathcal{IG}2(\\underline{s}_{\\kappa_0},\\underline{\\nu}_{\\kappa_0})\\\\\np(\\kappa_+|\\underline{s}_{\\kappa_+},\\underline{\\nu}_{\\kappa_+}) &\\sim \\mathcal{IG}2(\\underline{s}_{\\kappa_+},\\underline{\\nu}_{\\kappa_+})\n\\end{align}\\]\nThe full-conditional posterior distribution of \\(\\kappa_0\\) can be found to be:\n\\[\\begin{align}\np(\\kappa_0|Y,X,B_0,B_+,\\kappa_+) &\\propto p(B_0|\\kappa_0)p(\\kappa_0)\\\\\n&\\propto \\prod_{n=1}^N\\kappa_0^{\\frac{r_n}{2}}\\exp \\left\\{  -\\frac{1}{2}\\sum_{n=1}^N b_nV_n(\\kappa_0 I_{r_n})^{-1}V_n'b_n'\\right\\}\\kappa_0^{-\\frac{\\underline{\\nu}_{\\kappa_0}+2}{2}}\\exp \\left\\{  -\\frac{1}{2} \\frac{\\underline{s}_{\\kappa_0}}{\\kappa_0}\\right\\}\\\\\n&\\propto \\prod_{n=1}^N\\kappa_0^{\\frac{r_n}{2}} \\exp \\left\\{  -\\frac{1}{2}\\frac{1}{\\kappa_0}\\sum_{n=1}^N b_nV_n I_{r_n}V_n'b_n'\\right\\}\\kappa_0^{-\\frac{\\underline{\\nu}_{\\kappa_0}+2}{2}}\\exp \\left\\{  -\\frac{1}{2} \\frac{\\underline{s}_{\\kappa_0}}{\\kappa_0}\\right\\}\n\\end{align}\\]\nSince \\(\\underline{S}=\\kappa_0I_N\\) and \\(b_n|\\kappa_0 \\sim \\mathcal{N}(0,\\kappa_0(V_nV_n')^{-1})=\\mathcal{N}_{r_n}(0_{r_n},\\kappa_0I_{r_n})\\). By collecting the components in an appropriate way, the full-conditional posterior can be written as:\n\\[\\begin{align}\np(\\kappa_0|Y,X,B_0,B_+,\\kappa_+) &\\propto \\kappa_0^{-\\frac{\\bar{\\nu}_{\\kappa_0}+2}{2}} \\exp \\left\\{ -\\frac{1}{2}\\frac{\\bar{s}_{\\kappa_0}}{\\kappa_0} \\right\\}\\\\\n\\bar{s}_{\\kappa_0} &= \\underline{s}_{\\kappa_0}+\\sum_{n=1}^N b_nV_nI_{r_n}V_n'b_n'\\\\\n\\bar{\\nu}_{\\kappa_0} &= \\underline{\\nu}_{\\kappa_0}+\\sum_{n=1}^N r_n\n\\end{align}\\]\nThe same procedure goes for the full-conditional posterior distribution of \\(\\kappa_+\\):\n\\[\\begin{align}\np(\\kappa_+|Y,X,B_0,B_+,\\kappa_0) &\\propto p(B_+|B_0,\\kappa_+)p(\\kappa_+)\\\\\n&\\propto \\kappa_+^{\\frac{K}{2}}\\exp \\left\\{-\\frac{1}{2}\\frac{1}{\\kappa_+} \\sum_{n=1}^N (B_n-b_nV_n\\underline{B})\\underline{\\Omega}^{-1}(B_n-b_nV_n\\underline{B})'\\right\\}\\kappa_+^{-\\frac{\\underline{\\nu}_{\\kappa_+}+2}{2}}\\exp \\left\\{  -\\frac{1}{2} \\frac{\\underline{s}_{\\kappa_+}}{\\kappa_+}\\right\\}\n\\end{align}\\]\nSince \\(B_n|b_n,\\kappa_+ \\sim \\mathcal{N}_{N+1}(b_nV_n\\underline{B},\\kappa_+\\Omega)\\)\nWhich further can be derived to:\n\\[\\begin{align}\np(\\kappa_+|Y,X,B_0,B_+,\\kappa_0) &\\propto \\kappa_+^{-\\frac{\\bar{\\nu}_{\\kappa_+}+2}{2}} \\exp \\left\\{ -\\frac{1}{2}\\frac{\\bar{s}_{\\kappa_+}}{\\kappa_+} \\right\\}\\\\\n\\bar{s}_{\\kappa_+} &= \\underline{s}_{\\kappa_+}+\\sum_{n=1}^N (B_n-b_nV_n\\underline{B})\\underline{\\Omega}^{-1}(B_n-b_nV_n\\underline{B})'\\\\\n\\bar{\\nu}_{\\kappa_+} &= \\underline{\\nu}_{\\kappa_+}+NK\n\\end{align}\\]\nThis facilitates writing up the gibbs sampler for the extended model. Note that new priors have to be set compared to the basic model, as the hyperparameters are now being estimated:\n\nGibbs.sampler.extended <- function(p,Y,X,S1,S2){\n  \n  N       = nrow(Y)\n  p       = p # calculate from X and Y (K and N)\n  K       = 1+N*p\n  S1      = S1\n  S2      = S2\n  \n  #set new priors\n  \n  priors = list(\n  B         = cbind(rep(0,N), diag(N), matrix(0, N, (p-1)*N)),\n  Omega     = diag(c(10,((1:p)^(-2))%x%rep(1,N))),\n  S         = diag(N),\n  nu        = N,\n  S.kappa0  = 1,\n  nu.kappa0 = 1,\n  S.kappa1  = 1,\n  nu.kappa1 = 1\n  )\n  \n  # create the V matrices\n  FF.V           = vector(\"list\",N)\n  for (n in 1:N){\n  FF.V[[n]]   = cbind(diag(n),matrix(0,n,N-n))\n  }\n\n  # create initial B0 matrix\n  B0.initial = matrix(0,N,N)\n  for (n in 1:N){\n  unrestricted    = apply(FF.V[[n]],2,sum)==1\n  B0.initial[n,unrestricted] = rnorm(sum(unrestricted))\n  }\n  \n  kappa0          <- rep(NA, S1 + S2)\n  kappa1          <- rep(NA, S1 + S2)\n  B0.posterior    <- array(NA,c(N,N,(S1+S2)))\n  Bp.posterior    <- array(NA,c(N,(1+N*p),(S1+S2)))  \n  \n  kappa0[1] <- 1\n  kappa1[1] <- 1 \n  \n  for (s in 1:(S1+S2)){\n    \n    # Computing posterior parameters\n    # Only Omega, B and S depend on kappa1\n    #cat(\"\\n kappa0: \", kappa0[s], \"kappa1: \", kappa1[s])\n    \n    Omega.inv      = solve(priors$Omega)\n    Omega.post.inv = X%*%t(X) + (1/kappa1[s])*Omega.inv\n    Omega.post     = solve(Omega.post.inv)\n    B.post         = (Y%*%t(X) + priors$B%*%((1/kappa1[s])*Omega.inv)) %*% Omega.post\n    S.post         = Y%*%t(Y) + (1/kappa0[s])*solve(priors$S) + priors$B%*%((1/kappa1[s])*Omega.inv)%*%t(priors$B) - B.post%*%Omega.post.inv%*%t(B.post) \n    nu.post        = ncol(Y) + priors$nu\n    \n    # Use B0.initial for first iteration, otherwise the latest draw from B0.posterior\n    \n    if (s==1) {\n      B0.s = B0.initial\n    } else {\n      B0.s = B0.posterior[,,s-1]\n    }\n    \n    # sampling one draw B0 from the posterior distribution using Gibbs  \n    # rgn.function samples from a random conditional generalized normal distribution\n    \n    B0.tmp                  = rgn(n=1, S.inv=S.post, nu=nu.post, V=FF.V, B0.initial=B0.s)\n    B0.posterior[,,s]       = B0.tmp[,,1]\n    \n    #cat(\"B0: \", B0.posterior[,,s],\"\\n\")\n    \n    # sample one draw B+ from the normal conditional posterior\n    Bp.tmp              = rnorm.ngn(B0.tmp, B=B.post,Omega=Omega.post)\n    Bp.posterior[,,s]   = Bp.tmp[,,1]\n    \n    #compute posterior for the shrinkage parameter S.kappa and nu\n    S.kappa0.post = priors$S.kappa0 + sum(B0.posterior[,,s]^2)\n    \n    # S.kappa.post = sum(priors$S.kappa + (B0.posterior[i,,s]-priors$B[i,])%*%Omega.inv%*%t(B0.posterior[i,,s]-priors$B[i,]))\n    \n    # nu.kappa0.post  = priors$nu.kappa0 + i #change outside of loop count number rows (otherwise make as a sum of i's)\n    nu.kappa0.post  = priors$nu.kappa0 + sum(unlist(lapply(FF.V, nrow)))\n    \n    S.kappa1.post   = priors$S.kappa1\n    for (i in 1:N){\n      S.kappa1.post = S.kappa1.post + (Bp.posterior[i,,s]- B0.posterior[i,,s]%*%priors$B)%*%Omega.inv%*%t(Bp.posterior[i,,s]-B0.posterior[i,,s]%*%priors$B)\n    }\n    # S.kappa.post = sum(priors$S.kappa + (B0.posterior[i,,s]-priors$B[i,])%*%Omega.inv%*%t(B0.posterior[i,,s]-priors$B[i,]))\n    \n    nu.kappa1.post  = priors$nu.kappa1 + N*(p*N+1) \n    \n    \n    #Draw kappa0 and kappa1 from IG2\n    if (s != S1+S2) {\n      kappa0[s+1]    = S.kappa0.post / rchisq(1, df=nu.kappa0.post) \n      kappa1[s+1]    = S.kappa1.post / rchisq(1, df=nu.kappa1.post) \n    }\n  }\n  \n  #Discard first S1 draws\n  \n  B0.posterior <- B0.posterior[,,(S1+1):(S1+S2)]\n  Bp.posterior <- Bp.posterior[,,(S1+1):(S1+S2)]\n  kappa0       <- kappa0[(S1+1):(S1+S2)]\n  kappa1       <- kappa1[(S1+1):(S1+S2)]\n  \n  #normalisation of B0.posterior and Bp.posterior\n  \n  B0.hat             = diag(sign(diag(B0.tmp[,,1]))) %*% B0.tmp[,,1]\n  # t(chol((nu.post-N)*S.post))# normalisation using this B0.hat should work\n  \n  B0.posterior.N    <- array(NA,c(N,N,S2))\n  Bp.posterior.N    <- array(NA,c(N,(1+N*p),S2))\n  \n    B0.posteror.N.tmp      =  normalize.Gibbs.output.parallel(B0.posterior,B0.hat=B0.hat)\n  for (s in 1:S2){\n    B0.posterior.N[,,s]    = B0.posteror.N.tmp[,,1]\n    Bp.posterior.N[,,s]    = B0.posterior.N[,,s]%*%solve(B0.posterior[,,s])%*%Bp.posterior[,,s]\n  }\n  \n  return(list(B0.posterior.N = B0.posterior.N,\n              Bp.posterior.N = Bp.posterior.N,\n              kappa0 = kappa0,\n              kappa1 = kappa1))\n} \n\n\n\n            [,1]     [,2]\n[1,]  0.95786851 0.000000\n[2,] -0.07916275 1.038623\n\n\n            [,1]        [,2]          [,3]\n[1,] -0.08595068  0.94022812 -0.0005483824\n[2,]  0.37004105 -0.06610283  1.0317077687\n\n\nAgain, the estimation procedure for the extended model seems to be correct as well as the output aligns with the artificial data being a bivariate random walk.\nI now turn to plotting the diagonal elements of \\(B_+[,2:3]\\) in order to show whether the algorithm converges.\n\n\n\n\n\nFigure 4: Convergence Plots\n\n\n\n\nThis is indeed the case. The plots look like white noise processes as it fluctuates around the true value 1. This means the algorithm has converged."
=======
    "text": "Extended Model\nAs part of my extended model, I estimate the shrinkage parameters \\(\\kappa_0\\) and \\(\\kappa_+\\). Estimating those parameters instead of just setting them might lead to improved efficiency and reliability. By remembering how \\(\\kappa_0\\) and \\(\\kappa_+\\) affected the posterior parameters in the basic model, we can now write up the kernel for the new conjugate-prior up for the extended model:\n\\[\\begin{align}\np(B_+,B_0|Y,X) &\\propto L(B_+,B_0|Y,X)p(B_+,B_0|\\kappa_0,\\kappa_+)p(\\kappa_0)p(\\kappa_+)\\\\\n\\end{align}\\]\n\\[\\begin{align}\np(\\kappa_0|\\underline{s}_{\\kappa_0},\\underline{\\nu}_{\\kappa_0}) &\\sim \\mathcal{IG}2(\\underline{s}_{\\kappa_0},\\underline{\\nu}_{\\kappa_0})\\\\\np(\\kappa_+|\\underline{s}_{\\kappa_+},\\underline{\\nu}_{\\kappa_+}) &\\sim \\mathcal{IG}2(\\underline{s}_{\\kappa_+},\\underline{\\nu}_{\\kappa_+})\n\\end{align}\\]\nThe full-conditional posterior distribution of \\(\\kappa_0\\) can be found to be:\n\\[\\begin{align}\np(\\kappa_0|Y,X,B_0,B_+,\\kappa_+) &\\propto p(B_0|\\kappa_0)p(\\kappa_0)\\\\\n&\\propto \\prod_{n=1}^N\\kappa_0^{\\frac{r_n}{2}}\\exp \\left\\{  -\\frac{1}{2}\\sum_{n=1}^N b_nV_n(\\kappa_0 I_{r_n})^{-1}V_n'b_n'\\right\\}\\kappa_0^{-\\frac{\\underline{\\nu}_{\\kappa_0}+2}{2}}\\exp \\left\\{  -\\frac{1}{2} \\frac{\\underline{s}_{\\kappa_0}}{\\kappa_0}\\right\\}\\\\\n&\\propto \\prod_{n=1}^N\\kappa_0^{\\frac{r_n}{2}} \\exp \\left\\{  -\\frac{1}{2}\\frac{1}{\\kappa_0}\\sum_{n=1}^N b_nV_n I_{r_n}V_n'b_n'\\right\\}\\kappa_0^{-\\frac{\\underline{\\nu}_{\\kappa_0}+2}{2}}\\exp \\left\\{  -\\frac{1}{2} \\frac{\\underline{s}_{\\kappa_0}}{\\kappa_0}\\right\\}\n\\end{align}\\]\nSince \\(\\underline{S}=\\kappa_0I_N\\) and \\(b_n|\\kappa_0 \\sim \\mathcal{N}(0,\\kappa_0(V_nV_n')^{-1})=\\mathcal{N}_{r_n}(0_{r_n},\\kappa_0I_{r_n})\\). By collecting the components in an appropriate way, the full-conditional posterior can be written as:\n\\[\\begin{align}\np(\\kappa_0|Y,X,B_0,B_+,\\kappa_+) &\\propto \\kappa_0^{-\\frac{\\bar{\\nu}_{\\kappa_0}+2}{2}} \\exp \\left\\{ -\\frac{1}{2}\\frac{\\bar{s}_{\\kappa_0}}{\\kappa_0} \\right\\}\\\\\n\\bar{s}_{\\kappa_0} &= \\underline{s}_{\\kappa_0}+\\sum_{n=1}^N b_nV_nI_{r_n}V_n'b_n'\\\\\n\\bar{\\nu}_{\\kappa_0} &= \\underline{\\nu}_{\\kappa_0}+\\sum_{n=1}^N r_n\n\\end{align}\\]\nThe same procedure goes for the full-conditional posterior distribution of \\(\\kappa_+\\):\n\\[\\begin{align}\np(\\kappa_+|Y,X,B_0,B_+,\\kappa_0) &\\propto p(B_+|B_0,\\kappa_+)p(\\kappa_+)\\\\\n&\\propto \\kappa_+^{\\frac{K}{2}}\\exp \\left\\{-\\frac{1}{2}\\frac{1}{\\kappa_+} \\sum_{n=1}^N (B_n-b_nV_n\\underline{B})\\underline{\\Omega}^{-1}(B_n-b_nV_n\\underline{B})'\\right\\}\\kappa_+^{-\\frac{\\underline{\\nu}_{\\kappa_+}+2}{2}}\\exp \\left\\{  -\\frac{1}{2} \\frac{\\underline{s}_{\\kappa_+}}{\\kappa_+}\\right\\}\n\\end{align}\\]\nSince \\(B_n|b_n,\\kappa_+ \\sim \\mathcal{N}_{N+1}(b_nV_n\\underline{B},\\kappa_+\\Omega)\\)\nWhich further can be derived to:\n\\[\\begin{align}\np(\\kappa_+|Y,X,B_0,B_+,\\kappa_0) &\\propto \\kappa_+^{-\\frac{\\bar{\\nu}_{\\kappa_+}+2}{2}} \\exp \\left\\{ -\\frac{1}{2}\\frac{\\bar{s}_{\\kappa_+}}{\\kappa_+} \\right\\}\\\\\n\\bar{s}_{\\kappa_+} &= \\underline{s}_{\\kappa_+}+\\sum_{n=1}^N (B_n-b_nV_n\\underline{B})\\underline{\\Omega}^{-1}(B_n-b_nV_n\\underline{B})'\\\\\n\\bar{\\nu}_{\\kappa_+} &= \\underline{\\nu}_{\\kappa_+}+NK\n\\end{align}\\]\nBefore I code this up in R the new priors need to be set. Note, that is just set to a constant now.\n\n### Setting new priors\n\npriors   = list(\n  B        = cbind(rep(0,N), diag(N), matrix(0, N, (p-1)*N)),\n  Omega    = diag(c(10,((1:p)^(-2))%x%rep(1,N))),\n  S        = diag(N),\n  nu       = N,\n  S.kappa0  = 1,\n  nu.kappa0 = 1,\n  S.kappa1  = 1,\n  nu.kappa1 = 1\n)\n\nWhich facilitates writing up the gibbs sampler for the extended model:\n\nGibbs.sampler.extended <- function(p,Y,X,priors,S1,S2, FF.V, B0.initial){\n  \n  N       = nrow(Y)\n  p       = 1 # calculate from X and Y (K and N)\n  K       = 1+N*p\n  S1      = S1\n  S2      = S2\n  \n  kappa0          <- rep(NA, S1 + S2)\n  kappa1          <- rep(NA, S1 + S2)\n  B0.posterior    <- array(NA,c(N,N,(S1+S2)))\n  Bp.posterior    <- array(NA,c(N,(1+N*p),(S1+S2)))  \n  \n  kappa0[1] <- 1\n  kappa1[1] <- 1 \n  \n  for (s in 1:(S1+S2)){\n    \n    # Computing posterior parameters\n    # Only Omega, B and S depend on kappa1\n    #cat(\"\\n kappa0: \", kappa0[s], \"kappa1: \", kappa1[s])\n    \n    Omega.inv      = solve(priors$Omega)\n    Omega.post.inv = X%*%t(X) + (1/kappa1[s])*Omega.inv\n    Omega.post     = solve(Omega.post.inv)\n    B.post         = (Y%*%t(X) + priors$B%*%((1/kappa1[s])*Omega.inv)) %*% Omega.post\n    S.post         = Y%*%t(Y) + (1/kappa0[s])*solve(priors$S) + priors$B%*%((1/kappa1[s])*Omega.inv)%*%t(priors$B) - B.post%*%Omega.post.inv%*%t(B.post) \n    nu.post        = ncol(Y) + priors$nu\n    \n    # Use B0.initial for first iteration, otherwise the latest draw from B0.posterior\n    \n    if (s==1) {\n      B0.s = B0.initial\n    } else {\n      B0.s = B0.posterior[,,s-1]\n    }\n    \n    # sampling one draw B0 from the posterior distribution using Gibbs  \n    # rgn.function samples from a random conditional generalized normal distribution\n    \n    B0.tmp                  = rgn(n=1, S.inv=S.post, nu=nu.post, V=FF.V, B0.initial=B0.s)\n    B0.posterior[,,s]       = B0.tmp[,,1]\n    \n    #cat(\"B0: \", B0.posterior[,,s],\"\\n\")\n    \n    # sample one draw B+ from the normal conditional posterior\n    Bp.tmp              = rnorm.ngn(B0.tmp, B=B.post,Omega=Omega.post)\n    Bp.posterior[,,s]   = Bp.tmp[,,1]\n    \n    #compute posterior for the shrinkage parameter S.kappa and nu\n    S.kappa0.post = priors$S.kappa0 + sum(B0.posterior[,,s]^2)\n    \n    # S.kappa.post = sum(priors$S.kappa + (B0.posterior[i,,s]-priors$B[i,])%*%Omega.inv%*%t(B0.posterior[i,,s]-priors$B[i,]))\n    \n    # nu.kappa0.post  = priors$nu.kappa0 + i #change outside of loop count number rows (otherwise make as a sum of i's)\n    nu.kappa0.post  = priors$nu.kappa0 + sum(unlist(lapply(FF.V, nrow)))\n    \n    S.kappa1.post   = priors$S.kappa1\n    for (i in 1:N){\n      S.kappa1.post = S.kappa1.post + (Bp.posterior[i,,s]- B0.posterior[i,,s]%*%priors$B)%*%Omega.inv%*%t(Bp.posterior[i,,s]-B0.posterior[i,,s]%*%priors$B)\n    }\n    # S.kappa.post = sum(priors$S.kappa + (B0.posterior[i,,s]-priors$B[i,])%*%Omega.inv%*%t(B0.posterior[i,,s]-priors$B[i,]))\n    \n    nu.kappa1.post  = priors$nu.kappa1 + N*(p*N+1) \n    \n    \n    #Draw kappa0 and kappa1 from IG2\n    if (s != S1+S2) {\n      kappa0[s+1]    = S.kappa0.post / rchisq(1, df=nu.kappa0.post) \n      kappa1[s+1]    = S.kappa1.post / rchisq(1, df=nu.kappa1.post) \n    }\n  }\n  \n  #Discard first S1 draws\n  \n  B0.posterior <- B0.posterior[,,(S1+1):(S1+S2)]\n  Bp.posterior <- Bp.posterior[,,(S1+1):(S1+S2)]\n  kappa0       <- kappa0[(S1+1):(S1+S2)]\n  kappa1       <- kappa1[(S1+1):(S1+S2)]\n  \n  #normalisation of B0.posterior and Bp.posterior\n  \n  B0.hat             = diag(sign(diag(B0.tmp[,,1]))) %*% B0.tmp[,,1]\n  # t(chol((nu.post-N)*S.post))# normalisation using this B0.hat should work\n  \n  B0.posterior.N    <- array(NA,c(N,N,S2))\n  Bp.posterior.N    <- array(NA,c(N,(1+N*p),S2))\n  \n    B0.posteror.N.tmp      =  normalize.Gibbs.output.parallel(B0.posterior,B0.hat=B0.hat)\n  for (s in 1:S2){\n    B0.posterior.N[,,s]    = B0.posteror.N.tmp[,,1]\n    Bp.posterior.N[,,s]    = B0.posterior.N[,,s]%*%solve(B0.posterior[,,s])%*%Bp.posterior[,,s]\n  }\n  \n  return(list(B0.posterior.N = B0.posterior.N,\n              Bp.posterior.N = Bp.posterior.N,\n              kappa0 = kappa0,\n              kappa1 = kappa1))\n} \n\n\n\n           [,1]     [,2]\n[1,] 1.02123929 0.000000\n[2,] 0.02458385 1.026761\n\n\n           [,1]       [,2]       [,3]\n[1,]  0.1228352 1.00056214 0.01287995\n[2,] -0.2534007 0.01442736 1.01473022\n\n\nAgain, the estimation procedure for the extended model seems to be correct as well as the output aligns with the artificial data being a bivariate random walk.\nI now turn to plotting the diagonal elements of \\(B_+[,2:3]\\) in order to show whether the algorithm converges.\n\n\n\n\n\nThis is indeed the case. The plots look like white noise processes as it fluctuates around the true value 1. This means the algorithm has converged."
>>>>>>> 41d47063279825e9a18740578892ff6012578e11
>>>>>>> Stashed changes
  }
]